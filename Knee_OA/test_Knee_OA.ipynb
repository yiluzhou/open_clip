{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_4: Processing images: 100%|██████████| 1656/1656 [14:15<00:00,  1.94image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_4. Total images: 1656, predicted_correct': 456, predicted_correct (%)': 27.536231884057973%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_8: Processing images: 100%|██████████| 1656/1656 [13:32<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_8. Total images: 1656, predicted_correct': 797, predicted_correct (%)': 48.128019323671495%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_12: Processing images: 100%|██████████| 1656/1656 [13:05<00:00,  2.11image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_12. Total images: 1656, predicted_correct': 949, predicted_correct (%)': 57.306763285024154%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_16: Processing images: 100%|██████████| 1656/1656 [13:09<00:00,  2.10image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_16. Total images: 1656, predicted_correct': 1029, predicted_correct (%)': 62.13768115942029%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_20: Processing images: 100%|██████████| 1656/1656 [13:13<00:00,  2.09image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_20. Total images: 1656, predicted_correct': 1052, predicted_correct (%)': 63.52657004830918%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_24: Processing images: 100%|██████████| 1656/1656 [13:08<00:00,  2.10image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_24. Total images: 1656, predicted_correct': 1038, predicted_correct (%)': 62.68115942028986%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_28: Processing images: 100%|██████████| 1656/1656 [13:09<00:00,  2.10image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_28. Total images: 1656, predicted_correct': 1081, predicted_correct (%)': 65.27777777777779%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_32: Processing images: 100%|██████████| 1656/1656 [13:06<00:00,  2.11image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_32. Total images: 1656, predicted_correct': 1092, predicted_correct (%)': 65.94202898550725%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_36: Processing images: 100%|██████████| 1656/1656 [13:05<00:00,  2.11image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_36. Total images: 1656, predicted_correct': 1116, predicted_correct (%)': 67.3913043478261%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_40: Processing images: 100%|██████████| 1656/1656 [13:04<00:00,  2.11image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_40. Total images: 1656, predicted_correct': 1088, predicted_correct (%)': 65.70048309178745%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_44: Processing images: 100%|██████████| 1656/1656 [13:51<00:00,  1.99image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_44. Total images: 1656, predicted_correct': 1077, predicted_correct (%)': 65.03623188405797%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_48: Processing images: 100%|██████████| 1656/1656 [13:47<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_48. Total images: 1656, predicted_correct': 1079, predicted_correct (%)': 65.15700483091787%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_52: Processing images: 100%|██████████| 1656/1656 [14:36<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_52. Total images: 1656, predicted_correct': 1067, predicted_correct (%)': 64.43236714975845%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_56: Processing images: 100%|██████████| 1656/1656 [14:58<00:00,  1.84image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_56. Total images: 1656, predicted_correct': 1092, predicted_correct (%)': 65.94202898550725%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_60: Processing images: 100%|██████████| 1656/1656 [15:29<00:00,  1.78image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_60. Total images: 1656, predicted_correct': 1118, predicted_correct (%)': 67.512077294686%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_64: Processing images: 100%|██████████| 1656/1656 [14:51<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_64. Total images: 1656, predicted_correct': 1149, predicted_correct (%)': 69.38405797101449%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_68: Processing images: 100%|██████████| 1656/1656 [14:05<00:00,  1.96image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_68. Total images: 1656, predicted_correct': 1108, predicted_correct (%)': 66.90821256038647%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_72: Processing images: 100%|██████████| 1656/1656 [13:59<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_72. Total images: 1656, predicted_correct': 1083, predicted_correct (%)': 65.39855072463769%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_76: Processing images: 100%|██████████| 1656/1656 [14:03<00:00,  1.96image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_76. Total images: 1656, predicted_correct': 1105, predicted_correct (%)': 66.72705314009661%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_80: Processing images: 100%|██████████| 1656/1656 [14:41<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_80. Total images: 1656, predicted_correct': 1116, predicted_correct (%)': 67.3913043478261%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_84: Processing images: 100%|██████████| 1656/1656 [15:56<00:00,  1.73image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_84. Total images: 1656, predicted_correct': 1133, predicted_correct (%)': 68.41787439613528%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_88: Processing images: 100%|██████████| 1656/1656 [14:13<00:00,  1.94image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_88. Total images: 1656, predicted_correct': 1121, predicted_correct (%)': 67.69323671497585%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_92: Processing images: 100%|██████████| 1656/1656 [14:12<00:00,  1.94image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_92. Total images: 1656, predicted_correct': 1136, predicted_correct (%)': 68.59903381642512%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_96: Processing images: 100%|██████████| 1656/1656 [13:53<00:00,  1.99image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_96. Total images: 1656, predicted_correct': 1119, predicted_correct (%)': 67.57246376811594%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_100: Processing images: 100%|██████████| 1656/1656 [13:25<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_100. Total images: 1656, predicted_correct': 1139, predicted_correct (%)': 68.78019323671496%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_104: Processing images: 100%|██████████| 1656/1656 [13:21<00:00,  2.07image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_104. Total images: 1656, predicted_correct': 1138, predicted_correct (%)': 68.71980676328504%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_108: Processing images: 100%|██████████| 1656/1656 [13:16<00:00,  2.08image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_108. Total images: 1656, predicted_correct': 1135, predicted_correct (%)': 68.53864734299518%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_112: Processing images: 100%|██████████| 1656/1656 [13:17<00:00,  2.08image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_112. Total images: 1656, predicted_correct': 1145, predicted_correct (%)': 69.14251207729468%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_116: Processing images: 100%|██████████| 1656/1656 [13:25<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_116. Total images: 1656, predicted_correct': 1108, predicted_correct (%)': 66.90821256038647%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_120: Processing images: 100%|██████████| 1656/1656 [13:23<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_120. Total images: 1656, predicted_correct': 1132, predicted_correct (%)': 68.35748792270532%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_124: Processing images: 100%|██████████| 1656/1656 [13:22<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_124. Total images: 1656, predicted_correct': 1111, predicted_correct (%)': 67.08937198067633%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_128: Processing images: 100%|██████████| 1656/1656 [13:24<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_128. Total images: 1656, predicted_correct': 1107, predicted_correct (%)': 66.84782608695652%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_132: Processing images: 100%|██████████| 1656/1656 [13:24<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_132. Total images: 1656, predicted_correct': 1122, predicted_correct (%)': 67.7536231884058%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_136: Processing images: 100%|██████████| 1656/1656 [13:23<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_136. Total images: 1656, predicted_correct': 1113, predicted_correct (%)': 67.21014492753623%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_140: Processing images: 100%|██████████| 1656/1656 [13:24<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_140. Total images: 1656, predicted_correct': 1083, predicted_correct (%)': 65.39855072463769%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_144: Processing images: 100%|██████████| 1656/1656 [13:22<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_144. Total images: 1656, predicted_correct': 1138, predicted_correct (%)': 68.71980676328504%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_148: Processing images: 100%|██████████| 1656/1656 [13:25<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_148. Total images: 1656, predicted_correct': 1121, predicted_correct (%)': 67.69323671497585%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_152: Processing images: 100%|██████████| 1656/1656 [13:22<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_152. Total images: 1656, predicted_correct': 1148, predicted_correct (%)': 69.32367149758454%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_156: Processing images: 100%|██████████| 1656/1656 [13:23<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_156. Total images: 1656, predicted_correct': 1123, predicted_correct (%)': 67.81400966183575%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_160: Processing images: 100%|██████████| 1656/1656 [13:31<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_160. Total images: 1656, predicted_correct': 1143, predicted_correct (%)': 69.02173913043478%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_164: Processing images: 100%|██████████| 1656/1656 [13:27<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_164. Total images: 1656, predicted_correct': 1128, predicted_correct (%)': 68.11594202898551%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_168: Processing images: 100%|██████████| 1656/1656 [13:32<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_168. Total images: 1656, predicted_correct': 1121, predicted_correct (%)': 67.69323671497585%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_172: Processing images: 100%|██████████| 1656/1656 [14:11<00:00,  1.94image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_172. Total images: 1656, predicted_correct': 1091, predicted_correct (%)': 65.88164251207729%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_176: Processing images:  11%|█         | 176/1656 [01:32<12:59,  1.90image/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 148\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[39mfor\u001b[39;00m pretrained_model \u001b[39min\u001b[39;00m sorted_pt_files:\n\u001b[1;32m    145\u001b[0m         process_single_pretrained_model(pretrained_model, csv_file)\n\u001b[0;32m--> 148\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[1], line 145\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m csv_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/yilu/Development/open_clip/Knee_OA/temp/benchmark_all_epochs.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m \u001b[39mfor\u001b[39;00m pretrained_model \u001b[39min\u001b[39;00m sorted_pt_files:\n\u001b[0;32m--> 145\u001b[0m     process_single_pretrained_model(pretrained_model, csv_file)\n",
      "Cell \u001b[0;32mIn[1], line 114\u001b[0m, in \u001b[0;36mprocess_single_pretrained_model\u001b[0;34m(pretrained_model, csv_file)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_single_pretrained_model\u001b[39m(pretrained_model, csv_file):\n\u001b[1;32m    113\u001b[0m     df_single_pretrained_model \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m--> 114\u001b[0m     df \u001b[39m=\u001b[39m process_single_epoch(pretrained_model, classification)\n\u001b[1;32m    115\u001b[0m     df_single_pretrained_model \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df_single_pretrained_model, df])\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    116\u001b[0m     save_to_csv(df_single_pretrained_model, csv_file)\n",
      "Cell \u001b[0;32mIn[1], line 88\u001b[0m, in \u001b[0;36mprocess_single_epoch\u001b[0;34m(pretrained_model, sentences)\u001b[0m\n\u001b[1;32m     86\u001b[0m all_images \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(img_dir)\n\u001b[1;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m img_filename \u001b[39min\u001b[39;00m tqdm(all_images, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch_\u001b[39m\u001b[39m{\u001b[39;00mepoch_number\u001b[39m}\u001b[39;00m\u001b[39m: Processing images\u001b[39m\u001b[39m\"\u001b[39m, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 88\u001b[0m     df \u001b[39m=\u001b[39m read_single_image(img_dir, img_filename, sentences, text_features)\n\u001b[1;32m     89\u001b[0m     df_all \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df_all, df])\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     91\u001b[0m \u001b[39m#save df_all to csv\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 66\u001b[0m, in \u001b[0;36mprocess_single_epoch.<locals>.read_single_image\u001b[0;34m(img_dir, img_filename, sentences, text_features)\u001b[0m\n\u001b[1;32m     63\u001b[0m image \u001b[39m=\u001b[39m preprocess(Image\u001b[39m.\u001b[39mopen(image_path)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(), torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast():\n\u001b[0;32m---> 66\u001b[0m     image_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode_image(image)\n\u001b[1;32m     67\u001b[0m     image_features \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m image_features\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m     text_probs \u001b[39m=\u001b[39m (\u001b[39m100.0\u001b[39m \u001b[39m*\u001b[39m image_features \u001b[39m@\u001b[39m text_features\u001b[39m.\u001b[39mT)\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/coca_model.py:143\u001b[0m, in \u001b[0;36mCoCa.encode_image\u001b[0;34m(self, images, normalize)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_image\u001b[39m(\u001b[39mself\u001b[39m, images, normalize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 143\u001b[0m     image_latent, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_image(images, normalize\u001b[39m=\u001b[39;49mnormalize)\n\u001b[1;32m    144\u001b[0m     \u001b[39mreturn\u001b[39;00m image_latent\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/coca_model.py:132\u001b[0m, in \u001b[0;36mCoCa._encode_image\u001b[0;34m(self, images, normalize)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_image\u001b[39m(\u001b[39mself\u001b[39m, images, normalize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 132\u001b[0m     image_latent, tokens_embs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual(images)\n\u001b[1;32m    133\u001b[0m     image_latent \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnormalize(image_latent, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m normalize \u001b[39melse\u001b[39;00m image_latent\n\u001b[1;32m    134\u001b[0m     \u001b[39mreturn\u001b[39;00m image_latent, tokens_embs\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/transformer.py:486\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    483\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_pre(x)\n\u001b[1;32m    485\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m    487\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_pool \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/transformer.py:321\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    319\u001b[0m         x \u001b[39m=\u001b[39m checkpoint(r, x, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, attn_mask)\n\u001b[1;32m    320\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m         x \u001b[39m=\u001b[39m r(x, attn_mask\u001b[39m=\u001b[39;49mattn_mask)\n\u001b[1;32m    322\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/transformer.py:243\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    240\u001b[0m v_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(v_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    242\u001b[0m x \u001b[39m=\u001b[39m q_x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(q_x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(q_x), k_x\u001b[39m=\u001b[39mk_x, v_x\u001b[39m=\u001b[39mv_x, attn_mask\u001b[39m=\u001b[39mattn_mask))\n\u001b[0;32m--> 243\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_2(x)))\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Batch test for Direct Classification benchmark 4 (cropped square & data augmentation images, normalizing with mean and std)\n",
    "import torch, os, open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import ToTensor\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "class AlbumentationsTransform2:\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)  # Convert to numpy array\n",
    "        img = self.transform(image=img)['image']\n",
    "        return Image.fromarray(img)  # Convert back to PIL image\n",
    "\n",
    "# Define the CLAHE transformation\n",
    "clahe = A.CLAHE(p=1.0, clip_limit=6.0, tile_grid_size=(12, 12))\n",
    "CLAHE = AlbumentationsTransform2(clahe) # normalizing using Adaptive Histogram Equalization (CLAHE)\n",
    "\n",
    "caption_map = {\n",
    "    \"healthy\": \"healthy normal\",\n",
    "    \"doubtful\": \"doubtful osteoarthritis\",\n",
    "    \"minimal\": \"minimal osteoarthritis\",\n",
    "    \"moderate\": \"moderate osteoarthritis\",\n",
    "    \"severe\": \"severe osteoarthritis\"\n",
    "}\n",
    "\n",
    "classification = list(caption_map.values())\n",
    "\n",
    "def get_ground_truth(filename, caption_map):\n",
    "    for keyword, ground_truth in caption_map.items():\n",
    "        if keyword in filename:\n",
    "            return ground_truth\n",
    "    return None\n",
    "\n",
    "def process_single_epoch(pretrained_model, sentences):\n",
    "    img_dir = '/mnt/g/Datasets/Knee_OA/Original/test/images/'\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        model_name=\"coca_ViT-L-14\",\n",
    "        pretrained=pretrained_model,\n",
    "    )\n",
    "    pt_dir = os.path.dirname(pretrained_model)\n",
    "    epoch_number = os.path.basename(pretrained_model).split('epoch_')[1].split('.pt')[0]\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    # Prepare the tokenizer for sentences\n",
    "    tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "    text = tokenizer(sentences)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def read_single_image(img_dir, img_filename, sentences, text_features):\n",
    "        #get the ground truth value\n",
    "        image_path = os.path.join(img_dir, img_filename)\n",
    "        ground_truth = get_ground_truth(img_filename, caption_map)\n",
    "        \n",
    "        # load a sample image\n",
    "        image = preprocess(Image.open(image_path).convert('RGB')).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            text_probs = text_probs.cpu().tolist()[0]\n",
    "\n",
    "        # Construct the dictionary\n",
    "        class_predict_dict = dict(zip(sentences, text_probs))\n",
    "        # Extract the key with the largest value\n",
    "        class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "        \n",
    "        if ground_truth == class_predict:\n",
    "            predicted_correct = 1\n",
    "        else:\n",
    "            predicted_correct = 0\n",
    "        \n",
    "        pred_dict = {'img_filename': [img_filename], 'class_predict_dict': [class_predict_dict], 'ground_truth': [ground_truth], 'class_predict': [class_predict], 'predicted_correct': [predicted_correct]}\n",
    "        df_single = pd.DataFrame(pred_dict)\n",
    "        \n",
    "        return df_single\n",
    "\n",
    "    all_images = os.listdir(img_dir)\n",
    "    for img_filename in tqdm(all_images, desc=f\"epoch_{epoch_number}: Processing images\", unit=\"image\"):\n",
    "        df = read_single_image(img_dir, img_filename, sentences, text_features)\n",
    "        df_all = pd.concat([df_all, df]).reset_index(drop=True)\n",
    "\n",
    "    #save df_all to csv\n",
    "    csv_file = f\"/home/yilu/Development/open_clip/Knee_OA/temp/epoch_{epoch_number}.csv\"\n",
    "    df_all.to_csv(csv_file, index=False)\n",
    "\n",
    "    total_rows = len(df_all)\n",
    "    # Count the occurrences of '1' in the 'predicted_correct' column\n",
    "    count_ones = df_all['predicted_correct'].sum()\n",
    "    epoch_filename = os.path.splitext(os.path.basename(pretrained_model))[0]\n",
    "    predicted_correct_pct = count_ones /total_rows *100\n",
    "    print(f\"Epoch_file: {epoch_filename}. Total images: {total_rows}, predicted_correct': {count_ones}, predicted_correct (%)': {predicted_correct_pct}%\")\n",
    "    \n",
    "    epoch_dict = {'pt_dir': [pt_dir],\n",
    "                'epoch_filename': [epoch_filename], \n",
    "                'sentences': [sentences], \n",
    "                'total_rows': [total_rows], \n",
    "                'predicted_correct': [count_ones], \n",
    "                'predicted_correct (%)': [predicted_correct_pct]}\n",
    "    df_single_epoch = pd.DataFrame(epoch_dict)\n",
    "    return df_single_epoch\n",
    "\n",
    "\n",
    "def process_single_pretrained_model(pretrained_model, csv_file):\n",
    "    df_single_pretrained_model = pd.DataFrame()\n",
    "    df = process_single_epoch(pretrained_model, classification)\n",
    "    df_single_pretrained_model = pd.concat([df_single_pretrained_model, df]).reset_index(drop=True)\n",
    "    save_to_csv(df_single_pretrained_model, csv_file)\n",
    "    \n",
    "def extract_epoch_num(filepath):\n",
    "    # Split the filename from the path\n",
    "    filename = os.path.basename(filepath) #filepath.split('/')[-1]\n",
    "    # Extract the number between \"epoch_\" and \".pt\"\n",
    "    epoch_num = int(filename.split('epoch_')[1].split('.pt')[0])\n",
    "    return epoch_num\n",
    "\n",
    "def save_to_csv(df, csv_file):\n",
    "    if not os.path.exists(csv_file):\n",
    "        df.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # If the CSV file already exists, append without header\n",
    "        df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "def main():\n",
    "    #obtain the latest pretrained model folder\n",
    "    pt_folder_root = '/mnt/g/Logtemp/open_clip/Knee_OA/'\n",
    "    dir_1 = [a for a in os.listdir(pt_folder_root) if a.startswith('2023_08_24-23')][0]\n",
    "    pt_folder_path = os.path.join(pt_folder_root, dir_1, 'checkpoints')\n",
    "\n",
    "    pt_files = [os.path.join(pt_folder_path, filename) for filename in os.listdir(pt_folder_path) if filename.endswith('.pt')]\n",
    "    #filter pt_files with low epoch number\n",
    "    pt_files = [pt_file for pt_file in pt_files if extract_epoch_num(pt_file) >= 0]\n",
    "    # Sort the list\n",
    "    sorted_pt_files = sorted(pt_files, key=extract_epoch_num)\n",
    "    csv_file = \"/home/yilu/Development/open_clip/Knee_OA/temp/benchmark_all_epochs.csv\"\n",
    "    for pretrained_model in sorted_pt_files:\n",
    "        process_single_pretrained_model(pretrained_model, csv_file)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_196: Processing images: 100%|██████████| 1656/1656 [12:41<00:00,  2.17image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_196. Total images: 1656, predicted_correct': 1123, predicted_correct (%)': 67.81400966183575%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_200: Processing images: 100%|██████████| 1656/1656 [12:43<00:00,  2.17image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_200. Total images: 1656, predicted_correct': 1129, predicted_correct (%)': 68.17632850241546%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_204: Processing images: 100%|██████████| 1656/1656 [12:24<00:00,  2.22image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_204. Total images: 1656, predicted_correct': 1103, predicted_correct (%)': 66.60628019323671%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_208: Processing images: 100%|██████████| 1656/1656 [12:52<00:00,  2.14image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_208. Total images: 1656, predicted_correct': 1108, predicted_correct (%)': 66.90821256038647%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_212: Processing images: 100%|██████████| 1656/1656 [13:24<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_212. Total images: 1656, predicted_correct': 1111, predicted_correct (%)': 67.08937198067633%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_216: Processing images: 100%|██████████| 1656/1656 [13:08<00:00,  2.10image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_216. Total images: 1656, predicted_correct': 1131, predicted_correct (%)': 68.29710144927536%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_220: Processing images: 100%|██████████| 1656/1656 [13:00<00:00,  2.12image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_220. Total images: 1656, predicted_correct': 1126, predicted_correct (%)': 67.9951690821256%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_224: Processing images: 100%|██████████| 1656/1656 [13:33<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_224. Total images: 1656, predicted_correct': 1148, predicted_correct (%)': 69.32367149758454%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_228: Processing images: 100%|██████████| 1656/1656 [13:36<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_228. Total images: 1656, predicted_correct': 1125, predicted_correct (%)': 67.93478260869566%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_232: Processing images: 100%|██████████| 1656/1656 [13:06<00:00,  2.10image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_232. Total images: 1656, predicted_correct': 1140, predicted_correct (%)': 68.84057971014492%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_236: Processing images: 100%|██████████| 1656/1656 [13:08<00:00,  2.10image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_236. Total images: 1656, predicted_correct': 1140, predicted_correct (%)': 68.84057971014492%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_240: Processing images: 100%|██████████| 1656/1656 [13:26<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_240. Total images: 1656, predicted_correct': 1134, predicted_correct (%)': 68.47826086956522%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_244: Processing images: 100%|██████████| 1656/1656 [13:26<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_244. Total images: 1656, predicted_correct': 1126, predicted_correct (%)': 67.9951690821256%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_248: Processing images: 100%|██████████| 1656/1656 [13:21<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_248. Total images: 1656, predicted_correct': 1116, predicted_correct (%)': 67.3913043478261%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_252: Processing images: 100%|██████████| 1656/1656 [13:22<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_252. Total images: 1656, predicted_correct': 1135, predicted_correct (%)': 68.53864734299518%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_256: Processing images: 100%|██████████| 1656/1656 [13:14<00:00,  2.08image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_256. Total images: 1656, predicted_correct': 1133, predicted_correct (%)': 68.41787439613528%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_260: Processing images: 100%|██████████| 1656/1656 [14:20<00:00,  1.93image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_260. Total images: 1656, predicted_correct': 1118, predicted_correct (%)': 67.512077294686%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_264: Processing images: 100%|██████████| 1656/1656 [13:39<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_264. Total images: 1656, predicted_correct': 1113, predicted_correct (%)': 67.21014492753623%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_268: Processing images: 100%|██████████| 1656/1656 [14:10<00:00,  1.95image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_268. Total images: 1656, predicted_correct': 1124, predicted_correct (%)': 67.8743961352657%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_272: Processing images: 100%|██████████| 1656/1656 [13:50<00:00,  1.99image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_272. Total images: 1656, predicted_correct': 1139, predicted_correct (%)': 68.78019323671496%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_276: Processing images:  14%|█▍        | 239/1656 [02:04<12:20,  1.91image/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 148\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[39mfor\u001b[39;00m pretrained_model \u001b[39min\u001b[39;00m sorted_pt_files:\n\u001b[1;32m    145\u001b[0m         process_single_pretrained_model(pretrained_model, csv_file)\n\u001b[0;32m--> 148\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[3], line 145\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m csv_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/yilu/Development/open_clip/Knee_OA/temp1/benchmark_all_epochs.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m \u001b[39mfor\u001b[39;00m pretrained_model \u001b[39min\u001b[39;00m sorted_pt_files:\n\u001b[0;32m--> 145\u001b[0m     process_single_pretrained_model(pretrained_model, csv_file)\n",
      "Cell \u001b[0;32mIn[3], line 114\u001b[0m, in \u001b[0;36mprocess_single_pretrained_model\u001b[0;34m(pretrained_model, csv_file)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_single_pretrained_model\u001b[39m(pretrained_model, csv_file):\n\u001b[1;32m    113\u001b[0m     df_single_pretrained_model \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m--> 114\u001b[0m     df \u001b[39m=\u001b[39m process_single_epoch(pretrained_model, classification)\n\u001b[1;32m    115\u001b[0m     df_single_pretrained_model \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df_single_pretrained_model, df])\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    116\u001b[0m     save_to_csv(df_single_pretrained_model, csv_file)\n",
      "Cell \u001b[0;32mIn[3], line 88\u001b[0m, in \u001b[0;36mprocess_single_epoch\u001b[0;34m(pretrained_model, sentences)\u001b[0m\n\u001b[1;32m     86\u001b[0m all_images \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(img_dir)\n\u001b[1;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m img_filename \u001b[39min\u001b[39;00m tqdm(all_images, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch_\u001b[39m\u001b[39m{\u001b[39;00mepoch_number\u001b[39m}\u001b[39;00m\u001b[39m: Processing images\u001b[39m\u001b[39m\"\u001b[39m, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 88\u001b[0m     df \u001b[39m=\u001b[39m read_single_image(img_dir, img_filename, sentences, text_features)\n\u001b[1;32m     89\u001b[0m     df_all \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df_all, df])\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     91\u001b[0m \u001b[39m#save df_all to csv\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 66\u001b[0m, in \u001b[0;36mprocess_single_epoch.<locals>.read_single_image\u001b[0;34m(img_dir, img_filename, sentences, text_features)\u001b[0m\n\u001b[1;32m     63\u001b[0m image \u001b[39m=\u001b[39m preprocess(Image\u001b[39m.\u001b[39mopen(image_path)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(), torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast():\n\u001b[0;32m---> 66\u001b[0m     image_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode_image(image)\n\u001b[1;32m     67\u001b[0m     image_features \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m image_features\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m     text_probs \u001b[39m=\u001b[39m (\u001b[39m100.0\u001b[39m \u001b[39m*\u001b[39m image_features \u001b[39m@\u001b[39m text_features\u001b[39m.\u001b[39mT)\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/coca_model.py:143\u001b[0m, in \u001b[0;36mCoCa.encode_image\u001b[0;34m(self, images, normalize)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_image\u001b[39m(\u001b[39mself\u001b[39m, images, normalize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 143\u001b[0m     image_latent, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_image(images, normalize\u001b[39m=\u001b[39;49mnormalize)\n\u001b[1;32m    144\u001b[0m     \u001b[39mreturn\u001b[39;00m image_latent\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/coca_model.py:132\u001b[0m, in \u001b[0;36mCoCa._encode_image\u001b[0;34m(self, images, normalize)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_image\u001b[39m(\u001b[39mself\u001b[39m, images, normalize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 132\u001b[0m     image_latent, tokens_embs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual(images)\n\u001b[1;32m    133\u001b[0m     image_latent \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnormalize(image_latent, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m normalize \u001b[39melse\u001b[39;00m image_latent\n\u001b[1;32m    134\u001b[0m     \u001b[39mreturn\u001b[39;00m image_latent, tokens_embs\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/transformer.py:486\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    483\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_pre(x)\n\u001b[1;32m    485\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m    487\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_pool \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/transformer.py:321\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    319\u001b[0m         x \u001b[39m=\u001b[39m checkpoint(r, x, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, attn_mask)\n\u001b[1;32m    320\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m         x \u001b[39m=\u001b[39m r(x, attn_mask\u001b[39m=\u001b[39;49mattn_mask)\n\u001b[1;32m    322\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/transformer.py:243\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    240\u001b[0m v_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(v_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    242\u001b[0m x \u001b[39m=\u001b[39m q_x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(q_x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(q_x), k_x\u001b[39m=\u001b[39mk_x, v_x\u001b[39m=\u001b[39mv_x, attn_mask\u001b[39m=\u001b[39mattn_mask))\n\u001b[0;32m--> 243\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_2(x)))\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Batch test for Direct Classification benchmark 5 (cropped square & data augmentation images, normalizing with mean and std)\n",
    "import torch, os, open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import ToTensor\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "class AlbumentationsTransform2:\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)  # Convert to numpy array\n",
    "        img = self.transform(image=img)['image']\n",
    "        return Image.fromarray(img)  # Convert back to PIL image\n",
    "\n",
    "# Define the CLAHE transformation\n",
    "clahe = A.CLAHE(p=1.0, clip_limit=6.0, tile_grid_size=(12, 12))\n",
    "CLAHE = AlbumentationsTransform2(clahe) # normalizing using Adaptive Histogram Equalization (CLAHE)\n",
    "\n",
    "caption_map = {\n",
    "    \"healthy\": \"healthy normal\",\n",
    "    \"doubtful\": \"doubtful osteoarthritis\",\n",
    "    \"minimal\": \"minimal osteoarthritis\",\n",
    "    \"moderate\": \"moderate osteoarthritis\",\n",
    "    \"severe\": \"severe osteoarthritis\"\n",
    "}\n",
    "\n",
    "classification = list(caption_map.values())\n",
    "\n",
    "def get_ground_truth(filename, caption_map):\n",
    "    for keyword, ground_truth in caption_map.items():\n",
    "        if keyword in filename:\n",
    "            return ground_truth\n",
    "    return None\n",
    "\n",
    "def process_single_epoch(pretrained_model, sentences):\n",
    "    img_dir = '/mnt/g/Datasets/Knee_OA/Original/test/images/'\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        model_name=\"coca_ViT-L-14\",\n",
    "        pretrained=pretrained_model,\n",
    "    )\n",
    "    pt_dir = os.path.dirname(pretrained_model)\n",
    "    epoch_number = os.path.basename(pretrained_model).split('epoch_')[1].split('.pt')[0]\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    # Prepare the tokenizer for sentences\n",
    "    tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "    text = tokenizer(sentences)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def read_single_image(img_dir, img_filename, sentences, text_features):\n",
    "        #get the ground truth value\n",
    "        image_path = os.path.join(img_dir, img_filename)\n",
    "        ground_truth = get_ground_truth(img_filename, caption_map)\n",
    "        \n",
    "        # load a sample image\n",
    "        image = preprocess(Image.open(image_path).convert('RGB')).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            text_probs = text_probs.cpu().tolist()[0]\n",
    "\n",
    "        # Construct the dictionary\n",
    "        class_predict_dict = dict(zip(sentences, text_probs))\n",
    "        # Extract the key with the largest value\n",
    "        class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "        \n",
    "        if ground_truth == class_predict:\n",
    "            predicted_correct = 1\n",
    "        else:\n",
    "            predicted_correct = 0\n",
    "        \n",
    "        pred_dict = {'img_filename': [img_filename], 'class_predict_dict': [class_predict_dict], 'ground_truth': [ground_truth], 'class_predict': [class_predict], 'predicted_correct': [predicted_correct]}\n",
    "        df_single = pd.DataFrame(pred_dict)\n",
    "        \n",
    "        return df_single\n",
    "\n",
    "    all_images = os.listdir(img_dir)\n",
    "    for img_filename in tqdm(all_images, desc=f\"epoch_{epoch_number}: Processing images\", unit=\"image\"):\n",
    "        df = read_single_image(img_dir, img_filename, sentences, text_features)\n",
    "        df_all = pd.concat([df_all, df]).reset_index(drop=True)\n",
    "\n",
    "    #save df_all to csv\n",
    "    csv_file = f\"/home/yilu/Development/open_clip/Knee_OA/temp1/epoch_{epoch_number}.csv\"\n",
    "    df_all.to_csv(csv_file, index=False)\n",
    "\n",
    "    total_rows = len(df_all)\n",
    "    # Count the occurrences of '1' in the 'predicted_correct' column\n",
    "    count_ones = df_all['predicted_correct'].sum()\n",
    "    epoch_filename = os.path.splitext(os.path.basename(pretrained_model))[0]\n",
    "    predicted_correct_pct = count_ones /total_rows *100\n",
    "    print(f\"Epoch_file: {epoch_filename}. Total images: {total_rows}, predicted_correct': {count_ones}, predicted_correct (%)': {predicted_correct_pct}%\")\n",
    "    \n",
    "    epoch_dict = {'pt_dir': [pt_dir],\n",
    "                'epoch_filename': [epoch_filename], \n",
    "                'sentences': [sentences], \n",
    "                'total_rows': [total_rows], \n",
    "                'predicted_correct': [count_ones], \n",
    "                'predicted_correct (%)': [predicted_correct_pct]}\n",
    "    df_single_epoch = pd.DataFrame(epoch_dict)\n",
    "    return df_single_epoch\n",
    "\n",
    "\n",
    "def process_single_pretrained_model(pretrained_model, csv_file):\n",
    "    df_single_pretrained_model = pd.DataFrame()\n",
    "    df = process_single_epoch(pretrained_model, classification)\n",
    "    df_single_pretrained_model = pd.concat([df_single_pretrained_model, df]).reset_index(drop=True)\n",
    "    save_to_csv(df_single_pretrained_model, csv_file)\n",
    "    \n",
    "def extract_epoch_num(filepath):\n",
    "    # Split the filename from the path\n",
    "    filename = os.path.basename(filepath) #filepath.split('/')[-1]\n",
    "    # Extract the number between \"epoch_\" and \".pt\"\n",
    "    epoch_num = int(filename.split('epoch_')[1].split('.pt')[0])\n",
    "    return epoch_num\n",
    "\n",
    "def save_to_csv(df, csv_file):\n",
    "    if not os.path.exists(csv_file):\n",
    "        df.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # If the CSV file already exists, append without header\n",
    "        df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "def main():\n",
    "    #obtain the latest pretrained model folder\n",
    "    pt_folder_root = '/mnt/g/Logtemp/open_clip/Knee_OA/'\n",
    "    dir_1 = [a for a in os.listdir(pt_folder_root) if a.startswith('2023_08_25-18_55_21')][0]\n",
    "    pt_folder_path = os.path.join(pt_folder_root, dir_1, 'checkpoints')\n",
    "\n",
    "    pt_files = [os.path.join(pt_folder_path, filename) for filename in os.listdir(pt_folder_path) if filename.endswith('.pt')]\n",
    "    #filter pt_files with low epoch number\n",
    "    pt_files = [pt_file for pt_file in pt_files if extract_epoch_num(pt_file) >= 193]\n",
    "    # Sort the list\n",
    "    sorted_pt_files = sorted(pt_files, key=extract_epoch_num)\n",
    "    csv_file = \"/home/yilu/Development/open_clip/Knee_OA/temp1/benchmark_all_epochs.csv\"\n",
    "    for pretrained_model in sorted_pt_files:\n",
    "        process_single_pretrained_model(pretrained_model, csv_file)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_112: Processing images: 100%|██████████| 1656/1656 [14:51<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_112. Total images: 1656, predicted_correct': 1120, predicted_correct (%)': 67.6328502415459%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_116: Processing images: 100%|██████████| 1656/1656 [14:11<00:00,  1.94image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_116. Total images: 1656, predicted_correct': 1131, predicted_correct (%)': 68.29710144927536%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_120: Processing images: 100%|██████████| 1656/1656 [13:28<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_120. Total images: 1656, predicted_correct': 1132, predicted_correct (%)': 68.35748792270532%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_124: Processing images: 100%|██████████| 1656/1656 [14:17<00:00,  1.93image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_124. Total images: 1656, predicted_correct': 1118, predicted_correct (%)': 67.512077294686%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_128: Processing images: 100%|██████████| 1656/1656 [14:13<00:00,  1.94image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_128. Total images: 1656, predicted_correct': 1077, predicted_correct (%)': 65.03623188405797%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_132: Processing images: 100%|██████████| 1656/1656 [14:14<00:00,  1.94image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_132. Total images: 1656, predicted_correct': 1129, predicted_correct (%)': 68.17632850241546%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_136: Processing images: 100%|██████████| 1656/1656 [14:15<00:00,  1.94image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_136. Total images: 1656, predicted_correct': 1119, predicted_correct (%)': 67.57246376811594%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_140: Processing images: 100%|██████████| 1656/1656 [14:30<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_140. Total images: 1656, predicted_correct': 1124, predicted_correct (%)': 67.8743961352657%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_144: Processing images: 100%|██████████| 1656/1656 [14:59<00:00,  1.84image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_144. Total images: 1656, predicted_correct': 1135, predicted_correct (%)': 68.53864734299518%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_148: Processing images: 100%|██████████| 1656/1656 [13:49<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_148. Total images: 1656, predicted_correct': 1109, predicted_correct (%)': 66.96859903381642%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_152: Processing images: 100%|██████████| 1656/1656 [13:29<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_152. Total images: 1656, predicted_correct': 1123, predicted_correct (%)': 67.81400966183575%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_156: Processing images: 100%|██████████| 1656/1656 [13:30<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_156. Total images: 1656, predicted_correct': 1125, predicted_correct (%)': 67.93478260869566%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_160: Processing images: 100%|██████████| 1656/1656 [13:31<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_160. Total images: 1656, predicted_correct': 1099, predicted_correct (%)': 66.3647342995169%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_164: Processing images: 100%|██████████| 1656/1656 [13:36<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_164. Total images: 1656, predicted_correct': 1075, predicted_correct (%)': 64.91545893719807%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_168: Processing images: 100%|██████████| 1656/1656 [13:40<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_168. Total images: 1656, predicted_correct': 1109, predicted_correct (%)': 66.96859903381642%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_172: Processing images: 100%|██████████| 1656/1656 [13:30<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_172. Total images: 1656, predicted_correct': 1141, predicted_correct (%)': 68.90096618357488%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_176: Processing images: 100%|██████████| 1656/1656 [13:31<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_176. Total images: 1656, predicted_correct': 1151, predicted_correct (%)': 69.5048309178744%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_180: Processing images: 100%|██████████| 1656/1656 [13:31<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_180. Total images: 1656, predicted_correct': 1071, predicted_correct (%)': 64.67391304347827%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_184: Processing images: 100%|██████████| 1656/1656 [13:31<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_184. Total images: 1656, predicted_correct': 1123, predicted_correct (%)': 67.81400966183575%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_188: Processing images: 100%|██████████| 1656/1656 [13:31<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_188. Total images: 1656, predicted_correct': 1153, predicted_correct (%)': 69.6256038647343%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_192: Processing images: 100%|██████████| 1656/1656 [14:47<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_192. Total images: 1656, predicted_correct': 1134, predicted_correct (%)': 68.47826086956522%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_196: Processing images: 100%|██████████| 1656/1656 [14:42<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_196. Total images: 1656, predicted_correct': 1133, predicted_correct (%)': 68.41787439613528%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_200: Processing images: 100%|██████████| 1656/1656 [14:44<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_200. Total images: 1656, predicted_correct': 1159, predicted_correct (%)': 69.987922705314%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_204: Processing images: 100%|██████████| 1656/1656 [14:40<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_204. Total images: 1656, predicted_correct': 1145, predicted_correct (%)': 69.14251207729468%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_208: Processing images: 100%|██████████| 1656/1656 [14:44<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_208. Total images: 1656, predicted_correct': 1112, predicted_correct (%)': 67.14975845410628%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_212: Processing images: 100%|██████████| 1656/1656 [14:45<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_212. Total images: 1656, predicted_correct': 1137, predicted_correct (%)': 68.65942028985508%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_216: Processing images: 100%|██████████| 1656/1656 [14:41<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_216. Total images: 1656, predicted_correct': 1123, predicted_correct (%)': 67.81400966183575%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_220: Processing images: 100%|██████████| 1656/1656 [14:35<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_220. Total images: 1656, predicted_correct': 1122, predicted_correct (%)': 67.7536231884058%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_224: Processing images: 100%|██████████| 1656/1656 [13:28<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_224. Total images: 1656, predicted_correct': 1141, predicted_correct (%)': 68.90096618357488%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_228: Processing images: 100%|██████████| 1656/1656 [13:29<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_228. Total images: 1656, predicted_correct': 1112, predicted_correct (%)': 67.14975845410628%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_232: Processing images: 100%|██████████| 1656/1656 [13:31<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_232. Total images: 1656, predicted_correct': 1146, predicted_correct (%)': 69.20289855072464%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_236: Processing images: 100%|██████████| 1656/1656 [13:32<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_236. Total images: 1656, predicted_correct': 1146, predicted_correct (%)': 69.20289855072464%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_240: Processing images: 100%|██████████| 1656/1656 [14:35<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_240. Total images: 1656, predicted_correct': 1137, predicted_correct (%)': 68.65942028985508%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_244: Processing images: 100%|██████████| 1656/1656 [14:40<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_244. Total images: 1656, predicted_correct': 1141, predicted_correct (%)': 68.90096618357488%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_248: Processing images: 100%|██████████| 1656/1656 [14:42<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_248. Total images: 1656, predicted_correct': 1132, predicted_correct (%)': 68.35748792270532%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_252: Processing images: 100%|██████████| 1656/1656 [14:42<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_252. Total images: 1656, predicted_correct': 1111, predicted_correct (%)': 67.08937198067633%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_256: Processing images: 100%|██████████| 1656/1656 [13:43<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_256. Total images: 1656, predicted_correct': 1134, predicted_correct (%)': 68.47826086956522%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_260: Processing images: 100%|██████████| 1656/1656 [13:29<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_260. Total images: 1656, predicted_correct': 1097, predicted_correct (%)': 66.24396135265701%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_264: Processing images: 100%|██████████| 1656/1656 [13:30<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_264. Total images: 1656, predicted_correct': 1112, predicted_correct (%)': 67.14975845410628%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_268: Processing images: 100%|██████████| 1656/1656 [13:30<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_268. Total images: 1656, predicted_correct': 1135, predicted_correct (%)': 68.53864734299518%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_272: Processing images: 100%|██████████| 1656/1656 [14:22<00:00,  1.92image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_272. Total images: 1656, predicted_correct': 1132, predicted_correct (%)': 68.35748792270532%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_276: Processing images: 100%|██████████| 1656/1656 [14:39<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_276. Total images: 1656, predicted_correct': 1151, predicted_correct (%)': 69.5048309178744%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_280: Processing images: 100%|██████████| 1656/1656 [14:43<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_280. Total images: 1656, predicted_correct': 1113, predicted_correct (%)': 67.21014492753623%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_284: Processing images: 100%|██████████| 1656/1656 [14:38<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_284. Total images: 1656, predicted_correct': 1135, predicted_correct (%)': 68.53864734299518%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_288: Processing images: 100%|██████████| 1656/1656 [13:47<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_288. Total images: 1656, predicted_correct': 1111, predicted_correct (%)': 67.08937198067633%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_292: Processing images: 100%|██████████| 1656/1656 [13:51<00:00,  1.99image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_292. Total images: 1656, predicted_correct': 1120, predicted_correct (%)': 67.6328502415459%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_296: Processing images: 100%|██████████| 1656/1656 [13:44<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_296. Total images: 1656, predicted_correct': 1119, predicted_correct (%)': 67.57246376811594%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_300: Processing images: 100%|██████████| 1656/1656 [13:45<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_300. Total images: 1656, predicted_correct': 1154, predicted_correct (%)': 69.68599033816425%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_304: Processing images: 100%|██████████| 1656/1656 [13:43<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_304. Total images: 1656, predicted_correct': 1124, predicted_correct (%)': 67.8743961352657%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_308: Processing images: 100%|██████████| 1656/1656 [13:49<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_308. Total images: 1656, predicted_correct': 1140, predicted_correct (%)': 68.84057971014492%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_312: Processing images: 100%|██████████| 1656/1656 [13:43<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_312. Total images: 1656, predicted_correct': 1135, predicted_correct (%)': 68.53864734299518%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_316: Processing images: 100%|██████████| 1656/1656 [13:41<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_316. Total images: 1656, predicted_correct': 1123, predicted_correct (%)': 67.81400966183575%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_320: Processing images: 100%|██████████| 1656/1656 [13:42<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_320. Total images: 1656, predicted_correct': 1124, predicted_correct (%)': 67.8743961352657%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_324: Processing images: 100%|██████████| 1656/1656 [13:38<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_324. Total images: 1656, predicted_correct': 1147, predicted_correct (%)': 69.26328502415458%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_328: Processing images: 100%|██████████| 1656/1656 [13:42<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_328. Total images: 1656, predicted_correct': 1102, predicted_correct (%)': 66.54589371980676%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_332: Processing images: 100%|██████████| 1656/1656 [13:38<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_332. Total images: 1656, predicted_correct': 1131, predicted_correct (%)': 68.29710144927536%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_336: Processing images: 100%|██████████| 1656/1656 [13:42<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_336. Total images: 1656, predicted_correct': 1128, predicted_correct (%)': 68.11594202898551%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_340: Processing images: 100%|██████████| 1656/1656 [13:42<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_340. Total images: 1656, predicted_correct': 1119, predicted_correct (%)': 67.57246376811594%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_344: Processing images: 100%|██████████| 1656/1656 [13:41<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_344. Total images: 1656, predicted_correct': 1114, predicted_correct (%)': 67.27053140096618%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_348: Processing images: 100%|██████████| 1656/1656 [13:39<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_348. Total images: 1656, predicted_correct': 1133, predicted_correct (%)': 68.41787439613528%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_352: Processing images: 100%|██████████| 1656/1656 [13:41<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_352. Total images: 1656, predicted_correct': 1125, predicted_correct (%)': 67.93478260869566%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_356: Processing images: 100%|██████████| 1656/1656 [13:39<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_356. Total images: 1656, predicted_correct': 1141, predicted_correct (%)': 68.90096618357488%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_360: Processing images: 100%|██████████| 1656/1656 [13:40<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_360. Total images: 1656, predicted_correct': 1138, predicted_correct (%)': 68.71980676328504%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_364: Processing images: 100%|██████████| 1656/1656 [13:42<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_364. Total images: 1656, predicted_correct': 1145, predicted_correct (%)': 69.14251207729468%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_368: Processing images: 100%|██████████| 1656/1656 [13:39<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_368. Total images: 1656, predicted_correct': 1122, predicted_correct (%)': 67.7536231884058%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_372: Processing images: 100%|██████████| 1656/1656 [13:41<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_372. Total images: 1656, predicted_correct': 1148, predicted_correct (%)': 69.32367149758454%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_376: Processing images: 100%|██████████| 1656/1656 [13:38<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_376. Total images: 1656, predicted_correct': 1124, predicted_correct (%)': 67.8743961352657%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_380: Processing images: 100%|██████████| 1656/1656 [13:45<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_380. Total images: 1656, predicted_correct': 1134, predicted_correct (%)': 68.47826086956522%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_384: Processing images: 100%|██████████| 1656/1656 [13:42<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_384. Total images: 1656, predicted_correct': 1137, predicted_correct (%)': 68.65942028985508%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_388: Processing images: 100%|██████████| 1656/1656 [13:42<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_388. Total images: 1656, predicted_correct': 1138, predicted_correct (%)': 68.71980676328504%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_392: Processing images: 100%|██████████| 1656/1656 [13:43<00:00,  2.01image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_392. Total images: 1656, predicted_correct': 1127, predicted_correct (%)': 68.05555555555556%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch test for Direct Classification benchmark 6 (cropped square & data augmentation images, starting to normalize with mean and std)\n",
    "import torch, os, open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import ToTensor\n",
    "import albumentations as A\n",
    "\n",
    "caption_map = {\n",
    "    \"healthy\": \"healthy normal\",\n",
    "    \"doubtful\": \"doubtful osteoarthritis\",\n",
    "    \"minimal\": \"minimal osteoarthritis\",\n",
    "    \"moderate\": \"moderate osteoarthritis\",\n",
    "    \"severe\": \"severe osteoarthritis\"\n",
    "}\n",
    "\n",
    "classification = list(caption_map.values())\n",
    "\n",
    "def get_ground_truth(filename, caption_map):\n",
    "    for keyword, ground_truth in caption_map.items():\n",
    "        if keyword in filename:\n",
    "            return ground_truth\n",
    "    return None\n",
    "\n",
    "def process_single_epoch(pretrained_model, sentences):\n",
    "    img_dir = '/mnt/g/Datasets/Knee_OA/Original/test/images/'\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        model_name=\"coca_ViT-L-14\",\n",
    "        pretrained=pretrained_model,\n",
    "        image_mean = (0.60623126, 0.60623126, 0.60623126),\n",
    "        image_std = (0.19362484, 0.19362484, 0.19362484),\n",
    "        # color_image = False,\n",
    "    )\n",
    "    pt_dir = os.path.dirname(pretrained_model)\n",
    "    epoch_number = os.path.basename(pretrained_model).split('epoch_')[1].split('.pt')[0]\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    # Prepare the tokenizer for sentences\n",
    "    tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "    text = tokenizer(sentences)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def read_single_image(img_dir, img_filename, sentences, text_features):\n",
    "        #get the ground truth value\n",
    "        image_path = os.path.join(img_dir, img_filename)\n",
    "        ground_truth = get_ground_truth(img_filename, caption_map)\n",
    "        \n",
    "        # load a sample image\n",
    "        image = preprocess(Image.open(image_path).convert('RGB')).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            text_probs = text_probs.cpu().tolist()[0]\n",
    "\n",
    "        # Construct the dictionary\n",
    "        class_predict_dict = dict(zip(sentences, text_probs))\n",
    "        # Extract the key with the largest value\n",
    "        class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "        \n",
    "        if ground_truth == class_predict:\n",
    "            predicted_correct = 1\n",
    "        else:\n",
    "            predicted_correct = 0\n",
    "        \n",
    "        pred_dict = {'img_filename': [img_filename], 'class_predict_dict': [class_predict_dict], 'ground_truth': [ground_truth], 'class_predict': [class_predict], 'predicted_correct': [predicted_correct]}\n",
    "        df_single = pd.DataFrame(pred_dict)\n",
    "        \n",
    "        return df_single\n",
    "\n",
    "    all_images = os.listdir(img_dir)\n",
    "    for img_filename in tqdm(all_images, desc=f\"epoch_{epoch_number}: Processing images\", unit=\"image\"):\n",
    "        df = read_single_image(img_dir, img_filename, sentences, text_features)\n",
    "        df_all = pd.concat([df_all, df]).reset_index(drop=True)\n",
    "\n",
    "    #save df_all to csv\n",
    "    csv_file = f\"/home/yilu/Development/open_clip/Knee_OA/temp2/epoch_{epoch_number}.csv\"\n",
    "    df_all.to_csv(csv_file, index=False)\n",
    "\n",
    "    total_rows = len(df_all)\n",
    "    # Count the occurrences of '1' in the 'predicted_correct' column\n",
    "    count_ones = df_all['predicted_correct'].sum()\n",
    "    epoch_filename = os.path.splitext(os.path.basename(pretrained_model))[0]\n",
    "    predicted_correct_pct = count_ones /total_rows *100\n",
    "    print(f\"Epoch_file: {epoch_filename}. Total images: {total_rows}, predicted_correct': {count_ones}, predicted_correct (%)': {predicted_correct_pct}%\")\n",
    "    \n",
    "    epoch_dict = {'pt_dir': [pt_dir],\n",
    "                'epoch_filename': [epoch_filename], \n",
    "                'sentences': [sentences], \n",
    "                'total_rows': [total_rows], \n",
    "                'predicted_correct': [count_ones], \n",
    "                'predicted_correct (%)': [predicted_correct_pct]}\n",
    "    df_single_epoch = pd.DataFrame(epoch_dict)\n",
    "    return df_single_epoch\n",
    "\n",
    "\n",
    "def process_single_pretrained_model(pretrained_model, csv_file):\n",
    "    df_single_pretrained_model = pd.DataFrame()\n",
    "    df = process_single_epoch(pretrained_model, classification)\n",
    "    df_single_pretrained_model = pd.concat([df_single_pretrained_model, df]).reset_index(drop=True)\n",
    "    save_to_csv(df_single_pretrained_model, csv_file)\n",
    "    \n",
    "def extract_epoch_num(filepath):\n",
    "    # Split the filename from the path\n",
    "    filename = os.path.basename(filepath) #filepath.split('/')[-1]\n",
    "    # Extract the number between \"epoch_\" and \".pt\"\n",
    "    epoch_num = int(filename.split('epoch_')[1].split('.pt')[0])\n",
    "    return epoch_num\n",
    "\n",
    "def save_to_csv(df, csv_file):\n",
    "    if not os.path.exists(csv_file):\n",
    "        df.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # If the CSV file already exists, append without header\n",
    "        df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "def main():\n",
    "    #obtain the latest pretrained model folder\n",
    "    pt_folder_root = '/mnt/g/Logtemp/open_clip/Knee_OA/'\n",
    "    dir_1 = [a for a in os.listdir(pt_folder_root) if a.startswith('2023_08_27-00')][0]\n",
    "    pt_folder_path = os.path.join(pt_folder_root, dir_1, 'checkpoints')\n",
    "\n",
    "    pt_files = [os.path.join(pt_folder_path, filename) for filename in os.listdir(pt_folder_path) if filename.endswith('.pt')]\n",
    "    #filter pt_files with low epoch number\n",
    "    pt_files = [pt_file for pt_file in pt_files if extract_epoch_num(pt_file) >= 112]\n",
    "    # Sort the list\n",
    "    sorted_pt_files = sorted(pt_files, key=extract_epoch_num)\n",
    "    csv_file = \"/home/yilu/Development/open_clip/Knee_OA/temp2/benchmark_all_epochs.csv\"\n",
    "    for pretrained_model in sorted_pt_files:\n",
    "        process_single_pretrained_model(pretrained_model, csv_file)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_400: Processing images: 100%|██████████| 1656/1656 [13:36<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_400. Total images: 1656, predicted_correct': 1141, predicted_correct (%)': 68.90096618357488%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_396: Processing images: 100%|██████████| 1656/1656 [13:27<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_396. Total images: 1656, predicted_correct': 1130, predicted_correct (%)': 68.23671497584542%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_392: Processing images: 100%|██████████| 1656/1656 [13:46<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_392. Total images: 1656, predicted_correct': 1134, predicted_correct (%)': 68.47826086956522%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_388: Processing images: 100%|██████████| 1656/1656 [13:47<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_388. Total images: 1656, predicted_correct': 1138, predicted_correct (%)': 68.71980676328504%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_384: Processing images: 100%|██████████| 1656/1656 [13:51<00:00,  1.99image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_384. Total images: 1656, predicted_correct': 1135, predicted_correct (%)': 68.53864734299518%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_380: Processing images: 100%|██████████| 1656/1656 [14:20<00:00,  1.92image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_380. Total images: 1656, predicted_correct': 1144, predicted_correct (%)': 69.08212560386472%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_376: Processing images: 100%|██████████| 1656/1656 [13:50<00:00,  1.99image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_376. Total images: 1656, predicted_correct': 1117, predicted_correct (%)': 67.45169082125604%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_372: Processing images: 100%|██████████| 1656/1656 [26:06<00:00,  1.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_372. Total images: 1656, predicted_correct': 1130, predicted_correct (%)': 68.23671497584542%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_368: Processing images: 100%|██████████| 1656/1656 [13:58<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_368. Total images: 1656, predicted_correct': 1135, predicted_correct (%)': 68.53864734299518%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_364: Processing images: 100%|██████████| 1656/1656 [13:19<00:00,  2.07image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_364. Total images: 1656, predicted_correct': 1134, predicted_correct (%)': 68.47826086956522%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_360: Processing images: 100%|██████████| 1656/1656 [13:15<00:00,  2.08image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_360. Total images: 1656, predicted_correct': 1139, predicted_correct (%)': 68.78019323671496%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_356: Processing images: 100%|██████████| 1656/1656 [13:02<00:00,  2.12image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_356. Total images: 1656, predicted_correct': 1142, predicted_correct (%)': 68.96135265700482%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_352: Processing images: 100%|██████████| 1656/1656 [13:37<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_352. Total images: 1656, predicted_correct': 1136, predicted_correct (%)': 68.59903381642512%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_348: Processing images: 100%|██████████| 1656/1656 [14:05<00:00,  1.96image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_348. Total images: 1656, predicted_correct': 1123, predicted_correct (%)': 67.81400966183575%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_344: Processing images: 100%|██████████| 1656/1656 [13:29<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_344. Total images: 1656, predicted_correct': 1133, predicted_correct (%)': 68.41787439613528%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_340: Processing images: 100%|██████████| 1656/1656 [13:37<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_340. Total images: 1656, predicted_correct': 1159, predicted_correct (%)': 69.987922705314%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_336: Processing images: 100%|██████████| 1656/1656 [13:02<00:00,  2.12image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_336. Total images: 1656, predicted_correct': 1123, predicted_correct (%)': 67.81400966183575%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_332: Processing images: 100%|██████████| 1656/1656 [12:48<00:00,  2.15image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_332. Total images: 1656, predicted_correct': 1146, predicted_correct (%)': 69.20289855072464%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_328: Processing images: 100%|██████████| 1656/1656 [12:33<00:00,  2.20image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_328. Total images: 1656, predicted_correct': 1110, predicted_correct (%)': 67.02898550724638%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_324: Processing images: 100%|██████████| 1656/1656 [13:22<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_324. Total images: 1656, predicted_correct': 1133, predicted_correct (%)': 68.41787439613528%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_320: Processing images: 100%|██████████| 1656/1656 [13:29<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_320. Total images: 1656, predicted_correct': 1141, predicted_correct (%)': 68.90096618357488%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_316: Processing images: 100%|██████████| 1656/1656 [13:30<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_316. Total images: 1656, predicted_correct': 1145, predicted_correct (%)': 69.14251207729468%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_312: Processing images: 100%|██████████| 1656/1656 [13:27<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_312. Total images: 1656, predicted_correct': 1120, predicted_correct (%)': 67.6328502415459%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_308: Processing images: 100%|██████████| 1656/1656 [12:50<00:00,  2.15image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_308. Total images: 1656, predicted_correct': 1136, predicted_correct (%)': 68.59903381642512%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_304: Processing images: 100%|██████████| 1656/1656 [12:37<00:00,  2.19image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_304. Total images: 1656, predicted_correct': 1132, predicted_correct (%)': 68.35748792270532%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_300: Processing images: 100%|██████████| 1656/1656 [12:38<00:00,  2.18image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_300. Total images: 1656, predicted_correct': 1118, predicted_correct (%)': 67.512077294686%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_296: Processing images: 100%|██████████| 1656/1656 [12:45<00:00,  2.16image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_296. Total images: 1656, predicted_correct': 1145, predicted_correct (%)': 69.14251207729468%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_292: Processing images: 100%|██████████| 1656/1656 [12:40<00:00,  2.18image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_292. Total images: 1656, predicted_correct': 1141, predicted_correct (%)': 68.90096618357488%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_288: Processing images: 100%|██████████| 1656/1656 [13:06<00:00,  2.11image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_288. Total images: 1656, predicted_correct': 1145, predicted_correct (%)': 69.14251207729468%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_284: Processing images: 100%|██████████| 1656/1656 [13:25<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_284. Total images: 1656, predicted_correct': 1142, predicted_correct (%)': 68.96135265700482%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_280: Processing images: 100%|██████████| 1656/1656 [13:27<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_280. Total images: 1656, predicted_correct': 1127, predicted_correct (%)': 68.05555555555556%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_276: Processing images: 100%|██████████| 1656/1656 [13:25<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_276. Total images: 1656, predicted_correct': 1146, predicted_correct (%)': 69.20289855072464%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_272: Processing images: 100%|██████████| 1656/1656 [13:17<00:00,  2.08image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_272. Total images: 1656, predicted_correct': 1152, predicted_correct (%)': 69.56521739130434%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_268: Processing images: 100%|██████████| 1656/1656 [12:39<00:00,  2.18image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_268. Total images: 1656, predicted_correct': 1153, predicted_correct (%)': 69.6256038647343%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_264: Processing images: 100%|██████████| 1656/1656 [12:50<00:00,  2.15image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_264. Total images: 1656, predicted_correct': 1140, predicted_correct (%)': 68.84057971014492%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_260: Processing images: 100%|██████████| 1656/1656 [12:43<00:00,  2.17image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_260. Total images: 1656, predicted_correct': 1118, predicted_correct (%)': 67.512077294686%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_256: Processing images: 100%|██████████| 1656/1656 [12:44<00:00,  2.17image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_256. Total images: 1656, predicted_correct': 1111, predicted_correct (%)': 67.08937198067633%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_252: Processing images: 100%|██████████| 1656/1656 [13:19<00:00,  2.07image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_252. Total images: 1656, predicted_correct': 1088, predicted_correct (%)': 65.70048309178745%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_248: Processing images: 100%|██████████| 1656/1656 [13:27<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_248. Total images: 1656, predicted_correct': 1118, predicted_correct (%)': 67.512077294686%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_244: Processing images: 100%|██████████| 1656/1656 [13:23<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_244. Total images: 1656, predicted_correct': 1119, predicted_correct (%)': 67.57246376811594%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_240: Processing images: 100%|██████████| 1656/1656 [13:25<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_240. Total images: 1656, predicted_correct': 1106, predicted_correct (%)': 66.78743961352657%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_236: Processing images: 100%|██████████| 1656/1656 [13:09<00:00,  2.10image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_236. Total images: 1656, predicted_correct': 1119, predicted_correct (%)': 67.57246376811594%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_232: Processing images: 100%|██████████| 1656/1656 [12:47<00:00,  2.16image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_232. Total images: 1656, predicted_correct': 1138, predicted_correct (%)': 68.71980676328504%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_228: Processing images: 100%|██████████| 1656/1656 [12:42<00:00,  2.17image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_228. Total images: 1656, predicted_correct': 1135, predicted_correct (%)': 68.53864734299518%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_224: Processing images: 100%|██████████| 1656/1656 [12:41<00:00,  2.17image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_224. Total images: 1656, predicted_correct': 1137, predicted_correct (%)': 68.65942028985508%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_220: Processing images: 100%|██████████| 1656/1656 [12:40<00:00,  2.18image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_220. Total images: 1656, predicted_correct': 1135, predicted_correct (%)': 68.53864734299518%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_216: Processing images: 100%|██████████| 1656/1656 [13:28<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_216. Total images: 1656, predicted_correct': 1125, predicted_correct (%)': 67.93478260869566%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_212: Processing images: 100%|██████████| 1656/1656 [13:34<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_212. Total images: 1656, predicted_correct': 1141, predicted_correct (%)': 68.90096618357488%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_208: Processing images: 100%|██████████| 1656/1656 [13:37<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_208. Total images: 1656, predicted_correct': 1138, predicted_correct (%)': 68.71980676328504%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_204: Processing images: 100%|██████████| 1656/1656 [13:24<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_204. Total images: 1656, predicted_correct': 1116, predicted_correct (%)': 67.3913043478261%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_200: Processing images: 100%|██████████| 1656/1656 [13:05<00:00,  2.11image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_200. Total images: 1656, predicted_correct': 1122, predicted_correct (%)': 67.7536231884058%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_196: Processing images: 100%|██████████| 1656/1656 [13:03<00:00,  2.11image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_196. Total images: 1656, predicted_correct': 1151, predicted_correct (%)': 69.5048309178744%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_192: Processing images: 100%|██████████| 1656/1656 [12:49<00:00,  2.15image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_192. Total images: 1656, predicted_correct': 1134, predicted_correct (%)': 68.47826086956522%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_188: Processing images: 100%|██████████| 1656/1656 [12:41<00:00,  2.18image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_188. Total images: 1656, predicted_correct': 1132, predicted_correct (%)': 68.35748792270532%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_184: Processing images: 100%|██████████| 1656/1656 [12:50<00:00,  2.15image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_184. Total images: 1656, predicted_correct': 1117, predicted_correct (%)': 67.45169082125604%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_180: Processing images: 100%|██████████| 1656/1656 [12:52<00:00,  2.14image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_180. Total images: 1656, predicted_correct': 1140, predicted_correct (%)': 68.84057971014492%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_176: Processing images: 100%|██████████| 1656/1656 [13:18<00:00,  2.07image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_176. Total images: 1656, predicted_correct': 1120, predicted_correct (%)': 67.6328502415459%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_172: Processing images: 100%|██████████| 1656/1656 [13:25<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_172. Total images: 1656, predicted_correct': 1114, predicted_correct (%)': 67.27053140096618%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_168: Processing images: 100%|██████████| 1656/1656 [13:29<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_168. Total images: 1656, predicted_correct': 1133, predicted_correct (%)': 68.41787439613528%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_164: Processing images: 100%|██████████| 1656/1656 [13:27<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_164. Total images: 1656, predicted_correct': 1106, predicted_correct (%)': 66.78743961352657%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_160: Processing images: 100%|██████████| 1656/1656 [13:26<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_160. Total images: 1656, predicted_correct': 1099, predicted_correct (%)': 66.3647342995169%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_156: Processing images: 100%|██████████| 1656/1656 [13:02<00:00,  2.12image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_156. Total images: 1656, predicted_correct': 1103, predicted_correct (%)': 66.60628019323671%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_152: Processing images: 100%|██████████| 1656/1656 [12:41<00:00,  2.17image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_152. Total images: 1656, predicted_correct': 1128, predicted_correct (%)': 68.11594202898551%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_148: Processing images: 100%|██████████| 1656/1656 [13:00<00:00,  2.12image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_148. Total images: 1656, predicted_correct': 1128, predicted_correct (%)': 68.11594202898551%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_144: Processing images: 100%|██████████| 1656/1656 [13:19<00:00,  2.07image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_144. Total images: 1656, predicted_correct': 1096, predicted_correct (%)': 66.18357487922705%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_140: Processing images: 100%|██████████| 1656/1656 [13:19<00:00,  2.07image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_140. Total images: 1656, predicted_correct': 1097, predicted_correct (%)': 66.24396135265701%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_136: Processing images: 100%|██████████| 1656/1656 [13:25<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_136. Total images: 1656, predicted_correct': 1122, predicted_correct (%)': 67.7536231884058%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_132: Processing images: 100%|██████████| 1656/1656 [13:18<00:00,  2.07image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_132. Total images: 1656, predicted_correct': 1136, predicted_correct (%)': 68.59903381642512%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_128: Processing images: 100%|██████████| 1656/1656 [12:50<00:00,  2.15image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_128. Total images: 1656, predicted_correct': 1115, predicted_correct (%)': 67.33091787439614%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_124: Processing images: 100%|██████████| 1656/1656 [12:38<00:00,  2.18image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_124. Total images: 1656, predicted_correct': 1107, predicted_correct (%)': 66.84782608695652%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_120: Processing images: 100%|██████████| 1656/1656 [12:48<00:00,  2.16image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_120. Total images: 1656, predicted_correct': 1103, predicted_correct (%)': 66.60628019323671%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_116: Processing images: 100%|██████████| 1656/1656 [12:54<00:00,  2.14image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_116. Total images: 1656, predicted_correct': 1122, predicted_correct (%)': 67.7536231884058%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_112: Processing images: 100%|██████████| 1656/1656 [13:21<00:00,  2.07image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_112. Total images: 1656, predicted_correct': 1117, predicted_correct (%)': 67.45169082125604%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_108: Processing images: 100%|██████████| 1656/1656 [13:27<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_108. Total images: 1656, predicted_correct': 1137, predicted_correct (%)': 68.65942028985508%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_104: Processing images: 100%|██████████| 1656/1656 [13:23<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_104. Total images: 1656, predicted_correct': 1060, predicted_correct (%)': 64.00966183574879%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_100: Processing images: 100%|██████████| 1656/1656 [13:29<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_100. Total images: 1656, predicted_correct': 1103, predicted_correct (%)': 66.60628019323671%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_96: Processing images: 100%|██████████| 1656/1656 [13:11<00:00,  2.09image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_96. Total images: 1656, predicted_correct': 1115, predicted_correct (%)': 67.33091787439614%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_92: Processing images: 100%|██████████| 1656/1656 [12:43<00:00,  2.17image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_92. Total images: 1656, predicted_correct': 1073, predicted_correct (%)': 64.79468599033817%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_88: Processing images: 100%|██████████| 1656/1656 [12:51<00:00,  2.15image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_88. Total images: 1656, predicted_correct': 1139, predicted_correct (%)': 68.78019323671496%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_84: Processing images: 100%|██████████| 1656/1656 [12:46<00:00,  2.16image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_84. Total images: 1656, predicted_correct': 1086, predicted_correct (%)': 65.57971014492753%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_80: Processing images: 100%|██████████| 1656/1656 [12:43<00:00,  2.17image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_80. Total images: 1656, predicted_correct': 1113, predicted_correct (%)': 67.21014492753623%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_76: Processing images: 100%|██████████| 1656/1656 [13:25<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_76. Total images: 1656, predicted_correct': 1118, predicted_correct (%)': 67.512077294686%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_72: Processing images: 100%|██████████| 1656/1656 [13:29<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_72. Total images: 1656, predicted_correct': 1101, predicted_correct (%)': 66.48550724637681%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_68: Processing images: 100%|██████████| 1656/1656 [13:33<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_68. Total images: 1656, predicted_correct': 1059, predicted_correct (%)': 63.949275362318836%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_64: Processing images: 100%|██████████| 1656/1656 [13:37<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_64. Total images: 1656, predicted_correct': 1081, predicted_correct (%)': 65.27777777777779%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_60: Processing images: 100%|██████████| 1656/1656 [12:57<00:00,  2.13image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_60. Total images: 1656, predicted_correct': 1085, predicted_correct (%)': 65.51932367149759%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_56: Processing images: 100%|██████████| 1656/1656 [12:43<00:00,  2.17image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_56. Total images: 1656, predicted_correct': 1131, predicted_correct (%)': 68.29710144927536%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_52: Processing images: 100%|██████████| 1656/1656 [12:42<00:00,  2.17image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_52. Total images: 1656, predicted_correct': 1114, predicted_correct (%)': 67.27053140096618%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_48: Processing images: 100%|██████████| 1656/1656 [12:33<00:00,  2.20image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_48. Total images: 1656, predicted_correct': 1112, predicted_correct (%)': 67.14975845410628%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_44: Processing images: 100%|██████████| 1656/1656 [12:52<00:00,  2.14image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_44. Total images: 1656, predicted_correct': 1113, predicted_correct (%)': 67.21014492753623%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_40: Processing images: 100%|██████████| 1656/1656 [13:31<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_40. Total images: 1656, predicted_correct': 1047, predicted_correct (%)': 63.22463768115942%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_36: Processing images: 100%|██████████| 1656/1656 [13:29<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_36. Total images: 1656, predicted_correct': 1060, predicted_correct (%)': 64.00966183574879%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_32: Processing images: 100%|██████████| 1656/1656 [13:27<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_32. Total images: 1656, predicted_correct': 1091, predicted_correct (%)': 65.88164251207729%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_28: Processing images: 100%|██████████| 1656/1656 [13:32<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_28. Total images: 1656, predicted_correct': 1083, predicted_correct (%)': 65.39855072463769%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_24: Processing images: 100%|██████████| 1656/1656 [12:49<00:00,  2.15image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_24. Total images: 1656, predicted_correct': 1047, predicted_correct (%)': 63.22463768115942%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_20: Processing images: 100%|██████████| 1656/1656 [12:34<00:00,  2.20image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_20. Total images: 1656, predicted_correct': 1069, predicted_correct (%)': 64.55314009661835%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_16: Processing images: 100%|██████████| 1656/1656 [12:48<00:00,  2.15image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_16. Total images: 1656, predicted_correct': 1004, predicted_correct (%)': 60.6280193236715%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_12: Processing images: 100%|██████████| 1656/1656 [13:13<00:00,  2.09image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_12. Total images: 1656, predicted_correct': 1047, predicted_correct (%)': 63.22463768115942%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_8: Processing images: 100%|██████████| 1656/1656 [13:33<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_8. Total images: 1656, predicted_correct': 891, predicted_correct (%)': 53.80434782608695%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_4: Processing images: 100%|██████████| 1656/1656 [13:53<00:00,  1.99image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_4. Total images: 1656, predicted_correct': 687, predicted_correct (%)': 41.48550724637681%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch test for Direct Classification benchmark 6 (cropped square & data augmentation images, starting to normalize with mean and std, no resizing)\n",
    "import torch, os, open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import ToTensor\n",
    "import albumentations as A\n",
    "\n",
    "caption_map = {\n",
    "    \"healthy\": \"healthy normal\",\n",
    "    \"doubtful\": \"doubtful osteoarthritis\",\n",
    "    \"minimal\": \"minimal osteoarthritis\",\n",
    "    \"moderate\": \"moderate osteoarthritis\",\n",
    "    \"severe\": \"severe osteoarthritis\"\n",
    "}\n",
    "\n",
    "classification = list(caption_map.values())\n",
    "\n",
    "def get_ground_truth(filename, caption_map):\n",
    "    for keyword, ground_truth in caption_map.items():\n",
    "        if keyword in filename:\n",
    "            return ground_truth\n",
    "    return None\n",
    "\n",
    "def process_single_epoch(pretrained_model, sentences):\n",
    "    img_dir = '/mnt/g/Datasets/Knee_OA/Original/test/images/'\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        model_name=\"coca_ViT-L-14\",\n",
    "        pretrained=pretrained_model,\n",
    "        image_mean = (0.60623126, 0.60623126, 0.60623126),\n",
    "        image_std = (0.19362484, 0.19362484, 0.19362484),\n",
    "        # color_image = False,\n",
    "    )\n",
    "    pt_dir = os.path.dirname(pretrained_model)\n",
    "    epoch_number = os.path.basename(pretrained_model).split('epoch_')[1].split('.pt')[0]\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    # Prepare the tokenizer for sentences\n",
    "    tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "    text = tokenizer(sentences)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def read_single_image(img_dir, img_filename, sentences, text_features):\n",
    "        #get the ground truth value\n",
    "        image_path = os.path.join(img_dir, img_filename)\n",
    "        ground_truth = get_ground_truth(img_filename, caption_map)\n",
    "        \n",
    "        # load a sample image\n",
    "        image = preprocess(Image.open(image_path).convert('RGB')).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            text_probs = text_probs.cpu().tolist()[0]\n",
    "\n",
    "        # Construct the dictionary\n",
    "        class_predict_dict = dict(zip(sentences, text_probs))\n",
    "        # Extract the key with the largest value\n",
    "        class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "        \n",
    "        if ground_truth == class_predict:\n",
    "            predicted_correct = 1\n",
    "        else:\n",
    "            predicted_correct = 0\n",
    "        \n",
    "        pred_dict = {'img_filename': [img_filename], 'class_predict_dict': [class_predict_dict], 'ground_truth': [ground_truth], 'class_predict': [class_predict], 'predicted_correct': [predicted_correct]}\n",
    "        df_single = pd.DataFrame(pred_dict)\n",
    "        \n",
    "        return df_single\n",
    "\n",
    "    all_images = os.listdir(img_dir)\n",
    "    for img_filename in tqdm(all_images, desc=f\"epoch_{epoch_number}: Processing images\", unit=\"image\"):\n",
    "        df = read_single_image(img_dir, img_filename, sentences, text_features)\n",
    "        df_all = pd.concat([df_all, df]).reset_index(drop=True)\n",
    "\n",
    "    #save df_all to csv\n",
    "    csv_file = f\"/home/yilu/Development/open_clip/Knee_OA/temp3/epoch_{epoch_number}.csv\"\n",
    "    df_all.to_csv(csv_file, index=False)\n",
    "\n",
    "    total_rows = len(df_all)\n",
    "    # Count the occurrences of '1' in the 'predicted_correct' column\n",
    "    count_ones = df_all['predicted_correct'].sum()\n",
    "    epoch_filename = os.path.splitext(os.path.basename(pretrained_model))[0]\n",
    "    predicted_correct_pct = count_ones /total_rows *100\n",
    "    print(f\"Epoch_file: {epoch_filename}. Total images: {total_rows}, predicted_correct': {count_ones}, predicted_correct (%)': {predicted_correct_pct}%\")\n",
    "    \n",
    "    epoch_dict = {'pt_dir': [pt_dir],\n",
    "                'epoch_filename': [epoch_filename], \n",
    "                'sentences': [sentences], \n",
    "                'total_rows': [total_rows], \n",
    "                'predicted_correct': [count_ones], \n",
    "                'predicted_correct (%)': [predicted_correct_pct]}\n",
    "    df_single_epoch = pd.DataFrame(epoch_dict)\n",
    "    return df_single_epoch\n",
    "\n",
    "\n",
    "def process_single_pretrained_model(pretrained_model, csv_file):\n",
    "    df_single_pretrained_model = pd.DataFrame()\n",
    "    df = process_single_epoch(pretrained_model, classification)\n",
    "    df_single_pretrained_model = pd.concat([df_single_pretrained_model, df]).reset_index(drop=True)\n",
    "    save_to_csv(df_single_pretrained_model, csv_file)\n",
    "    \n",
    "def extract_epoch_num(filepath):\n",
    "    # Split the filename from the path\n",
    "    filename = os.path.basename(filepath) #filepath.split('/')[-1]\n",
    "    # Extract the number between \"epoch_\" and \".pt\"\n",
    "    epoch_num = int(filename.split('epoch_')[1].split('.pt')[0])\n",
    "    return epoch_num\n",
    "\n",
    "def save_to_csv(df, csv_file):\n",
    "    if not os.path.exists(csv_file):\n",
    "        df.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # If the CSV file already exists, append without header\n",
    "        df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "def main():\n",
    "    #obtain the latest pretrained model folder\n",
    "    pt_folder_root = '/mnt/g/Logtemp/open_clip/Knee_OA/'\n",
    "    dir_1 = [a for a in os.listdir(pt_folder_root) if a.startswith('2023_08_27-21')][0]\n",
    "    pt_folder_path = os.path.join(pt_folder_root, dir_1, 'checkpoints')\n",
    "\n",
    "    pt_files = [os.path.join(pt_folder_path, filename) for filename in os.listdir(pt_folder_path) if filename.endswith('.pt')]\n",
    "    #filter pt_files with low epoch number\n",
    "    pt_files = [pt_file for pt_file in pt_files if extract_epoch_num(pt_file) >= 0]\n",
    "    # Sort the list\n",
    "    sorted_pt_files = sorted(pt_files, key=extract_epoch_num, reverse=True)\n",
    "    csv_file = \"/home/yilu/Development/open_clip/Knee_OA/temp3/benchmark_all_epochs.csv\"\n",
    "    for pretrained_model in sorted_pt_files:\n",
    "        process_single_pretrained_model(pretrained_model, csv_file)\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
