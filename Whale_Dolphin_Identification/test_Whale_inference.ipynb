{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brydes whale',\n",
       " 'globis',\n",
       " 'long finned pilot whale',\n",
       " 'fin whale',\n",
       " 'spotted dolphin',\n",
       " 'frasiers dolphin',\n",
       " 'pilot whale',\n",
       " 'blue whale',\n",
       " 'beluga',\n",
       " 'rough toothed dolphin',\n",
       " 'cuviers beaked whale',\n",
       " 'killer whale',\n",
       " 'melon headed whale',\n",
       " 'bottlenose dolphin',\n",
       " 'gray whale',\n",
       " 'spinner dolphin',\n",
       " 'common dolphin',\n",
       " 'white sided dolphin',\n",
       " 'sei whale',\n",
       " 'southern right whale',\n",
       " 'short finned pilot whale',\n",
       " 'pygmy killer whale',\n",
       " 'pantropic spotted dolphin',\n",
       " 'commersons dolphin',\n",
       " 'false killer whale',\n",
       " 'bottlenose dolpin',\n",
       " 'dusky dolphin',\n",
       " 'minke whale',\n",
       " 'humpback whale',\n",
       " 'kiler whale']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_species(file_path):\n",
    "    species = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            columns = line.strip().split('\\t')\n",
    "            if len(columns) > 1:\n",
    "                species.append(columns[1])\n",
    "    species = list(set(species))\n",
    "    return species\n",
    "\n",
    "caption_path = '/mnt/g/Datasets/Whale_Dolphin_Identification/Square/test/captions.txt'\n",
    "species = extract_species(caption_path)\n",
    "\n",
    "species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yilu/miniconda3/envs/openclip/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[2.4179e-05, 3.1715e-02, 1.0379e-03, 6.5446e-06, 4.6195e-05, 2.4105e-05,\n",
      "         7.0505e-06, 2.9267e-08, 2.7521e-07, 4.8915e-05, 7.4288e-05, 7.7751e-06,\n",
      "         1.3812e-04, 3.5976e-05, 5.6864e-08, 3.4625e-05, 2.3820e-04, 3.1189e-07,\n",
      "         9.3151e-06, 1.7222e-06, 9.1095e-07, 1.0159e-03, 4.8858e-03, 2.0357e-04,\n",
      "         9.6033e-01, 5.9075e-06, 3.7162e-05, 1.4038e-05, 7.8107e-06, 4.4372e-05]])\n",
      "Ground truth: false killer whale\n",
      "Predicted class: brydes whale\n",
      "Predicted correct: 0\n"
     ]
    }
   ],
   "source": [
    "#classification\n",
    "\n",
    "import torch, os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import open_clip\n",
    "\n",
    "pretrained = '/mnt/g/Logtemp/open_clip/Whale_Dolphin_Identification/2023_08_18-21_14_32-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints/epoch_16.pt'\n",
    "# model, _, preprocess = open_clip.create_model_and_transforms('coca_ViT-L-14', pretrained='mscoco_finetuned_laion2B-s13B-b90k')\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "  model_name=\"coca_ViT-L-14\",\n",
    "  pretrained=pretrained, #mscoco_finetuned_laion2B-s13B-b90k\n",
    ")\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "\n",
    "image_path = '/mnt/g/Datasets/Whale_Dolphin_Identification/Square/test/images/false_killer_whale_bdea86a4d11fa9.jpg'\n",
    "img = Image.open(image_path).convert('RGB')\n",
    "img = img.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "image = preprocess(img).unsqueeze(0)\n",
    "text = tokenizer(species)\n",
    "\n",
    "filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "ground_truth = \"_\".join(filename.split('_')[:-1]).replace(\"_\", \" \")\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    # Construct the dictionary\n",
    "    class_predict_dict = dict(zip(species, text_probs))\n",
    "    # Extract the key with the largest value\n",
    "    class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "    \n",
    "    if ground_truth == class_predict:\n",
    "        predicted_correct = 1\n",
    "    else:\n",
    "        predicted_correct = 0\n",
    "\n",
    "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]\n",
    "print(\"Ground truth:\", ground_truth)  # prints: 0\n",
    "print(\"Predicted class:\", class_predict)  # prints: 0\n",
    "print(\"Predicted correct:\", predicted_correct)  # prints: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_41: Processing images: 100%|██████████| 5115/5115 [43:23<00:00,  1.96image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_41. Total images: 5115, predicted_correct': 5015, predicted_correct (%)': 98.04496578690127%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_42: Processing images: 100%|██████████| 5115/5115 [40:53<00:00,  2.09image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_42. Total images: 5115, predicted_correct': 5024, predicted_correct (%)': 98.22091886608015%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_43: Processing images: 100%|██████████| 5115/5115 [38:29<00:00,  2.21image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_43. Total images: 5115, predicted_correct': 5033, predicted_correct (%)': 98.39687194525905%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_44: Processing images: 100%|██████████| 5115/5115 [38:21<00:00,  2.22image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_44. Total images: 5115, predicted_correct': 5016, predicted_correct (%)': 98.06451612903226%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_45: Processing images: 100%|██████████| 5115/5115 [38:26<00:00,  2.22image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_45. Total images: 5115, predicted_correct': 5022, predicted_correct (%)': 98.18181818181819%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_46: Processing images: 100%|██████████| 5115/5115 [38:16<00:00,  2.23image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_46. Total images: 5115, predicted_correct': 5030, predicted_correct (%)': 98.33822091886609%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_47: Processing images: 100%|██████████| 5115/5115 [38:16<00:00,  2.23image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_47. Total images: 5115, predicted_correct': 5024, predicted_correct (%)': 98.22091886608015%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_48: Processing images: 100%|██████████| 5115/5115 [41:40<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_48. Total images: 5115, predicted_correct': 5033, predicted_correct (%)': 98.39687194525905%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_49: Processing images: 100%|██████████| 5115/5115 [39:21<00:00,  2.17image/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_49. Total images: 5115, predicted_correct': 5031, predicted_correct (%)': 98.35777126099707%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_50: Processing images: 100%|██████████| 5115/5115 [41:13<00:00,  2.07image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_50. Total images: 5115, predicted_correct': 5020, predicted_correct (%)': 98.1427174975562%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_51: Processing images: 100%|██████████| 5115/5115 [39:35<00:00,  2.15image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_51. Total images: 5115, predicted_correct': 5027, predicted_correct (%)': 98.27956989247312%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_52: Processing images: 100%|██████████| 5115/5115 [39:50<00:00,  2.14image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_52. Total images: 5115, predicted_correct': 5022, predicted_correct (%)': 98.18181818181819%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_53: Processing images: 100%|██████████| 5115/5115 [42:10<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_53. Total images: 5115, predicted_correct': 5028, predicted_correct (%)': 98.2991202346041%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_54: Processing images: 100%|██████████| 5115/5115 [40:03<00:00,  2.13image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_54. Total images: 5115, predicted_correct': 5030, predicted_correct (%)': 98.33822091886609%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_55: Processing images: 100%|██████████| 5115/5115 [39:04<00:00,  2.18image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_55. Total images: 5115, predicted_correct': 5028, predicted_correct (%)': 98.2991202346041%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_56: Processing images: 100%|██████████| 5115/5115 [40:33<00:00,  2.10image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_56. Total images: 5115, predicted_correct': 5031, predicted_correct (%)': 98.35777126099707%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_57: Processing images: 100%|██████████| 5115/5115 [38:48<00:00,  2.20image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_57. Total images: 5115, predicted_correct': 5026, predicted_correct (%)': 98.26001955034212%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_58: Processing images: 100%|██████████| 5115/5115 [38:46<00:00,  2.20image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_58. Total images: 5115, predicted_correct': 5034, predicted_correct (%)': 98.41642228739002%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_59: Processing images: 100%|██████████| 5115/5115 [38:50<00:00,  2.19image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_59. Total images: 5115, predicted_correct': 5029, predicted_correct (%)': 98.31867057673509%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_60: Processing images: 100%|██████████| 5115/5115 [38:47<00:00,  2.20image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_60. Total images: 5115, predicted_correct': 5027, predicted_correct (%)': 98.27956989247312%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_61: Processing images: 100%|██████████| 5115/5115 [38:52<00:00,  2.19image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_61. Total images: 5115, predicted_correct': 5025, predicted_correct (%)': 98.24046920821115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_62: Processing images: 100%|██████████| 5115/5115 [38:57<00:00,  2.19image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_62. Total images: 5115, predicted_correct': 5035, predicted_correct (%)': 98.43597262952102%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_63: Processing images: 100%|██████████| 5115/5115 [38:54<00:00,  2.19image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_63. Total images: 5115, predicted_correct': 5030, predicted_correct (%)': 98.33822091886609%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_64: Processing images: 100%|██████████| 5115/5115 [38:49<00:00,  2.20image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_64. Total images: 5115, predicted_correct': 5030, predicted_correct (%)': 98.33822091886609%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_65: Processing images: 100%|██████████| 5115/5115 [38:47<00:00,  2.20image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_65. Total images: 5115, predicted_correct': 5031, predicted_correct (%)': 98.35777126099707%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_66: Processing images: 100%|██████████| 5115/5115 [38:54<00:00,  2.19image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_66. Total images: 5115, predicted_correct': 5032, predicted_correct (%)': 98.37732160312805%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_67: Processing images: 100%|██████████| 5115/5115 [38:59<00:00,  2.19image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_67. Total images: 5115, predicted_correct': 5030, predicted_correct (%)': 98.33822091886609%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_68: Processing images: 100%|██████████| 5115/5115 [42:02<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_68. Total images: 5115, predicted_correct': 5031, predicted_correct (%)': 98.35777126099707%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_69: Processing images: 100%|██████████| 5115/5115 [40:21<00:00,  2.11image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_69. Total images: 5115, predicted_correct': 5031, predicted_correct (%)': 98.35777126099707%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_70: Processing images: 100%|██████████| 5115/5115 [39:01<00:00,  2.18image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_70. Total images: 5115, predicted_correct': 5025, predicted_correct (%)': 98.24046920821115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_71: Processing images: 100%|██████████| 5115/5115 [42:03<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_71. Total images: 5115, predicted_correct': 5022, predicted_correct (%)': 98.18181818181819%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_72: Processing images: 100%|██████████| 5115/5115 [39:52<00:00,  2.14image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_72. Total images: 5115, predicted_correct': 5025, predicted_correct (%)': 98.24046920821115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_73: Processing images: 100%|██████████| 5115/5115 [40:22<00:00,  2.11image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_73. Total images: 5115, predicted_correct': 5028, predicted_correct (%)': 98.2991202346041%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_74: Processing images: 100%|██████████| 5115/5115 [51:18<00:00,  1.66image/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_74. Total images: 5115, predicted_correct': 5030, predicted_correct (%)': 98.33822091886609%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_75: Processing images:  79%|███████▉  | 4057/5115 [40:38<11:48,  1.49image/s]  "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Batch test for Direct Classification benchmark\n",
    "import torch, os, open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_species(file_path):\n",
    "    species = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            columns = line.strip().split('\\t')\n",
    "            if len(columns) > 1:\n",
    "                species.append(columns[1])\n",
    "    species = list(set(species))\n",
    "    return species\n",
    "\n",
    "caption_path = '/mnt/g/Datasets/Whale_Dolphin_Identification/Square/test/captions.txt'\n",
    "species = extract_species(caption_path)\n",
    "\n",
    "\n",
    "def process_single_epoch(pretrained_model, sentences):\n",
    "    img_dir = '/mnt/g/Datasets/Whale_Dolphin_Identification/Square/test/images/'\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        model_name=\"coca_ViT-L-14\",\n",
    "        pretrained=pretrained_model,\n",
    "    )\n",
    "    pt_dir = os.path.dirname(pretrained_model)\n",
    "    epoch_number = os.path.basename(pretrained_model).split('epoch_')[1].split('.pt')[0]\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    # Prepare the tokenizer for sentences\n",
    "    tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "    text = tokenizer(sentences)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def read_single_image(img_dir, img_path, sentences, text_features):\n",
    "        #get the ground truth value\n",
    "        # Extract the filename without extension\n",
    "        filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        image_path = os.path.join(img_dir, img_path)\n",
    "        ground_truth = \"_\".join(filename.split('_')[:-1]).replace(\"_\", \" \")\n",
    "        \n",
    "        # load a sample image\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            text_probs = text_probs.cpu().tolist()[0]\n",
    "\n",
    "        # Construct the dictionary\n",
    "        class_predict_dict = dict(zip(sentences, text_probs))\n",
    "        # Extract the key with the largest value\n",
    "        class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "        \n",
    "        if ground_truth == class_predict:\n",
    "            predicted_correct = 1\n",
    "        else:\n",
    "            predicted_correct = 0\n",
    "        \n",
    "        pred_dict = {'img_path': [img_path], 'class_predict_dict': [class_predict_dict], 'ground_truth': [ground_truth], 'class_predict': [class_predict], 'predicted_correct': [predicted_correct]}\n",
    "        df_single = pd.DataFrame(pred_dict)\n",
    "        \n",
    "        return df_single\n",
    "\n",
    "    all_images = os.listdir(img_dir)\n",
    "    for img_path in tqdm(all_images, desc=f\"epoch_{epoch_number}: Processing images\", unit=\"image\"):\n",
    "        df = read_single_image(img_dir, img_path, sentences, text_features)\n",
    "        df_all = pd.concat([df_all, df]).reset_index(drop=True)\n",
    "\n",
    "    #save df_all to csv\n",
    "    csv_file = f\"temp/epoch_{epoch_number}.csv\"\n",
    "    df_all.to_csv(csv_file, index=False)\n",
    "\n",
    "    total_rows = len(df_all)\n",
    "    # Count the occurrences of '1' in the 'predicted_correct' column\n",
    "    count_ones = df_all['predicted_correct'].sum()\n",
    "    epoch_filename = os.path.splitext(os.path.basename(pretrained_model))[0]\n",
    "    predicted_correct_pct = count_ones /total_rows *100\n",
    "    print(f\"Epoch_file: {epoch_filename}. Total images: {total_rows}, predicted_correct': {count_ones}, predicted_correct (%)': {predicted_correct_pct}%\")\n",
    "    \n",
    "    epoch_dict = {'pt_dir': [pt_dir],\n",
    "                'epoch_filename': [epoch_filename], \n",
    "                'sentences': [sentences], \n",
    "                'total_rows': [total_rows], \n",
    "                'predicted_correct': [count_ones], \n",
    "                'predicted_correct (%)': [predicted_correct_pct]}\n",
    "    df_single_epoch = pd.DataFrame(epoch_dict)\n",
    "    return df_single_epoch\n",
    "\n",
    "\n",
    "def process_single_pretrained_model(pretrained_model, csv_file):\n",
    "    df_single_pretrained_model = pd.DataFrame()\n",
    "    df = process_single_epoch(pretrained_model, species)\n",
    "    df_single_pretrained_model = pd.concat([df_single_pretrained_model, df]).reset_index(drop=True)\n",
    "    save_to_csv(df_single_pretrained_model, csv_file)\n",
    "    \n",
    "def extract_epoch_num(filepath):\n",
    "    # Split the filename from the path\n",
    "    filename = os.path.basename(filepath) #filepath.split('/')[-1]\n",
    "    # Extract the number between \"epoch_\" and \".pt\"\n",
    "    epoch_num = int(filename.split('epoch_')[1].split('.pt')[0])\n",
    "    return epoch_num\n",
    "\n",
    "def save_to_csv(df, csv_file):\n",
    "    if not os.path.exists(csv_file):\n",
    "        df.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # If the CSV file already exists, append without header\n",
    "        df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "def main():\n",
    "    pt_folder_path = '/mnt/g/Logtemp/open_clip/Whale_Dolphin_Identification/2023_08_18-21_14_32-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints/'\n",
    "    pt_files = [os.path.join(pt_folder_path, filename) for filename in os.listdir(pt_folder_path) if filename.endswith('.pt')]\n",
    "    #filter pt_files with low epoch number\n",
    "    pt_files = [pt_file for pt_file in pt_files if extract_epoch_num(pt_file) >= 41]\n",
    "    # Sort the list\n",
    "    sorted_pt_files = sorted(pt_files, key=extract_epoch_num)\n",
    "    csv_file = \"benchmark_all_epochs.csv\"\n",
    "    for pretrained_model in sorted_pt_files:\n",
    "        process_single_pretrained_model(pretrained_model, csv_file)\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
