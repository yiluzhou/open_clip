{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['globis',\n",
       " 'kiler whale',\n",
       " 'spotted dolphin',\n",
       " 'frasiers dolphin',\n",
       " 'long finned pilot whale',\n",
       " 'pantropic spotted dolphin',\n",
       " 'killer whale',\n",
       " 'rough toothed dolphin',\n",
       " 'false killer whale',\n",
       " 'short finned pilot whale',\n",
       " 'cuviers beaked whale',\n",
       " 'bottlenose dolphin',\n",
       " 'commersons dolphin',\n",
       " 'melon headed whale',\n",
       " 'beluga',\n",
       " 'white sided dolphin',\n",
       " 'humpback whale',\n",
       " 'common dolphin',\n",
       " 'bottlenose dolpin',\n",
       " 'sei whale',\n",
       " 'pygmy killer whale',\n",
       " 'pilot whale',\n",
       " 'dusky dolphin',\n",
       " 'minke whale',\n",
       " 'gray whale',\n",
       " 'fin whale',\n",
       " 'brydes whale',\n",
       " 'spinner dolphin',\n",
       " 'southern right whale',\n",
       " 'blue whale']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "csv_path = '/mnt/g/Datasets/Whale_Dolphin_Identification/train_new3.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "species = list(set(df['species'].tolist()))\n",
    "species = [a.replace('_', ' ') for a in species]\n",
    "species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: color_image = True\n",
      "TODO: color_image = True\n",
      "Label probs: tensor([[4.0117e-03, 9.9646e-03, 8.8292e-08, 2.8080e-04, 1.3491e-02, 2.4473e-05,\n",
      "         1.0136e-03, 2.0245e-02, 2.2700e-01, 4.9068e-03, 5.2500e-05, 5.4739e-07,\n",
      "         1.2419e-03, 2.0707e-04, 2.9619e-05, 6.1123e-12, 6.0200e-04, 2.1139e-08,\n",
      "         2.0880e-06, 4.5062e-05, 7.0411e-01, 1.2382e-02, 5.9633e-10, 1.1547e-07,\n",
      "         2.5476e-06, 3.7558e-05, 3.0966e-07, 4.8484e-05, 3.0047e-04, 1.9472e-08]])\n",
      "Ground truth: false killer whale\n",
      "Predicted class: globis\n",
      "Predicted correct: 0\n"
     ]
    }
   ],
   "source": [
    "#classification\n",
    "\n",
    "import torch, os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import open_clip\n",
    "\n",
    "pretrained = '/mnt/g/Logtemp/open_clip/Whale_Dolphin_Identification/2023_09_26-22_11_30-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints/epoch_61.pt'\n",
    "# model, _, preprocess = open_clip.create_model_and_transforms('coca_ViT-L-14', pretrained='mscoco_finetuned_laion2B-s13B-b90k')\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "  model_name=\"coca_ViT-L-14\",\n",
    "  pretrained=pretrained, #mscoco_finetuned_laion2B-s13B-b90k\n",
    "  color_image=True,\n",
    ")\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "\n",
    "image_path = '/mnt/g/Datasets/Whale_Dolphin_Identification/Square/test/images/false_killer_whale_bdea86a4d11fa9.jpg'\n",
    "img = Image.open(image_path).convert('RGB')\n",
    "img = img.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "image = preprocess(img).unsqueeze(0)\n",
    "text = tokenizer(species)\n",
    "\n",
    "filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "ground_truth = \"_\".join(filename.split('_')[:-1]).replace(\"_\", \" \")\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    # Construct the dictionary\n",
    "    class_predict_dict = dict(zip(species, text_probs))\n",
    "    # Extract the key with the largest value\n",
    "    class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "    \n",
    "    if ground_truth == class_predict:\n",
    "        predicted_correct = 1\n",
    "    else:\n",
    "        predicted_correct = 0\n",
    "\n",
    "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]\n",
    "print(\"Ground truth:\", ground_truth)  # prints: 0\n",
    "print(\"Predicted class:\", class_predict)  # prints: 0\n",
    "print(\"Predicted correct:\", predicted_correct)  # prints: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_100: Processing images:  18%|█▊        | 946/5115 [08:45<38:37,  1.80image/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m     \u001b[39mfor\u001b[39;00m pretrained_model \u001b[39min\u001b[39;00m sorted_pt_files:\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m         process_single_pretrained_model(pretrained_model, csv_file)\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=119'>120</a>\u001b[0m main()\n",
      "\u001b[1;32m/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m csv_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbenchmark_all_epochs.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39mfor\u001b[39;00m pretrained_model \u001b[39min\u001b[39;00m sorted_pt_files:\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m     process_single_pretrained_model(pretrained_model, csv_file)\n",
      "\u001b[1;32m/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_single_pretrained_model\u001b[39m(pretrained_model, csv_file):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m     df_single_pretrained_model \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m     df \u001b[39m=\u001b[39m process_single_epoch(pretrained_model, species)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m     df_single_pretrained_model \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df_single_pretrained_model, df])\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m     save_to_csv(df_single_pretrained_model, csv_file)\n",
      "\u001b[1;32m/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m all_images \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(img_dir)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mfor\u001b[39;00m img_path \u001b[39min\u001b[39;00m tqdm(all_images, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch_\u001b[39m\u001b[39m{\u001b[39;00mepoch_number\u001b[39m}\u001b[39;00m\u001b[39m: Processing images\u001b[39m\u001b[39m\"\u001b[39m, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     df \u001b[39m=\u001b[39m read_single_image(img_dir, img_path, sentences, text_features)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     df_all \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df_all, df])\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m#save df_all to csv\u001b[39;00m\n",
      "\u001b[1;32m/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m image \u001b[39m=\u001b[39m preprocess(Image\u001b[39m.\u001b[39mopen(image_path))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(), torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast():\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     image_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode_image(image)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     image_features \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m image_features\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yilu/Development/open_clip/Whale_Dolphin_Identification/test_Whale_inference.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     text_probs \u001b[39m=\u001b[39m (\u001b[39m100.0\u001b[39m \u001b[39m*\u001b[39m image_features \u001b[39m@\u001b[39m text_features\u001b[39m.\u001b[39mT)\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/coca_model.py:143\u001b[0m, in \u001b[0;36mCoCa.encode_image\u001b[0;34m(self, images, normalize)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_image\u001b[39m(\u001b[39mself\u001b[39m, images, normalize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 143\u001b[0m     image_latent, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_image(images, normalize\u001b[39m=\u001b[39;49mnormalize)\n\u001b[1;32m    144\u001b[0m     \u001b[39mreturn\u001b[39;00m image_latent\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/coca_model.py:132\u001b[0m, in \u001b[0;36mCoCa._encode_image\u001b[0;34m(self, images, normalize)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_image\u001b[39m(\u001b[39mself\u001b[39m, images, normalize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 132\u001b[0m     image_latent, tokens_embs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual(images)\n\u001b[1;32m    133\u001b[0m     image_latent \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnormalize(image_latent, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m normalize \u001b[39melse\u001b[39;00m image_latent\n\u001b[1;32m    134\u001b[0m     \u001b[39mreturn\u001b[39;00m image_latent, tokens_embs\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/transformer.py:486\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    483\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_pre(x)\n\u001b[1;32m    485\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m    487\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_pool \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/transformer.py:321\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    319\u001b[0m         x \u001b[39m=\u001b[39m checkpoint(r, x, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, attn_mask)\n\u001b[1;32m    320\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m         x \u001b[39m=\u001b[39m r(x, attn_mask\u001b[39m=\u001b[39;49mattn_mask)\n\u001b[1;32m    322\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/transformer.py:242\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    239\u001b[0m k_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(k_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m k_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m v_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(v_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m x \u001b[39m=\u001b[39m q_x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(q_x\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(q_x), k_x\u001b[39m=\u001b[39;49mk_x, v_x\u001b[39m=\u001b[39;49mv_x, attn_mask\u001b[39m=\u001b[39;49mattn_mask))\n\u001b[1;32m    243\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x)))\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/open_clip/transformer.py:228\u001b[0m, in \u001b[0;36mResidualAttentionBlock.attention\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    225\u001b[0m v_x \u001b[39m=\u001b[39m v_x \u001b[39mif\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m q_x\n\u001b[1;32m    227\u001b[0m attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39mto(q_x\u001b[39m.\u001b[39mdtype) \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    229\u001b[0m     q_x, k_x, v_x, need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, attn_mask\u001b[39m=\u001b[39;49mattn_mask\n\u001b[1;32m    230\u001b[0m )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/modules/activation.py:1237\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1223\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1224\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1225\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1234\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1235\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1237\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1238\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1239\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1240\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1241\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1242\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1243\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1244\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1245\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1246\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1247\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1248\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1249\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/functional.py:5302\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5300\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5301\u001b[0m     \u001b[39massert\u001b[39;00m in_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 5302\u001b[0m     q, k, v \u001b[39m=\u001b[39m _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n\u001b[1;32m   5303\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5304\u001b[0m     \u001b[39massert\u001b[39;00m q_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.11/site-packages/torch/nn/functional.py:4826\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4823\u001b[0m \u001b[39mif\u001b[39;00m k \u001b[39mis\u001b[39;00m v:\n\u001b[1;32m   4824\u001b[0m     \u001b[39mif\u001b[39;00m q \u001b[39mis\u001b[39;00m k:\n\u001b[1;32m   4825\u001b[0m         \u001b[39m# self-attention\u001b[39;00m\n\u001b[0;32m-> 4826\u001b[0m         proj \u001b[39m=\u001b[39m linear(q, w, b)\n\u001b[1;32m   4827\u001b[0m         \u001b[39m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[1;32m   4828\u001b[0m         proj \u001b[39m=\u001b[39m proj\u001b[39m.\u001b[39munflatten(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, (\u001b[39m3\u001b[39m, E))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Batch test for Direct Classification benchmark\n",
    "import torch, os, open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "csv_path = '/mnt/g/Datasets/Whale_Dolphin_Identification/train_new3.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "species = list(set(df['species'].tolist()))\n",
    "species = [a.replace('_', ' ') for a in species]\n",
    "\n",
    "\n",
    "def process_single_epoch(pretrained_model, sentences):\n",
    "    img_dir = '/mnt/g/Datasets/Whale_Dolphin_Identification/Square/test/images/'\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        model_name=\"coca_ViT-L-14\",\n",
    "        pretrained=pretrained_model,\n",
    "        color_image=True,\n",
    "    )\n",
    "    pt_dir = os.path.dirname(pretrained_model)\n",
    "    epoch_number = os.path.basename(pretrained_model).split('epoch_')[1].split('.pt')[0]\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    # Prepare the tokenizer for sentences\n",
    "    tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "    text = tokenizer(sentences)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def read_single_image(img_dir, img_path, sentences, text_features):\n",
    "        #get the ground truth value\n",
    "        # Extract the filename without extension\n",
    "        filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        image_path = os.path.join(img_dir, img_path)\n",
    "        ground_truth = \"_\".join(filename.split('_')[:-1]).replace(\"_\", \" \")\n",
    "        \n",
    "        # load a sample image\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            text_probs = text_probs.cpu().tolist()[0]\n",
    "\n",
    "        # Construct the dictionary\n",
    "        class_predict_dict = dict(zip(sentences, text_probs))\n",
    "        # Extract the key with the largest value\n",
    "        class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "        \n",
    "        if ground_truth == class_predict:\n",
    "            predicted_correct = 1\n",
    "        else:\n",
    "            predicted_correct = 0\n",
    "        \n",
    "        pred_dict = {'img_path': [img_path], 'class_predict_dict': [class_predict_dict], 'ground_truth': [ground_truth], 'class_predict': [class_predict], 'predicted_correct': [predicted_correct]}\n",
    "        df_single = pd.DataFrame(pred_dict)\n",
    "        \n",
    "        return df_single\n",
    "\n",
    "    all_images = os.listdir(img_dir)\n",
    "    for img_path in tqdm(all_images, desc=f\"epoch_{epoch_number}: Processing images\", unit=\"image\"):\n",
    "        df = read_single_image(img_dir, img_path, sentences, text_features)\n",
    "        df_all = pd.concat([df_all, df]).reset_index(drop=True)\n",
    "\n",
    "    #save df_all to csv\n",
    "    csv_file = f\"temp/epoch_{epoch_number}.csv\"\n",
    "    df_all.to_csv(csv_file, index=False)\n",
    "\n",
    "    total_rows = len(df_all)\n",
    "    # Count the occurrences of '1' in the 'predicted_correct' column\n",
    "    count_ones = df_all['predicted_correct'].sum()\n",
    "    epoch_filename = os.path.splitext(os.path.basename(pretrained_model))[0]\n",
    "    predicted_correct_pct = count_ones /total_rows *100\n",
    "    print(f\"Epoch_file: {epoch_filename}. Total images: {total_rows}, predicted_correct': {count_ones}, predicted_correct (%)': {predicted_correct_pct}%\")\n",
    "    \n",
    "    epoch_dict = {'pt_dir': [pt_dir],\n",
    "                'epoch_filename': [epoch_filename], \n",
    "                'sentences': [sentences], \n",
    "                'total_rows': [total_rows], \n",
    "                'predicted_correct': [count_ones], \n",
    "                'predicted_correct (%)': [predicted_correct_pct]}\n",
    "    df_single_epoch = pd.DataFrame(epoch_dict)\n",
    "    return df_single_epoch\n",
    "\n",
    "\n",
    "def process_single_pretrained_model(pretrained_model, csv_file):\n",
    "    df_single_pretrained_model = pd.DataFrame()\n",
    "    df = process_single_epoch(pretrained_model, species)\n",
    "    df_single_pretrained_model = pd.concat([df_single_pretrained_model, df]).reset_index(drop=True)\n",
    "    save_to_csv(df_single_pretrained_model, csv_file)\n",
    "    \n",
    "def extract_epoch_num(filepath):\n",
    "    # Split the filename from the path\n",
    "    filename = os.path.basename(filepath) #filepath.split('/')[-1]\n",
    "    # Extract the number between \"epoch_\" and \".pt\"\n",
    "    epoch_num = int(filename.split('epoch_')[1].split('.pt')[0])\n",
    "    return epoch_num\n",
    "\n",
    "def save_to_csv(df, csv_file):\n",
    "    if not os.path.exists(csv_file):\n",
    "        df.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # If the CSV file already exists, append without header\n",
    "        df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "def main():\n",
    "    pt_folder_path = '/mnt/g/Logtemp/open_clip/Whale_Dolphin_Identification/2023_09_28-21_57_51-model_coca_ViT-L-14-lr_2e-05-b_32-j_4-p_amp/checkpoints/'\n",
    "    pt_files = [os.path.join(pt_folder_path, filename) for filename in os.listdir(pt_folder_path) if filename.endswith('.pt')]\n",
    "    #filter pt_files with low epoch number\n",
    "    pt_files = [pt_file for pt_file in pt_files if extract_epoch_num(pt_file) >= 69]\n",
    "    # Sort the list\n",
    "    sorted_pt_files = sorted(pt_files, key=extract_epoch_num, reverse=True) #\n",
    "    csv_file = \"benchmark_all_epochs.csv\"\n",
    "    for pretrained_model in sorted_pt_files:\n",
    "        process_single_pretrained_model(pretrained_model, csv_file)\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
