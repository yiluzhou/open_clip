{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption prediction\n",
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "#('coca_ViT-B-32', 'laion2b_s13b_b90k')\n",
    "# model, _, transform = open_clip.create_model_and_transforms(\n",
    "#     model_name=\"coca_ViT-B-32\",  #coca_ViT-L-14, \n",
    "#     pretrained=\"laion2b_s13b_b90k\", #mscoco_finetuned_laion2B-s13B-b90k\n",
    "# )\n",
    "# /mnt/eds_share/Users/yilu.zhou/Development/log/open_clip/2023_07_28-13_55_44-model_coca_ViT-L-14-lr_1e-05-b_32-j_4-p_amp/checkpoints/epoch_1.pt\n",
    "model, _, transform = open_clip.create_model_and_transforms(\n",
    "    model_name=\"coca_ViT-L-14\",\n",
    "    pretrained=\"/mnt/eds_share/Users/yilu.zhou/Development/log/open_clip_GlobusSrgMapData_crop_square/2023_08_08-22_02_21-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints/epoch_78.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0.dev20230816+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/coca_model.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  text = text[:, :-1] if embed_cls else text # make space for CLS token\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[255, 36, 64]' is invalid for input of size 195840",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m dummy_image \u001b[39m=\u001b[39m transform(im)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m ONNX_FILE_PATH \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mopenclip.onnx\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 19\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(model, (dummy_image, dummy_texts), ONNX_FILE_PATH, input_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mIMAGE\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mTEXT\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     20\u001b[0m                   output_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mLOGITS_PER_IMAGE\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mLOGITS_PER_TEXT\u001b[39;49m\u001b[39m\"\u001b[39;49m], export_params\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     _export(\n\u001b[1;32m    517\u001b[0m         model,\n\u001b[1;32m    518\u001b[0m         args,\n\u001b[1;32m    519\u001b[0m         f,\n\u001b[1;32m    520\u001b[0m         export_params,\n\u001b[1;32m    521\u001b[0m         verbose,\n\u001b[1;32m    522\u001b[0m         training,\n\u001b[1;32m    523\u001b[0m         input_names,\n\u001b[1;32m    524\u001b[0m         output_names,\n\u001b[1;32m    525\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    526\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    527\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    528\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    529\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    530\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    531\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    532\u001b[0m         autograd_inlining\u001b[39m=\u001b[39;49mautograd_inlining,\n\u001b[1;32m    533\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:1582\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1580\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1582\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1583\u001b[0m     model,\n\u001b[1;32m   1584\u001b[0m     args,\n\u001b[1;32m   1585\u001b[0m     verbose,\n\u001b[1;32m   1586\u001b[0m     input_names,\n\u001b[1;32m   1587\u001b[0m     output_names,\n\u001b[1;32m   1588\u001b[0m     operator_export_type,\n\u001b[1;32m   1589\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1590\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1591\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1592\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1593\u001b[0m )\n\u001b[1;32m   1595\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1597\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1598\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:1135\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m   1134\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1135\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m   1136\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1138\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:1011\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m   1007\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m   1012\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m   1013\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:915\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    913\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    914\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 915\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[1;32m    916\u001b[0m     model,\n\u001b[1;32m    917\u001b[0m     args,\n\u001b[1;32m    918\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    919\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    920\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    921\u001b[0m )\n\u001b[1;32m    922\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    924\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/jit/_trace.py:1287\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1286\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1287\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(\n\u001b[1;32m   1288\u001b[0m     f, strict, _force_outplace, return_inputs, _return_inputs_states\n\u001b[1;32m   1289\u001b[0m )(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1290\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/jit/_trace.py:133\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 133\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[1;32m    134\u001b[0m     wrapper,\n\u001b[1;32m    135\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[1;32m    136\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[1;32m    137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/jit/_trace.py:124\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    123\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 124\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    126\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/coca_model.py:158\u001b[0m, in \u001b[0;36mCoCa.forward\u001b[0;34m(self, image, text, embed_cls, image_latent, image_embs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m# TODO: add assertion to avoid bugs?\u001b[39;00m\n\u001b[1;32m    156\u001b[0m labels \u001b[39m=\u001b[39m text[:, \u001b[39m-\u001b[39mtoken_embs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:]\n\u001b[0;32m--> 158\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_decoder(image_embs, token_embs)\n\u001b[1;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    160\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimage_features\u001b[39m\u001b[39m\"\u001b[39m: image_latent,\n\u001b[1;32m    161\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtext_features\u001b[39m\u001b[39m\"\u001b[39m: text_latent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlogit_scale\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogit_scale\u001b[39m.\u001b[39mexp()\n\u001b[1;32m    165\u001b[0m }\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/transformer.py:714\u001b[0m, in \u001b[0;36mMultimodalTransformer.forward\u001b[0;34m(self, image_embs, text_embs)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m         text_embs \u001b[39m=\u001b[39m resblock(text_embs, attn_mask\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_mask[:seq_len, :seq_len])\n\u001b[0;32m--> 714\u001b[0m         text_embs \u001b[39m=\u001b[39m cross_attn(text_embs, k_x\u001b[39m=\u001b[39;49mimage_embs, v_x\u001b[39m=\u001b[39;49mimage_embs)\n\u001b[1;32m    716\u001b[0m x \u001b[39m=\u001b[39m text_embs\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    717\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final(x)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/transformer.py:242\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    239\u001b[0m k_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(k_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m k_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m v_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(v_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m x \u001b[39m=\u001b[39m q_x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(q_x\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(q_x), k_x\u001b[39m=\u001b[39;49mk_x, v_x\u001b[39m=\u001b[39;49mv_x, attn_mask\u001b[39m=\u001b[39;49mattn_mask))\n\u001b[1;32m    243\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x)))\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/transformer.py:228\u001b[0m, in \u001b[0;36mResidualAttentionBlock.attention\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    225\u001b[0m v_x \u001b[39m=\u001b[39m v_x \u001b[39mif\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m q_x\n\u001b[1;32m    227\u001b[0m attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39mto(q_x\u001b[39m.\u001b[39mdtype) \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    229\u001b[0m     q_x, k_x, v_x, need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, attn_mask\u001b[39m=\u001b[39;49mattn_mask\n\u001b[1;32m    230\u001b[0m )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/activation.py:1237\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1223\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1224\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1225\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1234\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1235\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1237\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1238\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1239\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1240\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1241\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1242\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1243\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1244\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1245\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1246\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1247\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1248\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1249\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/functional.py:5348\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5346\u001b[0m q \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mview(tgt_len, bsz \u001b[39m*\u001b[39m num_heads, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m   5347\u001b[0m \u001b[39mif\u001b[39;00m static_k \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 5348\u001b[0m     k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39;49mview(k\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], bsz \u001b[39m*\u001b[39;49m num_heads, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m   5349\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5350\u001b[0m     \u001b[39m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001b[39;00m\n\u001b[1;32m   5351\u001b[0m     \u001b[39massert\u001b[39;00m static_k\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m bsz \u001b[39m*\u001b[39m num_heads, \\\n\u001b[1;32m   5352\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpecting static_k.size(0) of \u001b[39m\u001b[39m{\u001b[39;00mbsz\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39mnum_heads\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00mstatic_k\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[255, 36, 64]' is invalid for input of size 195840"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from albumentations import Resize, Compose\n",
    "from albumentations.pytorch.transforms import  ToTensor\n",
    "from albumentations.augmentations.transforms import Normalize\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "sentences = [\"thoracic\", \"lumbar\", \"thoracic lumbar\"]\n",
    "dummy_texts = tokenizer(sentences) \n",
    "\n",
    "image_path = '/mnt/eds_share/share/Spine2D/GlobusSrgMapData_crop_square/test/images/anon_d4730414.dcm_lateral_full_lumbar_thoracic.jpg'\n",
    "im = Image.open(image_path).convert(\"RGB\")\n",
    "im = im.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "dummy_image = transform(im).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "ONNX_FILE_PATH = 'openclip.onnx'\n",
    "torch.onnx.export(model, (dummy_image, dummy_texts), ONNX_FILE_PATH, input_names=[\"IMAGE\", \"TEXT\"],\n",
    "                  output_names=[\"LOGITS_PER_IMAGE\", \"LOGITS_PER_TEXT\"], export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anteroposterior thoracic lumbar \n"
     ]
    }
   ],
   "source": [
    "# caption prediction\n",
    "image_path = '/mnt/eds_share/share/Spine2D/GlobusSrgMapData_crop_square/test/images/anon_d4730414.dcm_lateral_full_lumbar_thoracic.jpg'\n",
    "im = Image.open(image_path).convert(\"RGB\")\n",
    "im = im.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "im = transform(im).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    generated = model.generate(im)\n",
    "\n",
    "print(open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load standard CLIP model, image, text on cpu\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "# onnx cannot work with cuda\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name=\"coca_ViT-L-14\",\n",
    "    pretrained=\"/mnt/eds_share/Users/yilu.zhou/Development/log/open_clip_GlobusSrgMapData_crop_square/2023_08_08-22_02_21-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints/epoch_78.pt\"\n",
    ")\n",
    "# model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\", jit=False)\n",
    "\n",
    "# batch first\n",
    "image_path = '/mnt/eds_share/share/Spine2D/GlobusSrgMapData_crop_square/test/images/anon_d4730414.dcm_lateral_full_lumbar_thoracic.jpg'\n",
    "im = Image.open(image_path).convert(\"RGB\")\n",
    "im = im.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "image = preprocess(im).unsqueeze(0).cpu() # [1, 3, 224, 224]\n",
    "# image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).cpu() # [1, 3, 224, 224]\n",
    "image_onnx = image.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "# batch first\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "sentences = [\"thoracic\", \"lumbar\", \"thoracic lumbar\"]\n",
    "text = tokenizer(sentences).cpu()\n",
    "# text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).cpu() # [3, 77]\n",
    "text_onnx = text.detach().cpu().numpy().astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/97262\n",
    "import onnxscript\n",
    "\n",
    "# Assuming you use opset17\n",
    "from onnxscript.onnx_opset import opset13 as op\n",
    "\n",
    "custom_opset = onnxscript.values.Opset(domain=\"torch.onnx\", version=1)\n",
    "\n",
    "\n",
    "@onnxscript.script(custom_opset)\n",
    "def ScaledDotProductAttention(\n",
    "    query,\n",
    "    key,\n",
    "    value,\n",
    "    dropout_p,\n",
    "):\n",
    "    # Swap the last two axes of key\n",
    "    key_shape = op.Shape(key)\n",
    "    key_last_dim = key_shape[-1:]\n",
    "    key_second_last_dim = key_shape[-2:-1]\n",
    "    key_first_dims = key_shape[:-2]\n",
    "    # Contract the dimensions that are not the last two so we can transpose\n",
    "    # with a static permutation.\n",
    "    key_squeezed_shape = op.Concat(\n",
    "        op.Constant(value_ints=[-1]), key_second_last_dim, key_last_dim, axis=0\n",
    "    )\n",
    "    key_squeezed = op.Reshape(key, key_squeezed_shape)\n",
    "    key_squeezed_transposed = op.Transpose(key_squeezed, perm=[0, 2, 1])\n",
    "    key_transposed_shape = op.Concat(key_first_dims, key_last_dim, key_second_last_dim, axis=0)\n",
    "    key_transposed = op.Reshape(key_squeezed_transposed, key_transposed_shape)\n",
    "\n",
    "    embedding_size = op.CastLike(op.Shape(query)[-1], query)\n",
    "    scale = op.Div(1.0, op.Sqrt(embedding_size))\n",
    "\n",
    "    # https://github.com/pytorch/pytorch/blob/12da0c70378b5be9135c6fda62a9863bce4a4818/aten/src/ATen/native/transformers/attention.cpp#L653\n",
    "    # Scale q, k before matmul for stability see https://tinyurl.com/sudb9s96 for math\n",
    "    query_scaled = op.Mul(query, op.Sqrt(scale))\n",
    "    key_transposed_scaled = op.Mul(key_transposed, op.Sqrt(scale))\n",
    "    attn_weight = op.Softmax(\n",
    "        op.MatMul(query_scaled, key_transposed_scaled),\n",
    "        axis=-1,\n",
    "    )\n",
    "    attn_weight, _ = op.Dropout(attn_weight, dropout_p)\n",
    "    return op.MatMul(attn_weight, value)\n",
    "\n",
    "\n",
    "def custom_scaled_dot_product_attention(g, query, key, value, attn_mask, dropout, is_causal, scale=None):\n",
    "    return g.onnxscript_op(ScaledDotProductAttention, query, key, value, dropout).setType(value.type())\n",
    "\n",
    "\n",
    "torch.onnx.register_custom_op_symbolic(\n",
    "    symbolic_name=\"aten::scaled_dot_product_attention\",\n",
    "    symbolic_fn=custom_scaled_dot_product_attention,\n",
    "    opset_version=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:onnxscript:Selu: Already defined.\n"
     ]
    }
   ],
   "source": [
    "import onnxscript, torch\n",
    "from torch.onnx._internal import _beartype, jit_utils, registration\n",
    "# There are three opset version needed to be aligned\n",
    "# This is (1) the opset version in ONNX function\n",
    "from onnxscript.onnx_opset import opset15 as op\n",
    "opset_version = 13\n",
    "\n",
    "x = torch.randn(1, 2, 3, 4, requires_grad=True)\n",
    "model = torch.nn.SELU()\n",
    "\n",
    "custom_opset = onnxscript.values.Opset(domain=\"onnx-script\", version=1)\n",
    "\n",
    "@onnxscript.script(custom_opset)\n",
    "def Selu(X):\n",
    "    alpha = 1.67326  # auto wrapped as Constants\n",
    "    gamma = 1.0507\n",
    "    alphaX = op.CastLike(alpha, X)\n",
    "    gammaX = op.CastLike(gamma, X)\n",
    "    neg = gammaX * (alphaX * op.Exp(X) - alphaX)\n",
    "    pos = gammaX * X\n",
    "    zero = op.CastLike(0, X)\n",
    "    return op.Where(X <= zero, neg, pos)\n",
    "\n",
    "# setType API provides shape/type to ONNX shape/type inference\n",
    "def custom_selu(g: jit_utils.GraphContext, X):\n",
    "    return g.onnxscript_op(Selu, X).setType(X.type())\n",
    "\n",
    "# Register custom symbolic function\n",
    "# There are three opset version needed to be aligned\n",
    "# This is (2) the opset version in registry\n",
    "torch.onnx.register_custom_op_symbolic(\n",
    "    symbolic_name=\"aten::selu\",\n",
    "    symbolic_fn=custom_selu,\n",
    "    opset_version=opset_version,\n",
    ")\n",
    "\n",
    "# There are three opset version needed to be aligned\n",
    "# This is (2) the opset version in exporter\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    x,\n",
    "    \"model.onnx\",\n",
    "    opset_version=opset_version,\n",
    "    # only needed if you want to specify an opset version > 1.\n",
    "    custom_opsets={\"onnx-script\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIP ONNX] Start convert visual model\n"
     ]
    },
    {
     "ename": "UnsupportedOperatorError",
     "evalue": "Exporting the operator 'aten::scaled_dot_product_attention' to ONNX opset version 13 is not supported. Support for this operator was added in version 14, try exporting with this version.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m textual_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclip_textual.onnx\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m onnx_model \u001b[39m=\u001b[39m clip_onnx(model, visual_path\u001b[39m=\u001b[39mvisual_path, textual_path\u001b[39m=\u001b[39mtextual_path)\n\u001b[0;32m----> 8\u001b[0m onnx_model\u001b[39m.\u001b[39;49mconvert2onnx(image, text, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\u001b[39;00m\n\u001b[1;32m     10\u001b[0m onnx_model\u001b[39m.\u001b[39mstart_sessions(providers\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCPUExecutionProvider\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m# cpu mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/clip_onnx/clip_converter.py:79\u001b[0m, in \u001b[0;36mclip_converter.convert2onnx\u001b[0;34m(self, visual_input, textual_input, verbose, visual_wrapper, textual_wrapper, visual_export_params, textual_export_params)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m     78\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[CLIP ONNX] Start convert visual model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_visual(visual_input, visual_wrapper, visual_export_params)\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m     81\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[CLIP ONNX] Start check visual model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/clip_onnx/clip_converter.py:49\u001b[0m, in \u001b[0;36mclip_converter.convert_visual\u001b[0;34m(self, dummy_input, wrapper, export_params)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_visual\u001b[39m(\u001b[39mself\u001b[39m, dummy_input, wrapper\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x,\n\u001b[1;32m     47\u001b[0m                    export_params\u001b[39m=\u001b[39mDEFAULT_EXPORT):\n\u001b[1;32m     48\u001b[0m     visual \u001b[39m=\u001b[39m wrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mvisual)\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch_export(visual, dummy_input, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual_path,\n\u001b[1;32m     50\u001b[0m                       export_params\u001b[39m=\u001b[39;49mexport_params)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39monnx_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisual_path)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/clip_onnx/clip_converter.py:39\u001b[0m, in \u001b[0;36mclip_converter.torch_export\u001b[0;34m(self, model, dummy_input, path, export_params)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtorch_export\u001b[39m(\u001b[39mself\u001b[39m, model, dummy_input, path: \u001b[39mstr\u001b[39m, export_params\u001b[39m=\u001b[39mDEFAULT_EXPORT):\n\u001b[0;32m---> 39\u001b[0m     torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(model, dummy_input, path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mexport_params)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     _export(\n\u001b[1;32m    517\u001b[0m         model,\n\u001b[1;32m    518\u001b[0m         args,\n\u001b[1;32m    519\u001b[0m         f,\n\u001b[1;32m    520\u001b[0m         export_params,\n\u001b[1;32m    521\u001b[0m         verbose,\n\u001b[1;32m    522\u001b[0m         training,\n\u001b[1;32m    523\u001b[0m         input_names,\n\u001b[1;32m    524\u001b[0m         output_names,\n\u001b[1;32m    525\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    526\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    527\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    528\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    529\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    530\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    531\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    532\u001b[0m         autograd_inlining\u001b[39m=\u001b[39;49mautograd_inlining,\n\u001b[1;32m    533\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:1582\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1580\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1582\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1583\u001b[0m     model,\n\u001b[1;32m   1584\u001b[0m     args,\n\u001b[1;32m   1585\u001b[0m     verbose,\n\u001b[1;32m   1586\u001b[0m     input_names,\n\u001b[1;32m   1587\u001b[0m     output_names,\n\u001b[1;32m   1588\u001b[0m     operator_export_type,\n\u001b[1;32m   1589\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1590\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1591\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1592\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1593\u001b[0m )\n\u001b[1;32m   1595\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1597\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1598\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:1139\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1136\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1138\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     graph \u001b[39m=\u001b[39m _optimize_graph(\n\u001b[1;32m   1140\u001b[0m         graph,\n\u001b[1;32m   1141\u001b[0m         operator_export_type,\n\u001b[1;32m   1142\u001b[0m         _disable_torch_constant_prop\u001b[39m=\u001b[39;49m_disable_torch_constant_prop,\n\u001b[1;32m   1143\u001b[0m         fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1144\u001b[0m         params_dict\u001b[39m=\u001b[39;49mparams_dict,\n\u001b[1;32m   1145\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1146\u001b[0m         input_names\u001b[39m=\u001b[39;49minput_names,\n\u001b[1;32m   1147\u001b[0m         module\u001b[39m=\u001b[39;49mmodule,\n\u001b[1;32m   1148\u001b[0m     )\n\u001b[1;32m   1149\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1150\u001b[0m     torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mTorch IR graph at exception: \u001b[39m\u001b[39m\"\u001b[39m, graph)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:677\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    674\u001b[0m     _C\u001b[39m.\u001b[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001b[1;32m    675\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m--> 677\u001b[0m graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39;49m_jit_pass_onnx(graph, operator_export_type)\n\u001b[1;32m    678\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    679\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_lint(graph)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:1936\u001b[0m, in \u001b[0;36m_run_symbolic_function\u001b[0;34m(graph, block, node, inputs, env, operator_export_type)\u001b[0m\n\u001b[1;32m   1932\u001b[0m     \u001b[39mif\u001b[39;00m namespace \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39monnx\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1933\u001b[0m         \u001b[39m# Clone node to trigger ONNX shape inference\u001b[39;00m\n\u001b[1;32m   1934\u001b[0m         \u001b[39mreturn\u001b[39;00m graph_context\u001b[39m.\u001b[39mop(op_name, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mattrs, outputs\u001b[39m=\u001b[39mnode\u001b[39m.\u001b[39moutputsSize())  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1936\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mUnsupportedOperatorError(\n\u001b[1;32m   1937\u001b[0m         symbolic_function_name,\n\u001b[1;32m   1938\u001b[0m         opset_version,\n\u001b[1;32m   1939\u001b[0m         symbolic_function_group\u001b[39m.\u001b[39mget_min_supported()\n\u001b[1;32m   1940\u001b[0m         \u001b[39mif\u001b[39;00m symbolic_function_group\n\u001b[1;32m   1941\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1942\u001b[0m     )\n\u001b[1;32m   1944\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m:\n\u001b[1;32m   1945\u001b[0m     \u001b[39mif\u001b[39;00m operator_export_type \u001b[39m==\u001b[39m _C_onnx\u001b[39m.\u001b[39mOperatorExportTypes\u001b[39m.\u001b[39mONNX_FALLTHROUGH:\n",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m: Exporting the operator 'aten::scaled_dot_product_attention' to ONNX opset version 13 is not supported. Support for this operator was added in version 14, try exporting with this version."
     ]
    }
   ],
   "source": [
    "# Create CLIP-ONNX object to convert model to onnx\n",
    "from clip_onnx import clip_onnx\n",
    "\n",
    "visual_path = \"clip_visual.onnx\"\n",
    "textual_path = \"clip_textual.onnx\"\n",
    "\n",
    "onnx_model = clip_onnx(model, visual_path=visual_path, textual_path=textual_path)\n",
    "onnx_model.convert2onnx(image, text, verbose=True)\n",
    "# ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "onnx_model.start_sessions(providers=[\"CPUExecutionProvider\"]) # cpu mode\n",
    "\n",
    "#Use for standard CLIP API. Batch inference\n",
    "image_features = onnx_model.encode_image(image_onnx)\n",
    "text_features = onnx_model.encode_text(text_onnx)\n",
    "\n",
    "logits_per_image, logits_per_text = onnx_model(image_onnx, text_onnx)\n",
    "probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421067 0.00299571]]\n",
    "\n",
    "# https://github.com/pytorch/pytorch/issues/96944"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yz_openclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
