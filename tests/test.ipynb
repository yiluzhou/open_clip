{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import open_clip\n",
    "\n",
    "# model, _, preprocess = open_clip.create_model_and_transforms('ViT-bigG-14', pretrained='laion2b_s39b_b160k')\n",
    "pretrained_model = '/mnt/eds_share/Users/yilu.zhou/Development/log/open_clip/2023_07_17-13_09_01-model_ViT-L-14-lr_1e-06-b_256-j_4-p_amp/checkpoints/epoch_30.pt'\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained=pretrained_model)\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 427,616,513\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "# open_clip.list_pretrained()\n",
    "import numpy as np\n",
    "model.eval()\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[0.9975, 0.0025]])\n"
     ]
    }
   ],
   "source": [
    "image_path = '/mnt/eds_ml/Users/Yilu_ML/roco-dataset/data/test/radiology/images/ROCO_00001.jpg'\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
    "text = tokenizer([\"MRI\", \"CT\"])\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['filepath', 'caption'], dtype='object')\n",
      "['E:\\\\Work\\\\roco-dataset\\\\data\\\\validation\\\\radiology\\\\images\\\\ROCO_00020.jpg', 'E:\\\\Work\\\\roco-dataset\\\\data\\\\validation\\\\radiology\\\\images\\\\ROCO_00027.jpg', 'E:\\\\Work\\\\roco-dataset\\\\data\\\\validation\\\\radiology\\\\images\\\\ROCO_00059.jpg', 'E:\\\\Work\\\\roco-dataset\\\\data\\\\validation\\\\radiology\\\\images\\\\ROCO_00062.jpg', 'E:\\\\Work\\\\roco-dataset\\\\data\\\\validation\\\\radiology\\\\images\\\\ROCO_00068.jpg']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/roco_validation.csv', sep='\\t')\n",
    "img_key = 'filepath'\n",
    "print(df.keys())\n",
    "images = df[img_key].tolist()\n",
    "print(images[:5])\n",
    "caption_key = 'title'\n",
    "captions = df[caption_key].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32', 'datacomp_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_clip_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_laion_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_image_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_text_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_basic_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'datacomp_s_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_clip_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_laion_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_image_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_text_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_basic_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_s13m_b4k'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'laion2b_s34b_b88k'),\n",
       " ('ViT-B-16', 'datacomp_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_clip_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_laion_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_image_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_text_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_basic_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_s1b_b8k'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_clip_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_laion_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_s13b_b90k'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
       " ('ViT-g-14', 'laion2b_s34b_b88k'),\n",
       " ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n",
       " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
       " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
       " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n",
       " ('convnext_base', 'laion400m_s13b_b51k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n",
       " ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n",
       " ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft_soup'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_rewind'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_soup'),\n",
       " ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('EVA01-g-14', 'laion400m_s11b_b41k'),\n",
       " ('EVA01-g-14-plus', 'merged2b_s11b_b114k'),\n",
       " ('EVA02-B-16', 'merged2b_s8b_b131k'),\n",
       " ('EVA02-L-14', 'merged2b_s4b_b131k'),\n",
       " ('EVA02-L-14-336', 'merged2b_s6b_b61k'),\n",
       " ('EVA02-E-14', 'laion2b_s4b_b115k'),\n",
       " ('EVA02-E-14-plus', 'laion2b_s9b_b144k')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open_clip\n",
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a picture of the human head in a mri . \n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "#('coca_ViT-B-32', 'laion2b_s13b_b90k')\n",
    "# model, _, transform = open_clip.create_model_and_transforms(\n",
    "#   model_name=\"coca_ViT-B-32\",  #coca_ViT-L-14, \n",
    "#   pretrained=\"laion2b_s13b_b90k\", #mscoco_finetuned_laion2B-s13B-b90k\n",
    "# )\n",
    "# /mnt/eds_share/Users/yilu.zhou/Development/log/open_clip/2023_07_28-13_55_44-model_coca_ViT-L-14-lr_1e-05-b_32-j_4-p_amp/checkpoints/epoch_1.pt\n",
    "model, _, transform = open_clip.create_model_and_transforms(\n",
    "  model_name=\"coca_ViT-L-14\",\n",
    "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n",
    ")\n",
    "\n",
    "image_path = '/mnt/eds_ml/Users/Yilu_ML/roco-dataset/data/test/radiology/images/ROCO_00001.jpg'\n",
    "im = Image.open(image_path).convert(\"RGB\")\n",
    "im = transform(im).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "  generated = model.generate(im)\n",
    "\n",
    "print(open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[9.9364e-01, 3.2589e-04, 6.0292e-03]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import open_clip\n",
    "\n",
    "# /mnt/eds_share/Users/yilu.zhou/Development/log/open_clip/2023_07_28-13_55_44-model_coca_ViT-L-14-lr_1e-05-b_32-j_4-p_amp/checkpoints/epoch_3.pt\n",
    "# model, _, preprocess = open_clip.create_model_and_transforms('coca_ViT-L-14', pretrained='mscoco_finetuned_laion2B-s13B-b90k')\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "  model_name=\"coca_ViT-L-14\",\n",
    "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n",
    ")\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "\n",
    "image_path = '/mnt/eds_ml/Users/Yilu_ML/roco-dataset/data/test/radiology/images/ROCO_00001.jpg'\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
    "text = tokenizer([\"MRI\", \"CT\", \"Xray\"])\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Classification benchmark\n",
    "import torch, os, open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "pretrained_model = 'mscoco_finetuned_laion2B-s13B-b90k'\n",
    "sentences = [\"key1\", \"key2\"]\n",
    "img_dir = 'img_dir'\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name=\"coca_ViT-L-14\",\n",
    "    pretrained=pretrained_model,\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "# Prepare the tokenizer for sentences\n",
    "text = tokenizer(sentences)\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    text_features = model.encode_text(text)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def read_single_image(img_dir, img_path, sentences, text_features):\n",
    "    #get the ground truth value\n",
    "    # Extract the filename without extension\n",
    "    filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    image_path = os.path.join(img_dir, img_path)\n",
    "    # Check if any of the words in sentences are in the filename\n",
    "    ground_truth = [word for word in sentences if word in filename][0]\n",
    "    \n",
    "    # load a sample image\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        text_probs = text_probs.cpu().tolist()[0]\n",
    "\n",
    "    # Construct the dictionary\n",
    "    class_predict_dict = dict(zip(sentences, text_probs))\n",
    "    # Extract the key with the largest value\n",
    "    class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "    \n",
    "    if ground_truth == class_predict:\n",
    "        predicted_correct = 1\n",
    "    else:\n",
    "        predicted_correct = 0\n",
    "    \n",
    "    pred_dict = {'img_path': [img_path], 'class_predict_dict': [class_predict_dict], 'ground_truth': [ground_truth], 'class_predict': [class_predict], 'predicted_correct': [predicted_correct]}\n",
    "    df_single = pd.DataFrame(pred_dict)\n",
    "    \n",
    "    return df_single\n",
    "\n",
    "\n",
    "all_images = os.listdir(img_dir)\n",
    "for img_path in tqdm(all_images, desc=\"Processing images\", unit=\"image\"):\n",
    "    if '_full_' in img_path: #filaname\n",
    "        continue #skip full images\n",
    "    df = read_single_image(img_dir, img_path, sentences, text_features)\n",
    "    df_all = pd.concat([df_all, df]).reset_index(drop=True)\n",
    "\n",
    "# save csv file\n",
    "sentences_str = \"_\".join(sentences)\n",
    "df_all.to_csv(f\"{sentences_str}.csv\", index=False)\n",
    "# Count the number of rows in the 'predicted_correct' column\n",
    "total_rows = len(df_all)\n",
    "# Count the occurrences of '1' in the 'predicted_correct' column\n",
    "count_ones = df_all['predicted_correct'].sum()\n",
    "print(f\"Total images: {total_rows}\")\n",
    "print(f\"predicted_correct': {count_ones}\")\n",
    "predicted_correct_pct = count_ones /total_rows *100\n",
    "print(f\"predicted_correct (%)': {predicted_correct_pct}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 1 bounding box\n",
    "import torch\n",
    "from PIL import Image, ImageEnhance, ImageDraw, ImageColor\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "  model_name=\"coca_ViT-L-14\",\n",
    "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n",
    ")\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "\n",
    "image_path = 'Your image path'\n",
    "img = Image.open(image_path).convert('RGB')\n",
    "text = tokenizer([\"keyword1\", \"keyword2\"]) \n",
    "\n",
    "# Adapted from https://github.com/openai/CLIP/issues/82\n",
    "def generate_crop_dict(img, preprocess):\n",
    "    # Convert image into 56 by 56 crops\n",
    "    offset = 56\n",
    "    image_crop_dict = {}\n",
    "    for h in range(0, 224, offset):\n",
    "        for w in range(0, 224, offset):\n",
    "            area = (w, h, w + offset, h + offset)\n",
    "            crop = img.crop(area)\n",
    "            crop = crop.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "            # Include the batch dimension\n",
    "            image_crop_dict[area] = torch.tensor(preprocess(crop)).unsqueeze(0)\n",
    "    return image_crop_dict\n",
    "\n",
    "\n",
    "def highlight_area(img, region, factor, outline_color=None, outline_width=1):\n",
    "    \"\"\" Highlight specified rectangular region of image by `factor` with an\n",
    "        optional colored  boarder drawn around its edges and return the result.\n",
    "    \"\"\"\n",
    "    img = img.copy()  # Avoid changing original image.\n",
    "    img_crop = img.crop(region)\n",
    "\n",
    "    # brightner = ImageEnhance.Brightness(img_crop)\n",
    "    # img_crop = brightner.enhance(factor)\n",
    "    img.paste(img_crop, region)\n",
    "    # Optionally draw a colored outline around the edge of the rectangular region.\n",
    "    if outline_color:\n",
    "        draw = ImageDraw.Draw(img)  # Create a drawing context.\n",
    "        left, upper, right, lower = region  # Get bounds.\n",
    "        coords = [(left, upper), (right, upper), (right, lower), (left, lower),\n",
    "                  (left, upper)]\n",
    "        draw.line(coords, fill=outline_color, width=outline_width)\n",
    "    return img\n",
    "\n",
    "\n",
    "def viz_pseudo_attn_maps(model, img, text, preprocess):\n",
    "    image_dict = generate_crop_dict(img, preprocess)\n",
    "    sim_dict = {}\n",
    "    for crop_loc, image in image_dict.items():\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image).float()\n",
    "            text_features = model.encode_text(text).float()\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "            # print(f'similarity = {similarity}')\n",
    "            sim_dict[crop_loc] = similarity #.item()\n",
    "    print(f'sim_dict = {sim_dict}')\n",
    "    # bbox = max(sim_dict, key=sim_dict.get)\n",
    "    # bbox = max(sim_dict, key=lambda k: sim_dict[k][0])\n",
    "    bbox = max(sim_dict, key=lambda k: sim_dict[k][0])\n",
    "    print(f\"Max patch is {bbox}\")\n",
    "    # We pass the original image and the rectangle.\n",
    "    red = ImageColor.getrgb('red')\n",
    "    img2 = highlight_area(img, bbox, 2.5, outline_color=red, outline_width=2)\n",
    "    plt.imshow(img2)\n",
    "    plt.show()\n",
    "    return sim_dict\n",
    "\n",
    "sim_dict = viz_pseudo_attn_maps(model, img, text, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating heat map of attention mask\n",
    "import torch, os\n",
    "from PIL import Image, ImageEnhance, ImageDraw, ImageColor\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "  model_name=\"coca_ViT-L-14\",\n",
    "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n",
    ")\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "image_path = 'IMAGE path'\n",
    "sentences = [\"keywords1\", \"keywords2\"]\n",
    "filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "print(f'image name: {filename}')\n",
    "img = Image.open(image_path).convert('RGB')\n",
    "text = tokenizer(sentences)\n",
    "\n",
    "\n",
    "# https://github.com/openai/CLIP/issues/82\n",
    "def generate_crop_dict(img, preprocess):\n",
    "    # Convert image into 56 by 56 crops\n",
    "    shortest_pixel = min(img.size)\n",
    "    offset = min(56, int(shortest_pixel/8))\n",
    "    image_crop_dict = {}\n",
    "    for h in range(0, 224, offset):\n",
    "        for w in range(0, 224, offset):\n",
    "            area = (w, h, w + offset, h + offset)\n",
    "            crop = img.crop(area)\n",
    "            crop = crop.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "            # Include the batch dimension\n",
    "            # image_crop_dict[area] = torch.tensor(preprocess(crop)).unsqueeze(0)\n",
    "            image_crop_dict[area] = preprocess(crop).clone().detach().unsqueeze(0)\n",
    "    return image_crop_dict\n",
    "\n",
    "\n",
    "def highlight_area(img, region, factor, outline_color=None, outline_width=1):\n",
    "    \"\"\" Highlight specified rectangular region of image by `factor` with an\n",
    "        optional colored  boarder drawn around its edges and return the result.\n",
    "    \"\"\"\n",
    "    img = img.copy()  # Avoid changing original image.\n",
    "    img_crop = img.crop(region)\n",
    "\n",
    "    # brightner = ImageEnhance.Brightness(img_crop)\n",
    "    # img_crop = brightner.enhance(factor)\n",
    "    img.paste(img_crop, region)\n",
    "    # Optionally draw a colored outline around the edge of the rectangular region.\n",
    "    if outline_color:\n",
    "        draw = ImageDraw.Draw(img)  # Create a drawing context.\n",
    "        left, upper, right, lower = region  # Get bounds.\n",
    "        coords = [(left, upper), (right, upper), (right, lower), (left, lower),\n",
    "                  (left, upper)]\n",
    "        draw.line(coords, fill=outline_color, width=outline_width)\n",
    "    return img\n",
    "\n",
    "\n",
    "def viz_pseudo_attn_maps(model, img, text, preprocess, sentences):\n",
    "    image_dict = generate_crop_dict(img, preprocess)\n",
    "    sim_dict = {}\n",
    "    for crop_loc, image in image_dict.items():\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image).float()\n",
    "            text_features = model.encode_text(text).float()\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "            # print(f'similarity = {similarity}')\n",
    "            sim_dict[crop_loc] = similarity #.item()\n",
    "\n",
    "    # print(f'sim_dict={sim_dict}')\n",
    "    # load a sample image\n",
    "    image = preprocess(img).unsqueeze(0)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        text_probs = text_probs.cpu().tolist()[0]\n",
    "    #extract the prediction            \n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    text_probs = text_probs.cpu().tolist()[0]\n",
    "\n",
    "    # Construct the dictionary\n",
    "    class_predict_dict = dict(zip(sentences, text_probs))\n",
    "    # Extract the key with the largest value\n",
    "    predicted_class = max(class_predict_dict, key=class_predict_dict.get)\n",
    "    print(f'The heatmap of predicted_class ({predicted_class}) is as shown below:')\n",
    "    if predicted_class in sentences:\n",
    "        pred_choice = sentences.index(predicted_class)\n",
    "    else:\n",
    "        print(\"Predicted class not found in sentences.\")\n",
    "\n",
    "\n",
    "    # pred_choice = 0\n",
    "    # print(f'sim_dict={sim_dict}')\n",
    "    bbox = max(sim_dict, key=lambda k: sim_dict[k][pred_choice,0])\n",
    "    # print(f\"Max spot is {bbox}\")\n",
    "    # Pass the original image and the rectangle.\n",
    "    red = ImageColor.getrgb('red')\n",
    "    img2 = highlight_area(img, bbox, 2.5, outline_color=red, outline_width=2)\n",
    "    \n",
    "    # Create a 2D array with the same shape as your image (divided by the crop size)\n",
    "    heatmap = np.zeros((img.size[1], img.size[0]))\n",
    "    for crop_loc, similarity in sim_dict.items():\n",
    "        heatmap[crop_loc[1]:crop_loc[3], crop_loc[0]:crop_loc[2]] = similarity[pred_choice, 0]\n",
    "    # Normalize the heatmap\n",
    "    heatmap = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))\n",
    "    \n",
    "    # Plot original image with bounding box and heatmap side by side\n",
    "    # Create an additional subplot for the colorbar\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(11, 5), gridspec_kw={'width_ratios': [1, 1, 0.05]})\n",
    "    # fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(img2)\n",
    "    axs[0].axis('on')\n",
    "    im = axs[1].imshow(heatmap, cmap='Blues')\n",
    "    # axs[1].imshow(heatmap, cmap='Blues', interpolation='nearest')\n",
    "    # axs[1].colorbar()\n",
    "    axs[1].set_xticks([])\n",
    "    axs[1].set_yticks([])\n",
    "    axs[1].set_frame_on(True)\n",
    "    # Add a colorbar to the heatmap\n",
    "    fig.colorbar(im, cax=axs[2]) #, orientation='vertical'\n",
    "    axs[1].axis('on')\n",
    "    plt.show()\n",
    "    return sim_dict\n",
    "\n",
    "sim_dict = viz_pseudo_attn_maps(model, img, text, preprocess, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating heat map of attention mask for caption prediction\n",
    "import open_clip, torch, os\n",
    "from PIL import Image, ImageEnhance, ImageDraw, ImageColor\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name=\"coca_ViT-L-14\",\n",
    "    pretrained=\"\"\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "\n",
    "image_path = ''\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "img = img.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "im = preprocess(img).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    generated = model.generate(im)\n",
    "\n",
    "# Generating heat map of attention mask for sentences\n",
    "sentences = [open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\")]\n",
    "\n",
    "filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "print(f'image_path: {image_path}')\n",
    "print(f'image name: {filename}.jpg \\nPredicted caption: {sentences[0]}')\n",
    "print(f'The heatmap for predicted caption shown as below:')\n",
    "text = tokenizer(sentences)\n",
    "\n",
    "\n",
    "# https://github.com/openai/CLIP/issues/82\n",
    "def generate_crop_dict(img, preprocess):\n",
    "    # Convert image into 56 by 56 crops\n",
    "    shortest_pixel = min(img.size)\n",
    "    offset = min(56, int(shortest_pixel/8))\n",
    "    image_crop_dict = {}\n",
    "    for h in range(0, 224, offset):\n",
    "        for w in range(0, 224, offset):\n",
    "            area = (w, h, w + offset, h + offset)\n",
    "            crop = img.crop(area)\n",
    "            crop = crop.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "            # Include the batch dimension\n",
    "            image_crop_dict[area] = preprocess(crop).clone().detach().unsqueeze(0)\n",
    "    return image_crop_dict\n",
    "\n",
    "\n",
    "def highlight_area(img, region, factor, outline_color=None, outline_width=1):\n",
    "    \"\"\" Highlight specified rectangular region of image by `factor` with an\n",
    "        optional colored  boarder drawn around its edges and return the result.\n",
    "    \"\"\"\n",
    "    img = img.copy()  # Avoid changing original image.\n",
    "    img_crop = img.crop(region)\n",
    "\n",
    "    # brightner = ImageEnhance.Brightness(img_crop)\n",
    "    # img_crop = brightner.enhance(factor)\n",
    "    img.paste(img_crop, region)\n",
    "    # Optionally draw a colored outline around the edge of the rectangular region.\n",
    "    if outline_color:\n",
    "        draw = ImageDraw.Draw(img)  # Create a drawing context.\n",
    "        left, upper, right, lower = region  # Get bounds.\n",
    "        coords = [(left, upper), (right, upper), (right, lower), (left, lower),\n",
    "                  (left, upper)]\n",
    "        draw.line(coords, fill=outline_color, width=outline_width)\n",
    "    return img\n",
    "\n",
    "\n",
    "def viz_pseudo_attn_maps(model, img, text, preprocess, sentences):\n",
    "    image_dict = generate_crop_dict(img, preprocess)\n",
    "    sim_dict = {}\n",
    "    for crop_loc, image in image_dict.items():\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image).float()\n",
    "            text_features = model.encode_text(text).float()\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "            # print(f'similarity = {similarity}')\n",
    "            sim_dict[crop_loc] = similarity #.item()\n",
    "\n",
    "    # print(f'sim_dict={sim_dict}')\n",
    "    pred_choice = 0\n",
    "    bbox = max(sim_dict, key=lambda k: sim_dict[k][pred_choice,0])\n",
    "    # print(f\"Max spot is {bbox}\")\n",
    "    # Pass the original image and the rectangle.\n",
    "    red = ImageColor.getrgb('red')\n",
    "    img2 = highlight_area(img, bbox, 2.5, outline_color=red, outline_width=2)\n",
    "    \n",
    "    # Create a 2D array with the same shape as your image (divided by the crop size)\n",
    "    heatmap = np.zeros((img.size[1], img.size[0]))\n",
    "    for crop_loc, similarity in sim_dict.items():\n",
    "        heatmap[crop_loc[1]:crop_loc[3], crop_loc[0]:crop_loc[2]] = similarity[pred_choice, 0]\n",
    "    # Normalize the heatmap\n",
    "    heatmap = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))\n",
    "    \n",
    "    # Plot original image with bounding box and heatmap side by side\n",
    "    # Create an additional subplot for the colorbar\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(11, 5), gridspec_kw={'width_ratios': [1, 1, 0.05]})\n",
    "    # fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(img2)\n",
    "    axs[0].axis('on')\n",
    "    im = axs[1].imshow(heatmap, cmap='Blues')\n",
    "    # axs[1].imshow(heatmap, cmap='Blues', interpolation='nearest')\n",
    "    # axs[1].colorbar()\n",
    "    axs[1].set_xticks([])\n",
    "    axs[1].set_yticks([])\n",
    "    axs[1].set_frame_on(True)\n",
    "    # Add a colorbar to the heatmap\n",
    "    fig.colorbar(im, cax=axs[2]) #, orientation='vertical'\n",
    "    axs[1].axis('on')\n",
    "    plt.show()\n",
    "    return sim_dict\n",
    "\n",
    "sim_dict = viz_pseudo_attn_maps(model, img, text, preprocess, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indirect Classification benchmark (first convert to caption, then do classfication)\n",
    "import torch, os, open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "pretrained_model=\"\"\n",
    "sentences = [\"key1\", \"key2\"]\n",
    "img_dir = ''\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name=\"coca_ViT-L-14\",\n",
    "    pretrained=pretrained_model,\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "def read_single_image(img_dir, img_path, sentences):\n",
    "    #get the ground truth value\n",
    "    # Extract the filename without extension\n",
    "    filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    image_path = os.path.join(img_dir, img_path)\n",
    "    # Check if any of the words in sentences are in the filename\n",
    "    ground_truth = [word for word in sentences if word in filename][0]\n",
    "    \n",
    "    # load a sample image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = img.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "    im = preprocess(img).unsqueeze(0)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        generated = model.generate(im)\n",
    "    # Generating predicted_caption\n",
    "    predicted_caption = open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\")\n",
    "    class_predict = [s for s in sentences if s in predicted_caption]\n",
    "    \n",
    "    if ground_truth == class_predict[0]:\n",
    "        predicted_correct = 1\n",
    "    else:\n",
    "        predicted_correct = 0\n",
    "    \n",
    "    pred_dict = {'img_path': [img_path], 'predicted_caption': [predicted_caption], 'ground_truth': [ground_truth], 'class_predict': [class_predict[0]], 'predicted_correct': [predicted_correct]}\n",
    "    df_single = pd.DataFrame(pred_dict)\n",
    "    \n",
    "    return df_single\n",
    "\n",
    "\n",
    "all_images = os.listdir(img_dir)\n",
    "for img_path in tqdm(all_images, desc=\"Processing images\", unit=\"image\"):\n",
    "    if '_full_' in img_path: #filaname\n",
    "        continue #skip full images\n",
    "    df = read_single_image(img_dir, img_path, sentences)\n",
    "    df_all = pd.concat([df_all, df]).reset_index(drop=True)\n",
    "\n",
    "# save csv file\n",
    "sentences_str = \"_\".join(sentences)\n",
    "df_all.to_csv(f\"{sentences_str}.csv\", index=False)\n",
    "# Count the number of rows in the 'predicted_correct' column\n",
    "total_rows = len(df_all)\n",
    "# Count the occurrences of '1' in the 'predicted_correct' column\n",
    "count_ones = df_all['predicted_correct'].sum()\n",
    "print(f\"Total images: {total_rows}\")\n",
    "print(f\"predicted_correct': {count_ones}\")\n",
    "predicted_correct_pct = count_ones /total_rows *100\n",
    "print(f\"predicted_correct (%)': {predicted_correct_pct}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
