{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def extract_classification(file_path):\n",
    "    classification = []\n",
    "    df = pd.read_csv(file_path)\n",
    "    classification = df['tissue_types'].tolist()\n",
    "    classification = list(set(classification))\n",
    "    return classification\n",
    "\n",
    "csv_path = '/mnt/e/Temp_data/Body_Parts_XRay/Original/train_df_new1.csv'\n",
    "classification = extract_classification(csv_path)\n",
    "\n",
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification\n",
    "\n",
    "import torch, os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import open_clip\n",
    "\n",
    "pretrained = '/mnt/g/Logtemp/open_clip/Whale_Dolphin_Identification/2023_08_18-21_14_32-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints/epoch_16.pt'\n",
    "# model, _, preprocess = open_clip.create_model_and_transforms('coca_ViT-L-14', pretrained='mscoco_finetuned_laion2B-s13B-b90k')\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "  model_name=\"coca_ViT-L-14\",\n",
    "  pretrained=pretrained, #mscoco_finetuned_laion2B-s13B-b90k\n",
    ")\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "\n",
    "image_path = '/mnt/g/Datasets/Whale_Dolphin_Identification/Square/test/images/false_killer_whale_bdea86a4d11fa9.jpg'\n",
    "img = Image.open(image_path).convert('RGB')\n",
    "img = img.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "image = preprocess(img).unsqueeze(0)\n",
    "text = tokenizer(species)\n",
    "\n",
    "filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "ground_truth = \"_\".join(filename.split('_')[:-1]).replace(\"_\", \" \")\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    # Construct the dictionary\n",
    "    class_predict_dict = dict(zip(species, text_probs))\n",
    "    # Extract the key with the largest value\n",
    "    class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "    \n",
    "    if ground_truth == class_predict:\n",
    "        predicted_correct = 1\n",
    "    else:\n",
    "        predicted_correct = 0\n",
    "\n",
    "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]\n",
    "print(\"Ground truth:\", ground_truth)  # prints: 0\n",
    "print(\"Predicted class:\", class_predict)  # prints: 0\n",
    "print(\"Predicted correct:\", predicted_correct)  # prints: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_1: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_1. Total images: 174, predicted_correct': 20, predicted_correct (%)': 11.494252873563218%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_2: Processing images: 100%|██████████| 174/174 [01:30<00:00,  1.93image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_2. Total images: 174, predicted_correct': 20, predicted_correct (%)': 11.494252873563218%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_3: Processing images: 100%|██████████| 174/174 [01:30<00:00,  1.92image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_3. Total images: 174, predicted_correct': 21, predicted_correct (%)': 12.068965517241379%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_4: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.91image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_4. Total images: 174, predicted_correct': 20, predicted_correct (%)': 11.494252873563218%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_5: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_5. Total images: 174, predicted_correct': 22, predicted_correct (%)': 12.643678160919542%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_6: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.91image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_6. Total images: 174, predicted_correct': 24, predicted_correct (%)': 13.793103448275861%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_7: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_7. Total images: 174, predicted_correct': 25, predicted_correct (%)': 14.367816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_8: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_8. Total images: 174, predicted_correct': 25, predicted_correct (%)': 14.367816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_9: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.91image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_9. Total images: 174, predicted_correct': 25, predicted_correct (%)': 14.367816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_10: Processing images: 100%|██████████| 174/174 [01:30<00:00,  1.92image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_10. Total images: 174, predicted_correct': 22, predicted_correct (%)': 12.643678160919542%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_11: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_11. Total images: 174, predicted_correct': 24, predicted_correct (%)': 13.793103448275861%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_12: Processing images: 100%|██████████| 174/174 [01:30<00:00,  1.92image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_12. Total images: 174, predicted_correct': 25, predicted_correct (%)': 14.367816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_13: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.99image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_13. Total images: 174, predicted_correct': 27, predicted_correct (%)': 15.517241379310345%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_14: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_14. Total images: 174, predicted_correct': 27, predicted_correct (%)': 15.517241379310345%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_15: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_15. Total images: 174, predicted_correct': 36, predicted_correct (%)': 20.689655172413794%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_16: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_16. Total images: 174, predicted_correct': 39, predicted_correct (%)': 22.413793103448278%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_17: Processing images: 100%|██████████| 174/174 [01:39<00:00,  1.75image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_17. Total images: 174, predicted_correct': 43, predicted_correct (%)': 24.71264367816092%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_18: Processing images: 100%|██████████| 174/174 [01:36<00:00,  1.80image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_18. Total images: 174, predicted_correct': 41, predicted_correct (%)': 23.563218390804597%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_19: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_19. Total images: 174, predicted_correct': 45, predicted_correct (%)': 25.862068965517242%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_20: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_20. Total images: 174, predicted_correct': 48, predicted_correct (%)': 27.586206896551722%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_21: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_21. Total images: 174, predicted_correct': 53, predicted_correct (%)': 30.45977011494253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_22: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_22. Total images: 174, predicted_correct': 56, predicted_correct (%)': 32.18390804597701%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_23: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_23. Total images: 174, predicted_correct': 64, predicted_correct (%)': 36.7816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_24: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_24. Total images: 174, predicted_correct': 60, predicted_correct (%)': 34.48275862068966%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_25: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_25. Total images: 174, predicted_correct': 68, predicted_correct (%)': 39.08045977011494%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_26: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_26. Total images: 174, predicted_correct': 63, predicted_correct (%)': 36.206896551724135%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_27: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_27. Total images: 174, predicted_correct': 80, predicted_correct (%)': 45.97701149425287%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_28: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_28. Total images: 174, predicted_correct': 87, predicted_correct (%)': 50.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_29: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_29. Total images: 174, predicted_correct': 91, predicted_correct (%)': 52.29885057471264%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_30: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_30. Total images: 174, predicted_correct': 89, predicted_correct (%)': 51.14942528735632%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_31: Processing images: 100%|██████████| 174/174 [01:36<00:00,  1.80image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_31. Total images: 174, predicted_correct': 98, predicted_correct (%)': 56.32183908045977%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_32: Processing images: 100%|██████████| 174/174 [01:43<00:00,  1.68image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_32. Total images: 174, predicted_correct': 100, predicted_correct (%)': 57.47126436781609%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_33: Processing images: 100%|██████████| 174/174 [01:44<00:00,  1.67image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_33. Total images: 174, predicted_correct': 100, predicted_correct (%)': 57.47126436781609%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_34: Processing images: 100%|██████████| 174/174 [01:35<00:00,  1.82image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_34. Total images: 174, predicted_correct': 102, predicted_correct (%)': 58.620689655172406%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_35: Processing images: 100%|██████████| 174/174 [01:35<00:00,  1.83image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_35. Total images: 174, predicted_correct': 112, predicted_correct (%)': 64.36781609195403%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_36: Processing images: 100%|██████████| 174/174 [01:36<00:00,  1.80image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_36. Total images: 174, predicted_correct': 111, predicted_correct (%)': 63.793103448275865%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_37: Processing images: 100%|██████████| 174/174 [01:36<00:00,  1.81image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_37. Total images: 174, predicted_correct': 116, predicted_correct (%)': 66.66666666666666%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_38: Processing images: 100%|██████████| 174/174 [01:45<00:00,  1.64image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_38. Total images: 174, predicted_correct': 114, predicted_correct (%)': 65.51724137931035%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_39: Processing images: 100%|██████████| 174/174 [01:43<00:00,  1.68image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_39. Total images: 174, predicted_correct': 119, predicted_correct (%)': 68.39080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_40: Processing images: 100%|██████████| 174/174 [01:54<00:00,  1.52image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_40. Total images: 174, predicted_correct': 127, predicted_correct (%)': 72.98850574712644%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_41: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_41. Total images: 174, predicted_correct': 127, predicted_correct (%)': 72.98850574712644%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_42: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_42. Total images: 174, predicted_correct': 127, predicted_correct (%)': 72.98850574712644%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_43: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_43. Total images: 174, predicted_correct': 129, predicted_correct (%)': 74.13793103448276%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_44: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_44. Total images: 174, predicted_correct': 140, predicted_correct (%)': 80.45977011494253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_45: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_45. Total images: 174, predicted_correct': 127, predicted_correct (%)': 72.98850574712644%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_46: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_46. Total images: 174, predicted_correct': 135, predicted_correct (%)': 77.58620689655173%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_47: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_47. Total images: 174, predicted_correct': 141, predicted_correct (%)': 81.03448275862068%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_48: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_48. Total images: 174, predicted_correct': 140, predicted_correct (%)': 80.45977011494253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_49: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_49. Total images: 174, predicted_correct': 139, predicted_correct (%)': 79.88505747126436%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_50: Processing images: 100%|██████████| 174/174 [01:24<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_50. Total images: 174, predicted_correct': 138, predicted_correct (%)': 79.3103448275862%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_51: Processing images: 100%|██████████| 174/174 [01:24<00:00,  2.06image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_51. Total images: 174, predicted_correct': 137, predicted_correct (%)': 78.73563218390804%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_52: Processing images: 100%|██████████| 174/174 [01:23<00:00,  2.08image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_52. Total images: 174, predicted_correct': 138, predicted_correct (%)': 79.3103448275862%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_53: Processing images: 100%|██████████| 174/174 [01:22<00:00,  2.10image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_53. Total images: 174, predicted_correct': 141, predicted_correct (%)': 81.03448275862068%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_54: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.96image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_54. Total images: 174, predicted_correct': 140, predicted_correct (%)': 80.45977011494253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_55: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_55. Total images: 174, predicted_correct': 138, predicted_correct (%)': 79.3103448275862%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_56: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_56. Total images: 174, predicted_correct': 143, predicted_correct (%)': 82.18390804597702%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_57: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_57. Total images: 174, predicted_correct': 147, predicted_correct (%)': 84.48275862068965%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_58: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_58. Total images: 174, predicted_correct': 140, predicted_correct (%)': 80.45977011494253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_59: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_59. Total images: 174, predicted_correct': 136, predicted_correct (%)': 78.16091954022988%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_60: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_60. Total images: 174, predicted_correct': 125, predicted_correct (%)': 71.83908045977012%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_61: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_61. Total images: 174, predicted_correct': 126, predicted_correct (%)': 72.41379310344827%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_62: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_62. Total images: 174, predicted_correct': 126, predicted_correct (%)': 72.41379310344827%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_63: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_63. Total images: 174, predicted_correct': 144, predicted_correct (%)': 82.75862068965517%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_64: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_64. Total images: 174, predicted_correct': 127, predicted_correct (%)': 72.98850574712644%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_65: Processing images: 100%|██████████| 174/174 [01:24<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_65. Total images: 174, predicted_correct': 133, predicted_correct (%)': 76.4367816091954%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_66: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_66. Total images: 174, predicted_correct': 135, predicted_correct (%)': 77.58620689655173%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_67: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_67. Total images: 174, predicted_correct': 140, predicted_correct (%)': 80.45977011494253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_68: Processing images: 100%|██████████| 174/174 [01:24<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_68. Total images: 174, predicted_correct': 133, predicted_correct (%)': 76.4367816091954%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_69: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_69. Total images: 174, predicted_correct': 127, predicted_correct (%)': 72.98850574712644%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_70: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_70. Total images: 174, predicted_correct': 146, predicted_correct (%)': 83.9080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_71: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.04image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_71. Total images: 174, predicted_correct': 116, predicted_correct (%)': 66.66666666666666%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_72: Processing images: 100%|██████████| 174/174 [01:24<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_72. Total images: 174, predicted_correct': 147, predicted_correct (%)': 84.48275862068965%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_73: Processing images: 100%|██████████| 174/174 [01:24<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_73. Total images: 174, predicted_correct': 137, predicted_correct (%)': 78.73563218390804%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_74: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_74. Total images: 174, predicted_correct': 128, predicted_correct (%)': 73.5632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_75: Processing images: 100%|██████████| 174/174 [01:24<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_75. Total images: 174, predicted_correct': 145, predicted_correct (%)': 83.33333333333334%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_76: Processing images: 100%|██████████| 174/174 [01:22<00:00,  2.10image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_76. Total images: 174, predicted_correct': 147, predicted_correct (%)': 84.48275862068965%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_77: Processing images: 100%|██████████| 174/174 [01:22<00:00,  2.11image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_77. Total images: 174, predicted_correct': 138, predicted_correct (%)': 79.3103448275862%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_78: Processing images: 100%|██████████| 174/174 [01:22<00:00,  2.11image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_78. Total images: 174, predicted_correct': 139, predicted_correct (%)': 79.88505747126436%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_79: Processing images: 100%|██████████| 174/174 [01:22<00:00,  2.10image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_79. Total images: 174, predicted_correct': 139, predicted_correct (%)': 79.88505747126436%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_80: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_80. Total images: 174, predicted_correct': 147, predicted_correct (%)': 84.48275862068965%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_81: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_81. Total images: 174, predicted_correct': 135, predicted_correct (%)': 77.58620689655173%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_82: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_82. Total images: 174, predicted_correct': 145, predicted_correct (%)': 83.33333333333334%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_83: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_83. Total images: 174, predicted_correct': 140, predicted_correct (%)': 80.45977011494253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_84: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_84. Total images: 174, predicted_correct': 144, predicted_correct (%)': 82.75862068965517%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_85: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_85. Total images: 174, predicted_correct': 148, predicted_correct (%)': 85.0574712643678%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_86: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_86. Total images: 174, predicted_correct': 145, predicted_correct (%)': 83.33333333333334%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_87: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_87. Total images: 174, predicted_correct': 150, predicted_correct (%)': 86.20689655172413%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_88: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_88. Total images: 174, predicted_correct': 146, predicted_correct (%)': 83.9080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_89: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_89. Total images: 174, predicted_correct': 149, predicted_correct (%)': 85.63218390804597%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_90: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_90. Total images: 174, predicted_correct': 146, predicted_correct (%)': 83.9080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_91: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_91. Total images: 174, predicted_correct': 148, predicted_correct (%)': 85.0574712643678%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_92: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_92. Total images: 174, predicted_correct': 147, predicted_correct (%)': 84.48275862068965%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_93: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.99image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_93. Total images: 174, predicted_correct': 147, predicted_correct (%)': 84.48275862068965%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_94: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_94. Total images: 174, predicted_correct': 148, predicted_correct (%)': 85.0574712643678%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_95: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.99image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_95. Total images: 174, predicted_correct': 153, predicted_correct (%)': 87.93103448275862%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_96: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_96. Total images: 174, predicted_correct': 149, predicted_correct (%)': 85.63218390804597%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_97: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_97. Total images: 174, predicted_correct': 149, predicted_correct (%)': 85.63218390804597%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_98: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_98. Total images: 174, predicted_correct': 149, predicted_correct (%)': 85.63218390804597%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_99: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.96image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_99. Total images: 174, predicted_correct': 146, predicted_correct (%)': 83.9080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch test for Direct Classification benchmark 1\n",
    "import torch, os, open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_classification(file_path):\n",
    "    classification = []\n",
    "    df_classification = pd.read_csv(file_path)\n",
    "    classification = df_classification['tissue_types'].tolist()\n",
    "    classification = list(set(classification))\n",
    "    return classification, df_classification\n",
    "\n",
    "classification_csv_file = '/mnt/g/Datasets/Body_Parts_XRay/train_df_new1.csv'\n",
    "classification, df_classification = extract_classification(classification_csv_file)\n",
    "\n",
    "\n",
    "def process_single_epoch(pretrained_model, sentences):\n",
    "    img_dir = '/mnt/g/Datasets/Body_Parts_XRay/Original/test/images/'\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        model_name=\"coca_ViT-L-14\",\n",
    "        pretrained=pretrained_model,\n",
    "    )\n",
    "    pt_dir = os.path.dirname(pretrained_model)\n",
    "    epoch_number = os.path.basename(pretrained_model).split('epoch_')[1].split('.pt')[0]\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    # Prepare the tokenizer for sentences\n",
    "    tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "    text = tokenizer(sentences)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def read_single_image(img_dir, img_filename, sentences, text_features):\n",
    "        #get the ground truth value\n",
    "        image_path = os.path.join(img_dir, img_filename)\n",
    "        ground_truth = df_classification[df_classification['filename']==img_filename]['tissue_types'].iloc[0]\n",
    "        \n",
    "        # load a sample image\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            text_probs = text_probs.cpu().tolist()[0]\n",
    "\n",
    "        # Construct the dictionary\n",
    "        class_predict_dict = dict(zip(sentences, text_probs))\n",
    "        # Extract the key with the largest value\n",
    "        class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "        \n",
    "        if ground_truth == class_predict:\n",
    "            predicted_correct = 1\n",
    "        else:\n",
    "            predicted_correct = 0\n",
    "        \n",
    "        pred_dict = {'img_filename': [img_filename], 'class_predict_dict': [class_predict_dict], 'ground_truth': [ground_truth], 'class_predict': [class_predict], 'predicted_correct': [predicted_correct]}\n",
    "        df_single = pd.DataFrame(pred_dict)\n",
    "        \n",
    "        return df_single\n",
    "\n",
    "    all_images = os.listdir(img_dir)\n",
    "    for img_filename in tqdm(all_images, desc=f\"epoch_{epoch_number}: Processing images\", unit=\"image\"):\n",
    "        df = read_single_image(img_dir, img_filename, sentences, text_features)\n",
    "        df_all = pd.concat([df_all, df]).reset_index(drop=True)\n",
    "\n",
    "    #save df_all to csv\n",
    "    csv_file = f\"temp/epoch_{epoch_number}.csv\"\n",
    "    df_all.to_csv(csv_file, index=False)\n",
    "\n",
    "    total_rows = len(df_all)\n",
    "    # Count the occurrences of '1' in the 'predicted_correct' column\n",
    "    count_ones = df_all['predicted_correct'].sum()\n",
    "    epoch_filename = os.path.splitext(os.path.basename(pretrained_model))[0]\n",
    "    predicted_correct_pct = count_ones /total_rows *100\n",
    "    print(f\"Epoch_file: {epoch_filename}. Total images: {total_rows}, predicted_correct': {count_ones}, predicted_correct (%)': {predicted_correct_pct}%\")\n",
    "    \n",
    "    epoch_dict = {'pt_dir': [pt_dir],\n",
    "                'epoch_filename': [epoch_filename], \n",
    "                'sentences': [sentences], \n",
    "                'total_rows': [total_rows], \n",
    "                'predicted_correct': [count_ones], \n",
    "                'predicted_correct (%)': [predicted_correct_pct]}\n",
    "    df_single_epoch = pd.DataFrame(epoch_dict)\n",
    "    return df_single_epoch\n",
    "\n",
    "\n",
    "def process_single_pretrained_model(pretrained_model, csv_file):\n",
    "    df_single_pretrained_model = pd.DataFrame()\n",
    "    df = process_single_epoch(pretrained_model, classification)\n",
    "    df_single_pretrained_model = pd.concat([df_single_pretrained_model, df]).reset_index(drop=True)\n",
    "    save_to_csv(df_single_pretrained_model, csv_file)\n",
    "    \n",
    "def extract_epoch_num(filepath):\n",
    "    # Split the filename from the path\n",
    "    filename = os.path.basename(filepath) #filepath.split('/')[-1]\n",
    "    # Extract the number between \"epoch_\" and \".pt\"\n",
    "    epoch_num = int(filename.split('epoch_')[1].split('.pt')[0])\n",
    "    return epoch_num\n",
    "\n",
    "def save_to_csv(df, csv_file):\n",
    "    if not os.path.exists(csv_file):\n",
    "        df.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # If the CSV file already exists, append without header\n",
    "        df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "def main():\n",
    "    pt_folder_path = '/mnt/g/Logtemp/open_clip/Body_Parts_XRay/2023_08_20-12_05_28-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints/'\n",
    "    pt_files = [os.path.join(pt_folder_path, filename) for filename in os.listdir(pt_folder_path) if filename.endswith('.pt')]\n",
    "    #filter pt_files with low epoch number\n",
    "    pt_files = [pt_file for pt_file in pt_files if extract_epoch_num(pt_file) >= 0]\n",
    "    # Sort the list\n",
    "    sorted_pt_files = sorted(pt_files, key=extract_epoch_num)\n",
    "    csv_file = \"benchmark_all_epochs.csv\"\n",
    "    for pretrained_model in sorted_pt_files:\n",
    "        process_single_pretrained_model(pretrained_model, csv_file)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_2: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.05image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_2. Total images: 174, predicted_correct': 23, predicted_correct (%)': 13.218390804597702%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_4: Processing images: 100%|██████████| 174/174 [01:39<00:00,  1.75image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_4. Total images: 174, predicted_correct': 25, predicted_correct (%)': 14.367816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_6: Processing images: 100%|██████████| 174/174 [01:40<00:00,  1.73image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_6. Total images: 174, predicted_correct': 25, predicted_correct (%)': 14.367816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_8: Processing images: 100%|██████████| 174/174 [01:51<00:00,  1.56image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_8. Total images: 174, predicted_correct': 26, predicted_correct (%)': 14.942528735632186%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_10: Processing images: 100%|██████████| 174/174 [01:50<00:00,  1.57image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_10. Total images: 174, predicted_correct': 24, predicted_correct (%)': 13.793103448275861%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_12: Processing images: 100%|██████████| 174/174 [01:42<00:00,  1.70image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_12. Total images: 174, predicted_correct': 22, predicted_correct (%)': 12.643678160919542%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_14: Processing images: 100%|██████████| 174/174 [01:42<00:00,  1.70image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_14. Total images: 174, predicted_correct': 32, predicted_correct (%)': 18.39080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_16: Processing images: 100%|██████████| 174/174 [01:49<00:00,  1.58image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_16. Total images: 174, predicted_correct': 41, predicted_correct (%)': 23.563218390804597%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_18: Processing images: 100%|██████████| 174/174 [01:44<00:00,  1.67image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_18. Total images: 174, predicted_correct': 43, predicted_correct (%)': 24.71264367816092%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_20: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_20. Total images: 174, predicted_correct': 51, predicted_correct (%)': 29.310344827586203%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_22: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.60image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_22. Total images: 174, predicted_correct': 53, predicted_correct (%)': 30.45977011494253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_24: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.60image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_24. Total images: 174, predicted_correct': 61, predicted_correct (%)': 35.05747126436782%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_26: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_26. Total images: 174, predicted_correct': 61, predicted_correct (%)': 35.05747126436782%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_28: Processing images: 100%|██████████| 174/174 [01:45<00:00,  1.65image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_28. Total images: 174, predicted_correct': 73, predicted_correct (%)': 41.95402298850575%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_30: Processing images: 100%|██████████| 174/174 [01:44<00:00,  1.67image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_30. Total images: 174, predicted_correct': 76, predicted_correct (%)': 43.67816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_32: Processing images: 100%|██████████| 174/174 [01:44<00:00,  1.67image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_32. Total images: 174, predicted_correct': 97, predicted_correct (%)': 55.74712643678161%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_34: Processing images: 100%|██████████| 174/174 [01:43<00:00,  1.69image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_34. Total images: 174, predicted_correct': 94, predicted_correct (%)': 54.02298850574713%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_36: Processing images: 100%|██████████| 174/174 [01:45<00:00,  1.66image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_36. Total images: 174, predicted_correct': 101, predicted_correct (%)': 58.04597701149425%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_38: Processing images: 100%|██████████| 174/174 [01:46<00:00,  1.64image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_38. Total images: 174, predicted_correct': 109, predicted_correct (%)': 62.643678160919535%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_40: Processing images: 100%|██████████| 174/174 [01:43<00:00,  1.68image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_40. Total images: 174, predicted_correct': 128, predicted_correct (%)': 73.5632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_42: Processing images: 100%|██████████| 174/174 [01:46<00:00,  1.63image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_42. Total images: 174, predicted_correct': 130, predicted_correct (%)': 74.71264367816092%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_44: Processing images: 100%|██████████| 174/174 [01:45<00:00,  1.66image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_44. Total images: 174, predicted_correct': 135, predicted_correct (%)': 77.58620689655173%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_46: Processing images: 100%|██████████| 174/174 [01:44<00:00,  1.67image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_46. Total images: 174, predicted_correct': 131, predicted_correct (%)': 75.28735632183908%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_48: Processing images: 100%|██████████| 174/174 [01:44<00:00,  1.67image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_48. Total images: 174, predicted_correct': 139, predicted_correct (%)': 79.88505747126436%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_50: Processing images: 100%|██████████| 174/174 [01:43<00:00,  1.68image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_50. Total images: 174, predicted_correct': 135, predicted_correct (%)': 77.58620689655173%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_52: Processing images: 100%|██████████| 174/174 [01:36<00:00,  1.80image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_52. Total images: 174, predicted_correct': 136, predicted_correct (%)': 78.16091954022988%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_54: Processing images: 100%|██████████| 174/174 [01:42<00:00,  1.70image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_54. Total images: 174, predicted_correct': 139, predicted_correct (%)': 79.88505747126436%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_56: Processing images: 100%|██████████| 174/174 [01:46<00:00,  1.63image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_56. Total images: 174, predicted_correct': 137, predicted_correct (%)': 78.73563218390804%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_58: Processing images: 100%|██████████| 174/174 [01:43<00:00,  1.68image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_58. Total images: 174, predicted_correct': 132, predicted_correct (%)': 75.86206896551724%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_60: Processing images: 100%|██████████| 174/174 [01:44<00:00,  1.66image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_60. Total images: 174, predicted_correct': 140, predicted_correct (%)': 80.45977011494253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_62: Processing images: 100%|██████████| 174/174 [01:43<00:00,  1.69image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_62. Total images: 174, predicted_correct': 131, predicted_correct (%)': 75.28735632183908%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_64: Processing images: 100%|██████████| 174/174 [01:54<00:00,  1.52image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_64. Total images: 174, predicted_correct': 131, predicted_correct (%)': 75.28735632183908%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_66: Processing images: 100%|██████████| 174/174 [02:06<00:00,  1.37image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_66. Total images: 174, predicted_correct': 136, predicted_correct (%)': 78.16091954022988%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_68: Processing images: 100%|██████████| 174/174 [01:56<00:00,  1.50image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_68. Total images: 174, predicted_correct': 140, predicted_correct (%)': 80.45977011494253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_70: Processing images: 100%|██████████| 174/174 [01:55<00:00,  1.51image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_70. Total images: 174, predicted_correct': 116, predicted_correct (%)': 66.66666666666666%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_72: Processing images: 100%|██████████| 174/174 [01:50<00:00,  1.57image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_72. Total images: 174, predicted_correct': 145, predicted_correct (%)': 83.33333333333334%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_74: Processing images: 100%|██████████| 174/174 [01:45<00:00,  1.65image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_74. Total images: 174, predicted_correct': 129, predicted_correct (%)': 74.13793103448276%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_76: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_76. Total images: 174, predicted_correct': 127, predicted_correct (%)': 72.98850574712644%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_78: Processing images: 100%|██████████| 174/174 [01:41<00:00,  1.71image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_78. Total images: 174, predicted_correct': 138, predicted_correct (%)': 79.3103448275862%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_80: Processing images: 100%|██████████| 174/174 [01:46<00:00,  1.64image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_80. Total images: 174, predicted_correct': 118, predicted_correct (%)': 67.81609195402298%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_82: Processing images: 100%|██████████| 174/174 [01:41<00:00,  1.72image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_82. Total images: 174, predicted_correct': 129, predicted_correct (%)': 74.13793103448276%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_84: Processing images: 100%|██████████| 174/174 [01:40<00:00,  1.73image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_84. Total images: 174, predicted_correct': 148, predicted_correct (%)': 85.0574712643678%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_86: Processing images: 100%|██████████| 174/174 [01:45<00:00,  1.65image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_86. Total images: 174, predicted_correct': 150, predicted_correct (%)': 86.20689655172413%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_88: Processing images: 100%|██████████| 174/174 [01:41<00:00,  1.71image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_88. Total images: 174, predicted_correct': 151, predicted_correct (%)': 86.7816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_90: Processing images: 100%|██████████| 174/174 [01:42<00:00,  1.70image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_90. Total images: 174, predicted_correct': 151, predicted_correct (%)': 86.7816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_92: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.60image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_92. Total images: 174, predicted_correct': 151, predicted_correct (%)': 86.7816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_94: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_94. Total images: 174, predicted_correct': 152, predicted_correct (%)': 87.35632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_96: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_96. Total images: 174, predicted_correct': 152, predicted_correct (%)': 87.35632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_98: Processing images: 100%|██████████| 174/174 [01:41<00:00,  1.72image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_98. Total images: 174, predicted_correct': 145, predicted_correct (%)': 83.33333333333334%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch test for Direct Classification benchmark 2 (cropped square images)\n",
    "import torch, os, open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_classification(file_path):\n",
    "    classification = []\n",
    "    df_classification = pd.read_csv(file_path)\n",
    "    classification = df_classification['tissue_types'].tolist()\n",
    "    classification = list(set(classification))\n",
    "    return classification, df_classification\n",
    "\n",
    "classification_csv_file = '/mnt/g/Datasets/Body_Parts_XRay/train_df_new1.csv'\n",
    "classification, df_classification = extract_classification(classification_csv_file)\n",
    "\n",
    "\n",
    "def process_single_epoch(pretrained_model, sentences):\n",
    "    img_dir = '/mnt/g/Datasets/Body_Parts_XRay/Square/test/images/'\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        model_name=\"coca_ViT-L-14\",\n",
    "        pretrained=pretrained_model,\n",
    "    )\n",
    "    pt_dir = os.path.dirname(pretrained_model)\n",
    "    epoch_number = os.path.basename(pretrained_model).split('epoch_')[1].split('.pt')[0]\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    # Prepare the tokenizer for sentences\n",
    "    tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "    text = tokenizer(sentences)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def read_single_image(img_dir, img_filename, sentences, text_features):\n",
    "        #get the ground truth value\n",
    "        image_path = os.path.join(img_dir, img_filename)\n",
    "        ground_truth = df_classification[df_classification['filename']==img_filename]['tissue_types'].iloc[0]\n",
    "        \n",
    "        # load a sample image\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            text_probs = text_probs.cpu().tolist()[0]\n",
    "\n",
    "        # Construct the dictionary\n",
    "        class_predict_dict = dict(zip(sentences, text_probs))\n",
    "        # Extract the key with the largest value\n",
    "        class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "        \n",
    "        if ground_truth == class_predict:\n",
    "            predicted_correct = 1\n",
    "        else:\n",
    "            predicted_correct = 0\n",
    "        \n",
    "        pred_dict = {'img_filename': [img_filename], 'class_predict_dict': [class_predict_dict], 'ground_truth': [ground_truth], 'class_predict': [class_predict], 'predicted_correct': [predicted_correct]}\n",
    "        df_single = pd.DataFrame(pred_dict)\n",
    "        \n",
    "        return df_single\n",
    "\n",
    "    all_images = os.listdir(img_dir)\n",
    "    for img_filename in tqdm(all_images, desc=f\"epoch_{epoch_number}: Processing images\", unit=\"image\"):\n",
    "        df = read_single_image(img_dir, img_filename, sentences, text_features)\n",
    "        df_all = pd.concat([df_all, df]).reset_index(drop=True)\n",
    "\n",
    "    #save df_all to csv\n",
    "    csv_file = f\"temp/epoch_{epoch_number}.csv\"\n",
    "    df_all.to_csv(csv_file, index=False)\n",
    "\n",
    "    total_rows = len(df_all)\n",
    "    # Count the occurrences of '1' in the 'predicted_correct' column\n",
    "    count_ones = df_all['predicted_correct'].sum()\n",
    "    epoch_filename = os.path.splitext(os.path.basename(pretrained_model))[0]\n",
    "    predicted_correct_pct = count_ones /total_rows *100\n",
    "    print(f\"Epoch_file: {epoch_filename}. Total images: {total_rows}, predicted_correct': {count_ones}, predicted_correct (%)': {predicted_correct_pct}%\")\n",
    "    \n",
    "    epoch_dict = {'pt_dir': [pt_dir],\n",
    "                'epoch_filename': [epoch_filename], \n",
    "                'sentences': [sentences], \n",
    "                'total_rows': [total_rows], \n",
    "                'predicted_correct': [count_ones], \n",
    "                'predicted_correct (%)': [predicted_correct_pct]}\n",
    "    df_single_epoch = pd.DataFrame(epoch_dict)\n",
    "    return df_single_epoch\n",
    "\n",
    "\n",
    "def process_single_pretrained_model(pretrained_model, csv_file):\n",
    "    df_single_pretrained_model = pd.DataFrame()\n",
    "    df = process_single_epoch(pretrained_model, classification)\n",
    "    df_single_pretrained_model = pd.concat([df_single_pretrained_model, df]).reset_index(drop=True)\n",
    "    save_to_csv(df_single_pretrained_model, csv_file)\n",
    "    \n",
    "def extract_epoch_num(filepath):\n",
    "    # Split the filename from the path\n",
    "    filename = os.path.basename(filepath) #filepath.split('/')[-1]\n",
    "    # Extract the number between \"epoch_\" and \".pt\"\n",
    "    epoch_num = int(filename.split('epoch_')[1].split('.pt')[0])\n",
    "    return epoch_num\n",
    "\n",
    "def save_to_csv(df, csv_file):\n",
    "    if not os.path.exists(csv_file):\n",
    "        df.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # If the CSV file already exists, append without header\n",
    "        df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "def main():\n",
    "    pt_folder_path = '/mnt/g/Logtemp/open_clip/Body_Parts_XRay/2023_08_21-19_57_50-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints/'\n",
    "    pt_files = [os.path.join(pt_folder_path, filename) for filename in os.listdir(pt_folder_path) if filename.endswith('.pt')]\n",
    "    #filter pt_files with low epoch number\n",
    "    pt_files = [pt_file for pt_file in pt_files if extract_epoch_num(pt_file) >= 68]\n",
    "    # Sort the list\n",
    "    sorted_pt_files = sorted(pt_files, key=extract_epoch_num)\n",
    "    csv_file = \"benchmark_all_epochs.csv\"\n",
    "    for pretrained_model in sorted_pt_files:\n",
    "        process_single_pretrained_model(pretrained_model, csv_file)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_2: Processing images: 100%|██████████| 174/174 [01:44<00:00,  1.67image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_2. Total images: 174, predicted_correct': 23, predicted_correct (%)': 13.218390804597702%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_4: Processing images: 100%|██████████| 174/174 [01:35<00:00,  1.82image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_4. Total images: 174, predicted_correct': 23, predicted_correct (%)': 13.218390804597702%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_6: Processing images: 100%|██████████| 174/174 [01:34<00:00,  1.84image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_6. Total images: 174, predicted_correct': 25, predicted_correct (%)': 14.367816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_8: Processing images: 100%|██████████| 174/174 [01:35<00:00,  1.81image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_8. Total images: 174, predicted_correct': 26, predicted_correct (%)': 14.942528735632186%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_10: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_10. Total images: 174, predicted_correct': 23, predicted_correct (%)': 13.218390804597702%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_12: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_12. Total images: 174, predicted_correct': 23, predicted_correct (%)': 13.218390804597702%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_14: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_14. Total images: 174, predicted_correct': 27, predicted_correct (%)': 15.517241379310345%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_16: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_16. Total images: 174, predicted_correct': 29, predicted_correct (%)': 16.666666666666664%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_18: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_18. Total images: 174, predicted_correct': 28, predicted_correct (%)': 16.091954022988507%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_20: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_20. Total images: 174, predicted_correct': 32, predicted_correct (%)': 18.39080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_22: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_22. Total images: 174, predicted_correct': 41, predicted_correct (%)': 23.563218390804597%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_24: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_24. Total images: 174, predicted_correct': 42, predicted_correct (%)': 24.137931034482758%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_26: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_26. Total images: 174, predicted_correct': 42, predicted_correct (%)': 24.137931034482758%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_28: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.99image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_28. Total images: 174, predicted_correct': 42, predicted_correct (%)': 24.137931034482758%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_30: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_30. Total images: 174, predicted_correct': 47, predicted_correct (%)': 27.011494252873565%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_32: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_32. Total images: 174, predicted_correct': 59, predicted_correct (%)': 33.90804597701149%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_34: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_34. Total images: 174, predicted_correct': 61, predicted_correct (%)': 35.05747126436782%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_36: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_36. Total images: 174, predicted_correct': 58, predicted_correct (%)': 33.33333333333333%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_38: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_38. Total images: 174, predicted_correct': 81, predicted_correct (%)': 46.55172413793103%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_40: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_40. Total images: 174, predicted_correct': 101, predicted_correct (%)': 58.04597701149425%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_42: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_42. Total images: 174, predicted_correct': 84, predicted_correct (%)': 48.275862068965516%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_44: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_44. Total images: 174, predicted_correct': 100, predicted_correct (%)': 57.47126436781609%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_46: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_46. Total images: 174, predicted_correct': 93, predicted_correct (%)': 53.44827586206896%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_48: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_48. Total images: 174, predicted_correct': 99, predicted_correct (%)': 56.896551724137936%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_50: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_50. Total images: 174, predicted_correct': 114, predicted_correct (%)': 65.51724137931035%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_52: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_52. Total images: 174, predicted_correct': 108, predicted_correct (%)': 62.06896551724138%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_54: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_54. Total images: 174, predicted_correct': 120, predicted_correct (%)': 68.96551724137932%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_56: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_56. Total images: 174, predicted_correct': 118, predicted_correct (%)': 67.81609195402298%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_58: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_58. Total images: 174, predicted_correct': 83, predicted_correct (%)': 47.701149425287355%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_60: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_60. Total images: 174, predicted_correct': 86, predicted_correct (%)': 49.42528735632184%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_62: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_62. Total images: 174, predicted_correct': 102, predicted_correct (%)': 58.620689655172406%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_64: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_64. Total images: 174, predicted_correct': 90, predicted_correct (%)': 51.724137931034484%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_66: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_66. Total images: 174, predicted_correct': 94, predicted_correct (%)': 54.02298850574713%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_68: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_68. Total images: 174, predicted_correct': 88, predicted_correct (%)': 50.57471264367817%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_70: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_70. Total images: 174, predicted_correct': 91, predicted_correct (%)': 52.29885057471264%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_72: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_72. Total images: 174, predicted_correct': 93, predicted_correct (%)': 53.44827586206896%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_74: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_74. Total images: 174, predicted_correct': 95, predicted_correct (%)': 54.59770114942529%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_76: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_76. Total images: 174, predicted_correct': 103, predicted_correct (%)': 59.195402298850574%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_78: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_78. Total images: 174, predicted_correct': 112, predicted_correct (%)': 64.36781609195403%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_80: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_80. Total images: 174, predicted_correct': 103, predicted_correct (%)': 59.195402298850574%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_82: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_82. Total images: 174, predicted_correct': 86, predicted_correct (%)': 49.42528735632184%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_84: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_84. Total images: 174, predicted_correct': 100, predicted_correct (%)': 57.47126436781609%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_86: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_86. Total images: 174, predicted_correct': 113, predicted_correct (%)': 64.9425287356322%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_88: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_88. Total images: 174, predicted_correct': 95, predicted_correct (%)': 54.59770114942529%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_90: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_90. Total images: 174, predicted_correct': 115, predicted_correct (%)': 66.0919540229885%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_92: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_92. Total images: 174, predicted_correct': 104, predicted_correct (%)': 59.77011494252874%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_94: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_94. Total images: 174, predicted_correct': 94, predicted_correct (%)': 54.02298850574713%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_96: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_96. Total images: 174, predicted_correct': 116, predicted_correct (%)': 66.66666666666666%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_98: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_98. Total images: 174, predicted_correct': 96, predicted_correct (%)': 55.172413793103445%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_100: Processing images: 100%|██████████| 174/174 [01:27<00:00,  1.98image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_100. Total images: 174, predicted_correct': 106, predicted_correct (%)': 60.91954022988506%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_102: Processing images: 100%|██████████| 174/174 [01:27<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_102. Total images: 174, predicted_correct': 86, predicted_correct (%)': 49.42528735632184%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_104: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_104. Total images: 174, predicted_correct': 128, predicted_correct (%)': 73.5632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_106: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_106. Total images: 174, predicted_correct': 113, predicted_correct (%)': 64.9425287356322%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_108: Processing images: 100%|██████████| 174/174 [01:30<00:00,  1.93image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_108. Total images: 174, predicted_correct': 105, predicted_correct (%)': 60.3448275862069%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_110: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_110. Total images: 174, predicted_correct': 119, predicted_correct (%)': 68.39080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_112: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_112. Total images: 174, predicted_correct': 123, predicted_correct (%)': 70.6896551724138%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_114: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_114. Total images: 174, predicted_correct': 117, predicted_correct (%)': 67.24137931034483%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_116: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_116. Total images: 174, predicted_correct': 117, predicted_correct (%)': 67.24137931034483%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_118: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_118. Total images: 174, predicted_correct': 124, predicted_correct (%)': 71.26436781609196%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_120: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_120. Total images: 174, predicted_correct': 126, predicted_correct (%)': 72.41379310344827%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_122: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_122. Total images: 174, predicted_correct': 135, predicted_correct (%)': 77.58620689655173%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_124: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_124. Total images: 174, predicted_correct': 149, predicted_correct (%)': 85.63218390804597%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_126: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_126. Total images: 174, predicted_correct': 131, predicted_correct (%)': 75.28735632183908%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_128: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_128. Total images: 174, predicted_correct': 150, predicted_correct (%)': 86.20689655172413%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_130: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_130. Total images: 174, predicted_correct': 132, predicted_correct (%)': 75.86206896551724%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_132: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_132. Total images: 174, predicted_correct': 146, predicted_correct (%)': 83.9080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_134: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_134. Total images: 174, predicted_correct': 148, predicted_correct (%)': 85.0574712643678%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_136: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_136. Total images: 174, predicted_correct': 130, predicted_correct (%)': 74.71264367816092%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_138: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_138. Total images: 174, predicted_correct': 140, predicted_correct (%)': 80.45977011494253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_140: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.86image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_140. Total images: 174, predicted_correct': 151, predicted_correct (%)': 86.7816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_142: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.85image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_142. Total images: 174, predicted_correct': 154, predicted_correct (%)': 88.50574712643679%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_144: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_144. Total images: 174, predicted_correct': 146, predicted_correct (%)': 83.9080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_146: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_146. Total images: 174, predicted_correct': 151, predicted_correct (%)': 86.7816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_148: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_148. Total images: 174, predicted_correct': 149, predicted_correct (%)': 85.63218390804597%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_150: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_150. Total images: 174, predicted_correct': 146, predicted_correct (%)': 83.9080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_152: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_152. Total images: 174, predicted_correct': 146, predicted_correct (%)': 83.9080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_154: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_154. Total images: 174, predicted_correct': 146, predicted_correct (%)': 83.9080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_156: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_156. Total images: 174, predicted_correct': 139, predicted_correct (%)': 79.88505747126436%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_158: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_158. Total images: 174, predicted_correct': 151, predicted_correct (%)': 86.7816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_160: Processing images: 100%|██████████| 174/174 [01:33<00:00,  1.87image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_160. Total images: 174, predicted_correct': 155, predicted_correct (%)': 89.08045977011494%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_162: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_162. Total images: 174, predicted_correct': 149, predicted_correct (%)': 85.63218390804597%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_164: Processing images: 100%|██████████| 174/174 [01:29<00:00,  1.95image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_164. Total images: 174, predicted_correct': 153, predicted_correct (%)': 87.93103448275862%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_166: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_166. Total images: 174, predicted_correct': 151, predicted_correct (%)': 86.7816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_168: Processing images: 100%|██████████| 174/174 [01:27<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_168. Total images: 174, predicted_correct': 150, predicted_correct (%)': 86.20689655172413%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_170: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_170. Total images: 174, predicted_correct': 145, predicted_correct (%)': 83.33333333333334%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_172: Processing images: 100%|██████████| 174/174 [01:27<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_172. Total images: 174, predicted_correct': 151, predicted_correct (%)': 86.7816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_174: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_174. Total images: 174, predicted_correct': 152, predicted_correct (%)': 87.35632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_176: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_176. Total images: 174, predicted_correct': 150, predicted_correct (%)': 86.20689655172413%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_178: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_178. Total images: 174, predicted_correct': 152, predicted_correct (%)': 87.35632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_180: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_180. Total images: 174, predicted_correct': 152, predicted_correct (%)': 87.35632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_182: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_182. Total images: 174, predicted_correct': 152, predicted_correct (%)': 87.35632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_184: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_184. Total images: 174, predicted_correct': 152, predicted_correct (%)': 87.35632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_186: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.00image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_186. Total images: 174, predicted_correct': 151, predicted_correct (%)': 86.7816091954023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_188: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.01image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_188. Total images: 174, predicted_correct': 152, predicted_correct (%)': 87.35632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_190: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_190. Total images: 174, predicted_correct': 154, predicted_correct (%)': 88.50574712643679%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_192: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_192. Total images: 174, predicted_correct': 154, predicted_correct (%)': 88.50574712643679%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_194: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_194. Total images: 174, predicted_correct': 154, predicted_correct (%)': 88.50574712643679%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_196: Processing images: 100%|██████████| 174/174 [01:38<00:00,  1.77image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_196. Total images: 174, predicted_correct': 154, predicted_correct (%)': 88.50574712643679%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_198: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_198. Total images: 174, predicted_correct': 154, predicted_correct (%)': 88.50574712643679%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_200: Processing images: 100%|██████████| 174/174 [01:28<00:00,  1.96image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_200. Total images: 174, predicted_correct': 154, predicted_correct (%)': 88.50574712643679%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch test for Direct Classification benchmark 3 (cropped square & data augmentation images)\n",
    "import torch, os, open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import Normalize, ToTensor\n",
    "\n",
    "def _convert_to_rgb(image):\n",
    "    return image.convert('RGB')\n",
    "mean = (0.3453, 0.3453, 0.3453)\n",
    "std = (0.2566, 0.2566, 0.2566)\n",
    "to_tensor = ToTensor()\n",
    "normalize = Normalize(mean=mean, std=std)\n",
    "\n",
    "def extract_classification(file_path):\n",
    "    classification = []\n",
    "    df_classification = pd.read_csv(file_path)\n",
    "    classification = df_classification['tissue_types'].tolist()\n",
    "    classification = list(set(classification))\n",
    "    return classification, df_classification\n",
    "\n",
    "classification_csv_file = '/mnt/g/Datasets/Body_Parts_XRay/train_df_new1.csv'\n",
    "classification, df_classification = extract_classification(classification_csv_file)\n",
    "\n",
    "\n",
    "def process_single_epoch(pretrained_model, sentences):\n",
    "    img_dir = '/mnt/g/Datasets/Body_Parts_XRay/Square/test/images/'\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        model_name=\"coca_ViT-L-14\",\n",
    "        pretrained=pretrained_model,\n",
    "    )\n",
    "    pt_dir = os.path.dirname(pretrained_model)\n",
    "    epoch_number = os.path.basename(pretrained_model).split('epoch_')[1].split('.pt')[0]\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    # Prepare the tokenizer for sentences\n",
    "    tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "    text = tokenizer(sentences)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def read_single_image(img_dir, img_filename, sentences, text_features):\n",
    "        #get the ground truth value\n",
    "        image_path = os.path.join(img_dir, img_filename)\n",
    "        ground_truth = df_classification[df_classification['filename']==img_filename]['tissue_types'].iloc[0]\n",
    "        \n",
    "        # load a sample image\n",
    "        image = preprocess(Image.open(image_path)) #.unsqueeze(0)\n",
    "        image = _convert_to_rgb(image)\n",
    "        image = to_tensor(image)\n",
    "        image = normalize(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            text_probs = text_probs.cpu().tolist()[0]\n",
    "\n",
    "        # Construct the dictionary\n",
    "        class_predict_dict = dict(zip(sentences, text_probs))\n",
    "        # Extract the key with the largest value\n",
    "        class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "        \n",
    "        if ground_truth == class_predict:\n",
    "            predicted_correct = 1\n",
    "        else:\n",
    "            predicted_correct = 0\n",
    "        \n",
    "        pred_dict = {'img_filename': [img_filename], 'class_predict_dict': [class_predict_dict], 'ground_truth': [ground_truth], 'class_predict': [class_predict], 'predicted_correct': [predicted_correct]}\n",
    "        df_single = pd.DataFrame(pred_dict)\n",
    "        \n",
    "        return df_single\n",
    "\n",
    "    all_images = os.listdir(img_dir)\n",
    "    for img_filename in tqdm(all_images, desc=f\"epoch_{epoch_number}: Processing images\", unit=\"image\"):\n",
    "        df = read_single_image(img_dir, img_filename, sentences, text_features)\n",
    "        df_all = pd.concat([df_all, df]).reset_index(drop=True)\n",
    "\n",
    "    #save df_all to csv\n",
    "    csv_file = f\"temp/epoch_{epoch_number}.csv\"\n",
    "    df_all.to_csv(csv_file, index=False)\n",
    "\n",
    "    total_rows = len(df_all)\n",
    "    # Count the occurrences of '1' in the 'predicted_correct' column\n",
    "    count_ones = df_all['predicted_correct'].sum()\n",
    "    epoch_filename = os.path.splitext(os.path.basename(pretrained_model))[0]\n",
    "    predicted_correct_pct = count_ones /total_rows *100\n",
    "    print(f\"Epoch_file: {epoch_filename}. Total images: {total_rows}, predicted_correct': {count_ones}, predicted_correct (%)': {predicted_correct_pct}%\")\n",
    "    \n",
    "    epoch_dict = {'pt_dir': [pt_dir],\n",
    "                'epoch_filename': [epoch_filename], \n",
    "                'sentences': [sentences], \n",
    "                'total_rows': [total_rows], \n",
    "                'predicted_correct': [count_ones], \n",
    "                'predicted_correct (%)': [predicted_correct_pct]}\n",
    "    df_single_epoch = pd.DataFrame(epoch_dict)\n",
    "    return df_single_epoch\n",
    "\n",
    "\n",
    "def process_single_pretrained_model(pretrained_model, csv_file):\n",
    "    df_single_pretrained_model = pd.DataFrame()\n",
    "    df = process_single_epoch(pretrained_model, classification)\n",
    "    df_single_pretrained_model = pd.concat([df_single_pretrained_model, df]).reset_index(drop=True)\n",
    "    save_to_csv(df_single_pretrained_model, csv_file)\n",
    "    \n",
    "def extract_epoch_num(filepath):\n",
    "    # Split the filename from the path\n",
    "    filename = os.path.basename(filepath) #filepath.split('/')[-1]\n",
    "    # Extract the number between \"epoch_\" and \".pt\"\n",
    "    epoch_num = int(filename.split('epoch_')[1].split('.pt')[0])\n",
    "    return epoch_num\n",
    "\n",
    "def save_to_csv(df, csv_file):\n",
    "    if not os.path.exists(csv_file):\n",
    "        df.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # If the CSV file already exists, append without header\n",
    "        df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "def main():\n",
    "    pt_folder_path = '/mnt/g/Logtemp/open_clip/Body_Parts_XRay/2023_08_21-23_03_36-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints/'\n",
    "    pt_files = [os.path.join(pt_folder_path, filename) for filename in os.listdir(pt_folder_path) if filename.endswith('.pt')]\n",
    "    #filter pt_files with low epoch number\n",
    "    pt_files = [pt_file for pt_file in pt_files if extract_epoch_num(pt_file) >= 0]\n",
    "    # Sort the list\n",
    "    sorted_pt_files = sorted(pt_files, key=extract_epoch_num)\n",
    "    csv_file = \"benchmark_all_epochs_augment1.csv\"\n",
    "    for pretrained_model in sorted_pt_files:\n",
    "        process_single_pretrained_model(pretrained_model, csv_file)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_2: Processing images: 100%|██████████| 174/174 [01:57<00:00,  1.48image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_2. Total images: 174, predicted_correct': 33, predicted_correct (%)': 18.96551724137931%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_4: Processing images: 100%|██████████| 174/174 [01:55<00:00,  1.50image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_4. Total images: 174, predicted_correct': 32, predicted_correct (%)': 18.39080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_6: Processing images: 100%|██████████| 174/174 [01:50<00:00,  1.58image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_6. Total images: 174, predicted_correct': 39, predicted_correct (%)': 22.413793103448278%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_8: Processing images: 100%|██████████| 174/174 [01:49<00:00,  1.58image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_8. Total images: 174, predicted_correct': 40, predicted_correct (%)': 22.988505747126435%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_10: Processing images: 100%|██████████| 174/174 [01:45<00:00,  1.65image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_10. Total images: 174, predicted_correct': 44, predicted_correct (%)': 25.287356321839084%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_12: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.60image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_12. Total images: 174, predicted_correct': 44, predicted_correct (%)': 25.287356321839084%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_14: Processing images: 100%|██████████| 174/174 [01:46<00:00,  1.63image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_14. Total images: 174, predicted_correct': 43, predicted_correct (%)': 24.71264367816092%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_16: Processing images: 100%|██████████| 174/174 [01:46<00:00,  1.64image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_16. Total images: 174, predicted_correct': 48, predicted_correct (%)': 27.586206896551722%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_18: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_18. Total images: 174, predicted_correct': 48, predicted_correct (%)': 27.586206896551722%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_20: Processing images: 100%|██████████| 174/174 [01:44<00:00,  1.67image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_20. Total images: 174, predicted_correct': 52, predicted_correct (%)': 29.88505747126437%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_22: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_22. Total images: 174, predicted_correct': 51, predicted_correct (%)': 29.310344827586203%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_24: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_24. Total images: 174, predicted_correct': 50, predicted_correct (%)': 28.735632183908045%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_26: Processing images: 100%|██████████| 174/174 [01:45<00:00,  1.65image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_26. Total images: 174, predicted_correct': 47, predicted_correct (%)': 27.011494252873565%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_28: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_28. Total images: 174, predicted_correct': 58, predicted_correct (%)': 33.33333333333333%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_30: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_30. Total images: 174, predicted_correct': 67, predicted_correct (%)': 38.50574712643678%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_32: Processing images: 100%|██████████| 174/174 [01:46<00:00,  1.64image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_32. Total images: 174, predicted_correct': 81, predicted_correct (%)': 46.55172413793103%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_34: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_34. Total images: 174, predicted_correct': 86, predicted_correct (%)': 49.42528735632184%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_36: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_36. Total images: 174, predicted_correct': 81, predicted_correct (%)': 46.55172413793103%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_38: Processing images: 100%|██████████| 174/174 [01:45<00:00,  1.64image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_38. Total images: 174, predicted_correct': 104, predicted_correct (%)': 59.77011494252874%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_40: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_40. Total images: 174, predicted_correct': 117, predicted_correct (%)': 67.24137931034483%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_42: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.60image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_42. Total images: 174, predicted_correct': 107, predicted_correct (%)': 61.49425287356321%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_44: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_44. Total images: 174, predicted_correct': 116, predicted_correct (%)': 66.66666666666666%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_46: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_46. Total images: 174, predicted_correct': 109, predicted_correct (%)': 62.643678160919535%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_48: Processing images: 100%|██████████| 174/174 [01:42<00:00,  1.70image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_48. Total images: 174, predicted_correct': 112, predicted_correct (%)': 64.36781609195403%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_50: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_50. Total images: 174, predicted_correct': 124, predicted_correct (%)': 71.26436781609196%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_52: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_52. Total images: 174, predicted_correct': 113, predicted_correct (%)': 64.9425287356322%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_54: Processing images: 100%|██████████| 174/174 [01:49<00:00,  1.60image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_54. Total images: 174, predicted_correct': 131, predicted_correct (%)': 75.28735632183908%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_56: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_56. Total images: 174, predicted_correct': 132, predicted_correct (%)': 75.86206896551724%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_58: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_58. Total images: 174, predicted_correct': 105, predicted_correct (%)': 60.3448275862069%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_60: Processing images: 100%|██████████| 174/174 [01:46<00:00,  1.64image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_60. Total images: 174, predicted_correct': 106, predicted_correct (%)': 60.91954022988506%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_62: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.63image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_62. Total images: 174, predicted_correct': 117, predicted_correct (%)': 67.24137931034483%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_64: Processing images: 100%|██████████| 174/174 [01:49<00:00,  1.59image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_64. Total images: 174, predicted_correct': 105, predicted_correct (%)': 60.3448275862069%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_66: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_66. Total images: 174, predicted_correct': 110, predicted_correct (%)': 63.2183908045977%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_68: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_68. Total images: 174, predicted_correct': 109, predicted_correct (%)': 62.643678160919535%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_70: Processing images: 100%|██████████| 174/174 [01:40<00:00,  1.74image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_70. Total images: 174, predicted_correct': 104, predicted_correct (%)': 59.77011494252874%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_72: Processing images: 100%|██████████| 174/174 [01:45<00:00,  1.65image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_72. Total images: 174, predicted_correct': 104, predicted_correct (%)': 59.77011494252874%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_74: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_74. Total images: 174, predicted_correct': 99, predicted_correct (%)': 56.896551724137936%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_76: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_76. Total images: 174, predicted_correct': 114, predicted_correct (%)': 65.51724137931035%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_78: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.60image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_78. Total images: 174, predicted_correct': 115, predicted_correct (%)': 66.0919540229885%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_80: Processing images: 100%|██████████| 174/174 [01:46<00:00,  1.63image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_80. Total images: 174, predicted_correct': 108, predicted_correct (%)': 62.06896551724138%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_82: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_82. Total images: 174, predicted_correct': 85, predicted_correct (%)': 48.85057471264368%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_84: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_84. Total images: 174, predicted_correct': 99, predicted_correct (%)': 56.896551724137936%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_86: Processing images: 100%|██████████| 174/174 [01:44<00:00,  1.66image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_86. Total images: 174, predicted_correct': 118, predicted_correct (%)': 67.81609195402298%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_88: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_88. Total images: 174, predicted_correct': 91, predicted_correct (%)': 52.29885057471264%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_90: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_90. Total images: 174, predicted_correct': 119, predicted_correct (%)': 68.39080459770115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_92: Processing images: 100%|██████████| 174/174 [01:46<00:00,  1.64image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_92. Total images: 174, predicted_correct': 95, predicted_correct (%)': 54.59770114942529%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_94: Processing images: 100%|██████████| 174/174 [01:49<00:00,  1.59image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_94. Total images: 174, predicted_correct': 93, predicted_correct (%)': 53.44827586206896%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_96: Processing images: 100%|██████████| 174/174 [01:45<00:00,  1.65image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_96. Total images: 174, predicted_correct': 102, predicted_correct (%)': 58.620689655172406%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_98: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_98. Total images: 174, predicted_correct': 91, predicted_correct (%)': 52.29885057471264%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_100: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_100. Total images: 174, predicted_correct': 99, predicted_correct (%)': 56.896551724137936%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_102: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_102. Total images: 174, predicted_correct': 81, predicted_correct (%)': 46.55172413793103%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_104: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.60image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_104. Total images: 174, predicted_correct': 135, predicted_correct (%)': 77.58620689655173%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_106: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.60image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_106. Total images: 174, predicted_correct': 100, predicted_correct (%)': 57.47126436781609%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_108: Processing images: 100%|██████████| 174/174 [01:36<00:00,  1.80image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_108. Total images: 174, predicted_correct': 97, predicted_correct (%)': 55.74712643678161%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_110: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_110. Total images: 174, predicted_correct': 107, predicted_correct (%)': 61.49425287356321%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_112: Processing images: 100%|██████████| 174/174 [01:48<00:00,  1.60image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_112. Total images: 174, predicted_correct': 109, predicted_correct (%)': 62.643678160919535%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_114: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.63image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_114. Total images: 174, predicted_correct': 108, predicted_correct (%)': 62.06896551724138%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_116: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_116. Total images: 174, predicted_correct': 106, predicted_correct (%)': 60.91954022988506%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_118: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.61image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_118. Total images: 174, predicted_correct': 109, predicted_correct (%)': 62.643678160919535%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_120: Processing images: 100%|██████████| 174/174 [01:47<00:00,  1.62image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_120. Total images: 174, predicted_correct': 110, predicted_correct (%)': 63.2183908045977%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_122: Processing images: 100%|██████████| 174/174 [01:43<00:00,  1.69image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_122. Total images: 174, predicted_correct': 112, predicted_correct (%)': 64.36781609195403%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_124: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_124. Total images: 174, predicted_correct': 144, predicted_correct (%)': 82.75862068965517%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_126: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_126. Total images: 174, predicted_correct': 108, predicted_correct (%)': 62.06896551724138%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_128: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_128. Total images: 174, predicted_correct': 136, predicted_correct (%)': 78.16091954022988%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_130: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_130. Total images: 174, predicted_correct': 118, predicted_correct (%)': 67.81609195402298%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_132: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_132. Total images: 174, predicted_correct': 128, predicted_correct (%)': 73.5632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_134: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_134. Total images: 174, predicted_correct': 142, predicted_correct (%)': 81.60919540229885%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_136: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_136. Total images: 174, predicted_correct': 112, predicted_correct (%)': 64.36781609195403%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_138: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_138. Total images: 174, predicted_correct': 124, predicted_correct (%)': 71.26436781609196%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_140: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_140. Total images: 174, predicted_correct': 143, predicted_correct (%)': 82.18390804597702%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_142: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_142. Total images: 174, predicted_correct': 145, predicted_correct (%)': 83.33333333333334%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_144: Processing images: 100%|██████████| 174/174 [01:26<00:00,  2.02image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_144. Total images: 174, predicted_correct': 137, predicted_correct (%)': 78.73563218390804%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_146: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_146. Total images: 174, predicted_correct': 132, predicted_correct (%)': 75.86206896551724%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_148: Processing images: 100%|██████████| 174/174 [01:25<00:00,  2.03image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_148. Total images: 174, predicted_correct': 136, predicted_correct (%)': 78.16091954022988%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_150: Processing images: 100%|██████████| 174/174 [01:37<00:00,  1.78image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_150. Total images: 174, predicted_correct': 130, predicted_correct (%)': 74.71264367816092%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_152: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_152. Total images: 174, predicted_correct': 129, predicted_correct (%)': 74.13793103448276%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_154: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_154. Total images: 174, predicted_correct': 128, predicted_correct (%)': 73.5632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_156: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.88image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_156. Total images: 174, predicted_correct': 125, predicted_correct (%)': 71.83908045977012%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_158: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_158. Total images: 174, predicted_correct': 140, predicted_correct (%)': 80.45977011494253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_160: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.91image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_160. Total images: 174, predicted_correct': 145, predicted_correct (%)': 83.33333333333334%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_162: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_162. Total images: 174, predicted_correct': 132, predicted_correct (%)': 75.86206896551724%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_164: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_164. Total images: 174, predicted_correct': 139, predicted_correct (%)': 79.88505747126436%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_166: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_166. Total images: 174, predicted_correct': 136, predicted_correct (%)': 78.16091954022988%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_168: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_168. Total images: 174, predicted_correct': 133, predicted_correct (%)': 76.4367816091954%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_170: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_170. Total images: 174, predicted_correct': 128, predicted_correct (%)': 73.5632183908046%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_172: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_172. Total images: 174, predicted_correct': 134, predicted_correct (%)': 77.01149425287356%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_174: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_174. Total images: 174, predicted_correct': 136, predicted_correct (%)': 78.16091954022988%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_176: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_176. Total images: 174, predicted_correct': 132, predicted_correct (%)': 75.86206896551724%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_178: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_178. Total images: 174, predicted_correct': 131, predicted_correct (%)': 75.28735632183908%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_180: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_180. Total images: 174, predicted_correct': 131, predicted_correct (%)': 75.28735632183908%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_182: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_182. Total images: 174, predicted_correct': 134, predicted_correct (%)': 77.01149425287356%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_184: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_184. Total images: 174, predicted_correct': 134, predicted_correct (%)': 77.01149425287356%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_186: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_186. Total images: 174, predicted_correct': 134, predicted_correct (%)': 77.01149425287356%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_188: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.91image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_188. Total images: 174, predicted_correct': 134, predicted_correct (%)': 77.01149425287356%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_190: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_190. Total images: 174, predicted_correct': 137, predicted_correct (%)': 78.73563218390804%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_192: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_192. Total images: 174, predicted_correct': 141, predicted_correct (%)': 81.03448275862068%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_194: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_194. Total images: 174, predicted_correct': 141, predicted_correct (%)': 81.03448275862068%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_196: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_196. Total images: 174, predicted_correct': 140, predicted_correct (%)': 80.45977011494253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_198: Processing images: 100%|██████████| 174/174 [01:32<00:00,  1.89image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_198. Total images: 174, predicted_correct': 139, predicted_correct (%)': 79.88505747126436%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_200: Processing images: 100%|██████████| 174/174 [01:31<00:00,  1.90image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_file: epoch_200. Total images: 174, predicted_correct': 139, predicted_correct (%)': 79.88505747126436%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Batch test for Direct Classification benchmark 4 (cropped square & data augmentation images, normalizing with mean and std)\n",
    "import torch, os, open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import ToTensor\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class AlbumentationsTransform2:\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)  # Convert to numpy array\n",
    "        img = self.transform(image=img)['image']\n",
    "        return Image.fromarray(img)  # Convert back to PIL image\n",
    "\n",
    "# Define the CLAHE transformation\n",
    "clahe = A.CLAHE(p=1.0, clip_limit=4.0, tile_grid_size=(8, 8))\n",
    "CLAHE = AlbumentationsTransform2(clahe), # normalizing using Adaptive Histogram Equalization (CLAHE)\n",
    "\n",
    "def _convert_to_rgb(image):\n",
    "    return image.convert('RGB')\n",
    "to_tensor = ToTensor()\n",
    "\n",
    "\n",
    "def extract_classification(file_path):\n",
    "    classification = []\n",
    "    df_classification = pd.read_csv(file_path)\n",
    "    classification = df_classification['tissue_types'].tolist()\n",
    "    classification = list(set(classification))\n",
    "    return classification, df_classification\n",
    "\n",
    "classification_csv_file = '/mnt/g/Datasets/Body_Parts_XRay/train_df_new1.csv'\n",
    "classification, df_classification = extract_classification(classification_csv_file)\n",
    "\n",
    "\n",
    "def process_single_epoch(pretrained_model, sentences):\n",
    "    img_dir = '/mnt/g/Datasets/Body_Parts_XRay/Square/test/images/'\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        model_name=\"coca_ViT-L-14\",\n",
    "        pretrained=pretrained_model,\n",
    "    )\n",
    "    pt_dir = os.path.dirname(pretrained_model)\n",
    "    epoch_number = os.path.basename(pretrained_model).split('epoch_')[1].split('.pt')[0]\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    # Prepare the tokenizer for sentences\n",
    "    tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "    text = tokenizer(sentences)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def read_single_image(img_dir, img_filename, sentences, text_features):\n",
    "        #get the ground truth value\n",
    "        image_path = os.path.join(img_dir, img_filename)\n",
    "        ground_truth = df_classification[df_classification['filename']==img_filename]['tissue_types'].iloc[0]\n",
    "        \n",
    "        # load a sample image\n",
    "        image = preprocess(Image.open(image_path).convert('RGB')) #.unsqueeze(0)\n",
    "        # image = to_tensor(image)\n",
    "        image = normalize(image)\n",
    "        image = image.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            text_probs = text_probs.cpu().tolist()[0]\n",
    "\n",
    "        # Construct the dictionary\n",
    "        class_predict_dict = dict(zip(sentences, text_probs))\n",
    "        # Extract the key with the largest value\n",
    "        class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "        \n",
    "        if ground_truth == class_predict:\n",
    "            predicted_correct = 1\n",
    "        else:\n",
    "            predicted_correct = 0\n",
    "        \n",
    "        pred_dict = {'img_filename': [img_filename], 'class_predict_dict': [class_predict_dict], 'ground_truth': [ground_truth], 'class_predict': [class_predict], 'predicted_correct': [predicted_correct]}\n",
    "        df_single = pd.DataFrame(pred_dict)\n",
    "        \n",
    "        return df_single\n",
    "\n",
    "    all_images = os.listdir(img_dir)\n",
    "    for img_filename in tqdm(all_images, desc=f\"epoch_{epoch_number}: Processing images\", unit=\"image\"):\n",
    "        df = read_single_image(img_dir, img_filename, sentences, text_features)\n",
    "        df_all = pd.concat([df_all, df]).reset_index(drop=True)\n",
    "\n",
    "    #save df_all to csv\n",
    "    csv_file = f\"temp/epoch_{epoch_number}.csv\"\n",
    "    df_all.to_csv(csv_file, index=False)\n",
    "\n",
    "    total_rows = len(df_all)\n",
    "    # Count the occurrences of '1' in the 'predicted_correct' column\n",
    "    count_ones = df_all['predicted_correct'].sum()\n",
    "    epoch_filename = os.path.splitext(os.path.basename(pretrained_model))[0]\n",
    "    predicted_correct_pct = count_ones /total_rows *100\n",
    "    print(f\"Epoch_file: {epoch_filename}. Total images: {total_rows}, predicted_correct': {count_ones}, predicted_correct (%)': {predicted_correct_pct}%\")\n",
    "    \n",
    "    epoch_dict = {'pt_dir': [pt_dir],\n",
    "                'epoch_filename': [epoch_filename], \n",
    "                'sentences': [sentences], \n",
    "                'total_rows': [total_rows], \n",
    "                'predicted_correct': [count_ones], \n",
    "                'predicted_correct (%)': [predicted_correct_pct]}\n",
    "    df_single_epoch = pd.DataFrame(epoch_dict)\n",
    "    return df_single_epoch\n",
    "\n",
    "\n",
    "def process_single_pretrained_model(pretrained_model, csv_file):\n",
    "    df_single_pretrained_model = pd.DataFrame()\n",
    "    df = process_single_epoch(pretrained_model, classification)\n",
    "    df_single_pretrained_model = pd.concat([df_single_pretrained_model, df]).reset_index(drop=True)\n",
    "    save_to_csv(df_single_pretrained_model, csv_file)\n",
    "    \n",
    "def extract_epoch_num(filepath):\n",
    "    # Split the filename from the path\n",
    "    filename = os.path.basename(filepath) #filepath.split('/')[-1]\n",
    "    # Extract the number between \"epoch_\" and \".pt\"\n",
    "    epoch_num = int(filename.split('epoch_')[1].split('.pt')[0])\n",
    "    return epoch_num\n",
    "\n",
    "def save_to_csv(df, csv_file):\n",
    "    if not os.path.exists(csv_file):\n",
    "        df.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # If the CSV file already exists, append without header\n",
    "        df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "def main():\n",
    "    pt_folder_path = '/mnt/g/Logtemp/open_clip/Body_Parts_XRay/2023_08_21-23_03_36-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints/'\n",
    "    pt_files = [os.path.join(pt_folder_path, filename) for filename in os.listdir(pt_folder_path) if filename.endswith('.pt')]\n",
    "    #filter pt_files with low epoch number\n",
    "    pt_files = [pt_file for pt_file in pt_files if extract_epoch_num(pt_file) >= 0]\n",
    "    # Sort the list\n",
    "    sorted_pt_files = sorted(pt_files, key=extract_epoch_num)\n",
    "    csv_file = \"benchmark_all_epochs_augment3.csv\"\n",
    "    for pretrained_model in sorted_pt_files:\n",
    "        process_single_pretrained_model(pretrained_model, csv_file)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/g/Logtemp/open_clip/Body_Parts_XRay/2023_08_22-07_36_22-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "pt_folder_root = '/mnt/g/Logtemp/open_clip/Body_Parts_XRay/'\n",
    "dir_1 = [a for a in os.listdir(pt_folder_root) if a.startswith('2023_08_22')][0]\n",
    "os.path.join(pt_folder_root, dir_1, 'checkpoints')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
